{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emsmullen/VirtueSemantics/blob/main/Copy_of_VirtueSemantics_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoxpPoNvkNhw"
      },
      "source": [
        "#Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jy8Ma6rVDjy2"
      },
      "outputs": [],
      "source": [
        "#import packages\n",
        "import sys, os, random, requests, json #connects with interpreter; interacts with os; HTTP requests; JSON data\n",
        "import pandas as pd #data structures and analysis tools\n",
        "import numpy as np #for multi-dimensional data and mathematical functions (arrays)\n",
        "import scipy.stats as st #complex computations (numerical)\n",
        "import matplotlib.pyplot as plt #for plotting data\n",
        "import sklearn.decomposition #for PCA\n",
        "import git\n",
        "from google.colab import drive, data_table, userdata #access Google Drive; interactive pandas dataframes; secrets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#commit to git repo\n",
        "pat = userdata.get('git')\n",
        "username = 'emsmullen'\n",
        "repo = 'VirtueSemantics'\n",
        "!git clone https://{pat}@github.com/{username}/{repo}.git\n",
        "#!git remote add origin https://{pat}@github.com/{username}/{repo}.git\n",
        "#!git remote add origin https://github.com/emsmullen/VirtueSemantics.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoPa-BOY8MUa",
        "outputId": "2ef3d8bb-67dd-4647-a68e-2ec337ee3148"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'VirtueSemantics' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vfF-WOZhprD",
        "outputId": "08dc3a8f-e93f-411f-f04f-561f766131e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#set up paths for local modules and data sources\n",
        "drive.mount('/content/drive',force_remount=True)\n",
        "sys.path.append('/content/drive/MyDrive/')\n",
        "stim_path = '/content/drive/MyDrive/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ynj6PBEiyn3",
        "outputId": "3b61bab1-4dd1-4abe-f5a3-02203401a38c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=userdata.get('OAI') #APIs connect software applications\n",
            "env: github_token=userdata.get('git')\n"
          ]
        }
      ],
      "source": [
        "#set environment variable with API key and github token\n",
        "%env OPENAI_API_KEY = userdata.get('OAI') #APIs connect software applications\n",
        "%env github_token = userdata.get('git')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# commit to github\n",
        "!git clone https://{pat}@github.com/username/repo.git"
      ],
      "metadata": {
        "id": "p4cU1BN58Ib4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzSLWg5PjVQH"
      },
      "outputs": [],
      "source": [
        "#read in stimuli stored on google drive (must add this file to drive)\n",
        "#action_data = pd.read_csv(stim_path+\"ValueSemantics/actions_human_complete.csv\")\n",
        "action_data = pd.read_csv(stim_path+\"ValueSemantics/action_neurips.csv\")\n",
        "action_list_all = action_data[\"actions\"].tolist()\n",
        "#action_list_all = action_data[:,0].tolist()\n",
        "n_items = len(action_list_all)\n",
        "\n",
        "virtue_data = pd.read_csv(stim_path+\"ValueSemantics/excess_deficiency_mean.csv\")\n",
        "virtue_list_all = virtue_data[\"excess\"].tolist() + virtue_data[\"deficiency\"].tolist() + virtue_data[\"mean\"].tolist()\n",
        "n_virtues = len(virtue_list_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVvBF-aGl2-7"
      },
      "outputs": [],
      "source": [
        "excess_list = virtue_data[\"excess\"].tolist()\n",
        "deficiency_list = virtue_data[\"deficiency\"].tolist()\n",
        "mean_list = virtue_data[\"mean\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "mIbUC75p4yIf",
        "outputId": "fc5d9392-bb15-413f-9f62-cbdf937a97db"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"virtue_data\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"excess\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"shyness\",\n          \"obsequiousness\",\n          \"rashness\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"modesty\",\n          \"friendliness\",\n          \"courage\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deficiency\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"shamelessness\",\n          \"cantankerousness\",\n          \"cowardice\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "virtue_data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-9c61db27-6374-4b57-b800-79594153be3e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>excess</th>\n",
              "      <th>mean</th>\n",
              "      <th>deficiency</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rashness</td>\n",
              "      <td>courage</td>\n",
              "      <td>cowardice</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>licentiousness</td>\n",
              "      <td>temperance</td>\n",
              "      <td>insensibility</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>prodigality</td>\n",
              "      <td>liberality</td>\n",
              "      <td>meanness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vulgarity</td>\n",
              "      <td>magnificence</td>\n",
              "      <td>pettiness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vanity</td>\n",
              "      <td>magnanimity</td>\n",
              "      <td>pusillanimity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>empty vanity</td>\n",
              "      <td>pride</td>\n",
              "      <td>unambitiousness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>irascibility</td>\n",
              "      <td>patience</td>\n",
              "      <td>lack of spirit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>boastfulness</td>\n",
              "      <td>truthfulness</td>\n",
              "      <td>mock modesty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>buffoonery</td>\n",
              "      <td>wittiness</td>\n",
              "      <td>boorishness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>obsequiousness</td>\n",
              "      <td>friendliness</td>\n",
              "      <td>cantankerousness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>shyness</td>\n",
              "      <td>modesty</td>\n",
              "      <td>shamelessness</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>envy</td>\n",
              "      <td>righteous indignation</td>\n",
              "      <td>spitefulness</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c61db27-6374-4b57-b800-79594153be3e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9c61db27-6374-4b57-b800-79594153be3e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9c61db27-6374-4b57-b800-79594153be3e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-92ce5bc5-4ed4-4322-bd0e-27ae9f1d5dd8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-92ce5bc5-4ed4-4322-bd0e-27ae9f1d5dd8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-92ce5bc5-4ed4-4322-bd0e-27ae9f1d5dd8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_3a947223-98f3-4ea9-bc20-57944b9e789a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('virtue_data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_3a947223-98f3-4ea9-bc20-57944b9e789a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('virtue_data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            excess                   mean        deficiency\n",
              "0         rashness                courage         cowardice\n",
              "1   licentiousness             temperance     insensibility\n",
              "2      prodigality             liberality          meanness\n",
              "3        vulgarity           magnificence         pettiness\n",
              "4           vanity            magnanimity     pusillanimity\n",
              "5     empty vanity                  pride   unambitiousness\n",
              "6     irascibility               patience    lack of spirit\n",
              "7     boastfulness           truthfulness      mock modesty\n",
              "8       buffoonery              wittiness       boorishness\n",
              "9   obsequiousness           friendliness  cantankerousness\n",
              "10         shyness                modesty     shamelessness\n",
              "11            envy  righteous indignation      spitefulness"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "virtue_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLyoyE_1kLsA"
      },
      "source": [
        "# Embedding Projections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lfCh9i9kIXM"
      },
      "outputs": [],
      "source": [
        "#@title define functions\n",
        "def getEmbeddings(text):\n",
        "    url = 'https://api.openai.com/v1/embeddings'\n",
        "    headers = {\n",
        "      \"Content-Type\": \"application/json\",\n",
        "      \"Authorization\": userdata.get('OAI')\n",
        "      }\n",
        "    data = {\n",
        "        \"input\": text,\n",
        "        \"model\": \"text-embedding-ada-002\"\n",
        "    }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "\n",
        "    return response.json()\n",
        "\n",
        "#function to get the difference of embeddings of two texts\n",
        "def return_embeddings_diff(attributes_a, attributes_b):\n",
        "  '''\n",
        "  Pass two sets of attributes and get all vector differences of all a-b combinations\n",
        "  NOTE: The distinction between high and low embeddings is automatically made by given data\n",
        "  '''\n",
        "  emb_high = pd.DataFrame()\n",
        "  emb_low = pd.DataFrame()\n",
        "\n",
        "  #loop through each attribute set and save the embeddings in two dataframes\n",
        "  for a in range(len(attributes_a)):\n",
        "    #get embedding of the high, low, and compute difference\n",
        "    this_emb = getEmbeddings(attributes_a[a])\n",
        "    emb_high.insert(loc=0, column=a, value=this_emb[\"data\"][0][\"embedding\"])\n",
        "\n",
        "  for b in range(len(attributes_b)):\n",
        "    #get embedding of the high, low, and compute difference\n",
        "    this_emb = getEmbeddings(attributes_b[b])\n",
        "    emb_low.insert(loc=0, column=b, value=this_emb[\"data\"][0][\"embedding\"])\n",
        "\n",
        "  #get all differences\n",
        "  vector_diff = pd.DataFrame()\n",
        "\n",
        "  #report correlations\n",
        "  print(emb_low.corr()) #correlations are higher when closer to 1\n",
        "  print(emb_high.corr())\n",
        "\n",
        "  for a in range(len(attributes_a)):\n",
        "    for b in range(len(attributes_b)):\n",
        "      this_col = str(a)+'_'+str(b)\n",
        "      vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
        "\n",
        "  return(vector_diff.mean(axis=1)) #pandas series; contains average value per row for overall differences in embeddings\n",
        "\n",
        "\n",
        "def return_list_embeddings(alist):\n",
        "  '''\n",
        "  get embeddings for a list of items\n",
        "  '''\n",
        "  n_df = pd.DataFrame()\n",
        "  for i in alist:\n",
        "    this_em = getEmbeddings(i)[\"data\"][0][\"embedding\"]\n",
        "    n_df.insert(loc=0, column=i, value=this_em, allow_duplicates=True)\n",
        "\n",
        "  return (n_df)\n",
        "\n",
        "\n",
        "def get_projections(stim_list, moral_v, hedonic_v, movement_v):\n",
        "  '''\n",
        "  get projections for list of items\n",
        "  '''\n",
        "  #construct dataframe to save items\n",
        "  projection_df = pd.DataFrame(index=stim_list, columns=['moral_v','hedonic_v','movement_v'], data=0)\n",
        "\n",
        "  #loop through items to get embeddings in moral, hedonic, and movement vector directions\n",
        "\n",
        "  for a in stim_list:\n",
        "    this_em = getEmbeddings(a)[\"data\"][0][\"embedding\"]\n",
        "    projection_moral = np.inner(np.array(this_em),np.array(moral_v))\n",
        "    projection_hedonic = np.inner(np.array(this_em),np.array(hedonic_v))\n",
        "    projection_movement = np.inner(np.array(this_em),np.array(movement_v))\n",
        "    projection_df.loc[a, \"moral_v\"] = projection_moral\n",
        "    projection_df.loc[a, \"hedonic_v\"] = projection_hedonic\n",
        "    projection_df.loc[a, \"movement_v\"] = projection_movement\n",
        "\n",
        "  return projection_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWk6F30JW1BQ"
      },
      "outputs": [],
      "source": [
        "#redefine to be more general - feed in dataframe of axes to project actions on and return projections\n",
        "def get_projections_df(stim_list, moral_v, hedonic_v, movement_v):\n",
        "  '''\n",
        "  get projections for list of items onto dataframe of vectors\n",
        "  '''\n",
        "  #construct dataframe to save items\n",
        "  projection_df = pd.DataFrame(index=stim_list, columns=['moral_v','hedonic_v','movement_v'], data=0)\n",
        "\n",
        "  #loop through items to get embeddings in moral, hedonic, and movement vector directions\n",
        "\n",
        "  for a in stim_list:\n",
        "    this_em = getEmbeddings(a)[\"data\"][0][\"embedding\"]\n",
        "    projection_moral = np.inner(np.array(this_em),np.array(moral_v))\n",
        "    projection_hedonic = np.inner(np.array(this_em),np.array(hedonic_v))\n",
        "    projection_movement = np.inner(np.array(this_em),np.array(movement_v))\n",
        "    projection_df.loc[a, \"moral_v\"] = projection_moral\n",
        "    projection_df.loc[a, \"hedonic_v\"] = projection_hedonic\n",
        "    projection_df.loc[a, \"movement_v\"] = projection_movement\n",
        "\n",
        "  return projection_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuDy-cOHM5z3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFgxu3r41eh8"
      },
      "outputs": [],
      "source": [
        "virtue_embeddings = return_list_embeddings(virtue_list_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stOxSCQlojXG"
      },
      "source": [
        "## generate embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8hJiu33_7cJ",
        "outputId": "867e9f47-e12b-4a8a-d6f8-84a0b2964ca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.82467433 0.01410767 0.0125143 ]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-0.05173792,  0.00637147,  0.00251083],\n",
              "       [-0.05160807,  0.01002551, -0.01241943],\n",
              "       [ 0.0671698 ,  0.00702784, -0.02060437],\n",
              "       ...,\n",
              "       [-0.09018547,  0.00443659,  0.01770667],\n",
              "       [ 0.05626571, -0.00072268, -0.02218571],\n",
              "       [-0.15904461,  0.00272192,  0.00473151]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#virtues_PCA = sklearn.decomposition.PCA(virtue_embeddings,n_components=3)\n",
        "pca = sklearn.decomposition.PCA(n_components=3)\n",
        "pca.fit(virtue_embeddings)\n",
        "print(pca.explained_variance_ratio_)\n",
        "pca.fit_transform(virtue_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSEMgyLBbQl6"
      },
      "outputs": [],
      "source": [
        "virtue_emb_mean = []\n",
        "for virtue in mean_list:\n",
        "  virtue_emb = getEmbeddings(virtue)[\"data\"][0][\"embedding\"]\n",
        "  virtue_emb_mean.append(virtue_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sRNO7mmNbVw3",
        "outputId": "a0aa8c44-a586-4ca5-834e-cc0701fa25f2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"virtue_emb_mean_df\",\n  \"rows\": 1536,\n  \"fields\": [\n    {\n      \"column\": \"courage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02551301436135368,\n        \"min\": -0.6900963,\n        \"max\": 0.20491165,\n        \"num_unique_values\": 1417,\n        \"samples\": [\n          -0.0061840345,\n          0.0063412557,\n          -0.02142139\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025512349755059453,\n        \"min\": -0.6775013,\n        \"max\": 0.1895544,\n        \"num_unique_values\": 1426,\n        \"samples\": [\n          0.0046221325,\n          -0.007842196,\n          -0.002366411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liberality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025512352845558174,\n        \"min\": -0.6742681,\n        \"max\": 0.16945189,\n        \"num_unique_values\": 1420,\n        \"samples\": [\n          0.008212665,\n          -0.017946195,\n          -0.032021098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnificence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513054443564782,\n        \"min\": -0.6868657,\n        \"max\": 0.16577688,\n        \"num_unique_values\": 1419,\n        \"samples\": [\n          8.265477e-05,\n          0.023241114,\n          -0.007782392\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnanimity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513130020846687,\n        \"min\": -0.67204237,\n        \"max\": 0.18505043,\n        \"num_unique_values\": 1428,\n        \"samples\": [\n          -0.018328674,\n          -0.0095238555,\n          -0.034079667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pride\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513482159077117,\n        \"min\": -0.6949289,\n        \"max\": 0.19016917,\n        \"num_unique_values\": 1413,\n        \"samples\": [\n          -0.030325117,\n          -0.0053868694,\n          -0.00402471\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"patience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513313683963153,\n        \"min\": -0.6908585,\n        \"max\": 0.19752388,\n        \"num_unique_values\": 1414,\n        \"samples\": [\n          -0.022130381,\n          0.00065528427,\n          -0.006674085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"truthfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025512779954808232,\n        \"min\": -0.686021,\n        \"max\": 0.17760368,\n        \"num_unique_values\": 1393,\n        \"samples\": [\n          -0.019940292,\n          -0.023176745,\n          0.001433501\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wittiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02551261411464276,\n        \"min\": -0.69911015,\n        \"max\": 0.17242722,\n        \"num_unique_values\": 1424,\n        \"samples\": [\n          -0.018735703,\n          -0.0071176975,\n          0.02174036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"friendliness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025511509333260216,\n        \"min\": -0.6952347,\n        \"max\": 0.16809563,\n        \"num_unique_values\": 1404,\n        \"samples\": [\n          0.017345162,\n          -0.012703305,\n          0.023264214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"modesty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02551250311568573,\n        \"min\": -0.67740786,\n        \"max\": 0.19643947,\n        \"num_unique_values\": 1418,\n        \"samples\": [\n          0.0057292553,\n          -0.011038709,\n          0.011988424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"righteous indignation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513492612569477,\n        \"min\": -0.6799826,\n        \"max\": 0.20757252,\n        \"num_unique_values\": 1416,\n        \"samples\": [\n          -0.010254252,\n          -0.020296803,\n          0.012754966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "virtue_emb_mean_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-ed95de35-1a20-470f-ac65-af3dfa1f96e9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>courage</th>\n",
              "      <th>temperance</th>\n",
              "      <th>liberality</th>\n",
              "      <th>magnificence</th>\n",
              "      <th>magnanimity</th>\n",
              "      <th>pride</th>\n",
              "      <th>patience</th>\n",
              "      <th>truthfulness</th>\n",
              "      <th>wittiness</th>\n",
              "      <th>friendliness</th>\n",
              "      <th>modesty</th>\n",
              "      <th>righteous indignation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.012774</td>\n",
              "      <td>-0.004112</td>\n",
              "      <td>-0.005600</td>\n",
              "      <td>-0.008041</td>\n",
              "      <td>-0.004328</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.004526</td>\n",
              "      <td>-0.010826</td>\n",
              "      <td>-0.008253</td>\n",
              "      <td>0.004982</td>\n",
              "      <td>-0.002438</td>\n",
              "      <td>-0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.030501</td>\n",
              "      <td>-0.012391</td>\n",
              "      <td>-0.014227</td>\n",
              "      <td>0.007849</td>\n",
              "      <td>-0.016999</td>\n",
              "      <td>-0.023082</td>\n",
              "      <td>0.000872</td>\n",
              "      <td>-0.012531</td>\n",
              "      <td>-0.008199</td>\n",
              "      <td>-0.001430</td>\n",
              "      <td>-0.011328</td>\n",
              "      <td>-0.016460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.017832</td>\n",
              "      <td>0.007030</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>0.018468</td>\n",
              "      <td>0.019102</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.003711</td>\n",
              "      <td>-0.005146</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.005881</td>\n",
              "      <td>0.004673</td>\n",
              "      <td>-0.005365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.012879</td>\n",
              "      <td>-0.033489</td>\n",
              "      <td>-0.040953</td>\n",
              "      <td>-0.037228</td>\n",
              "      <td>-0.030091</td>\n",
              "      <td>-0.031573</td>\n",
              "      <td>-0.029560</td>\n",
              "      <td>-0.020114</td>\n",
              "      <td>-0.007872</td>\n",
              "      <td>-0.025036</td>\n",
              "      <td>-0.017756</td>\n",
              "      <td>-0.025483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.009165</td>\n",
              "      <td>-0.017670</td>\n",
              "      <td>-0.018043</td>\n",
              "      <td>0.005353</td>\n",
              "      <td>-0.004874</td>\n",
              "      <td>0.006235</td>\n",
              "      <td>0.006512</td>\n",
              "      <td>-0.005239</td>\n",
              "      <td>-0.004410</td>\n",
              "      <td>0.007567</td>\n",
              "      <td>-0.022807</td>\n",
              "      <td>-0.012689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>0.053979</td>\n",
              "      <td>0.035099</td>\n",
              "      <td>0.041091</td>\n",
              "      <td>0.056267</td>\n",
              "      <td>0.086881</td>\n",
              "      <td>0.045696</td>\n",
              "      <td>0.019887</td>\n",
              "      <td>0.015701</td>\n",
              "      <td>0.057476</td>\n",
              "      <td>0.037464</td>\n",
              "      <td>0.045173</td>\n",
              "      <td>0.056259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>0.023413</td>\n",
              "      <td>0.022259</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>-0.000325</td>\n",
              "      <td>-0.001485</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.015902</td>\n",
              "      <td>-0.000435</td>\n",
              "      <td>0.004771</td>\n",
              "      <td>0.015217</td>\n",
              "      <td>-0.008217</td>\n",
              "      <td>-0.004270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>-0.020937</td>\n",
              "      <td>-0.009908</td>\n",
              "      <td>-0.025233</td>\n",
              "      <td>-0.024567</td>\n",
              "      <td>-0.031828</td>\n",
              "      <td>-0.028843</td>\n",
              "      <td>-0.008729</td>\n",
              "      <td>-0.001844</td>\n",
              "      <td>-0.019537</td>\n",
              "      <td>-0.027110</td>\n",
              "      <td>-0.005640</td>\n",
              "      <td>-0.009368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>0.002782</td>\n",
              "      <td>-0.008064</td>\n",
              "      <td>0.018306</td>\n",
              "      <td>-0.003384</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.020715</td>\n",
              "      <td>0.016720</td>\n",
              "      <td>0.016369</td>\n",
              "      <td>0.004580</td>\n",
              "      <td>-0.001856</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>0.010565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>-0.012165</td>\n",
              "      <td>-0.027666</td>\n",
              "      <td>-0.044437</td>\n",
              "      <td>-0.028425</td>\n",
              "      <td>-0.040402</td>\n",
              "      <td>-0.015969</td>\n",
              "      <td>-0.020732</td>\n",
              "      <td>-0.031937</td>\n",
              "      <td>-0.050585</td>\n",
              "      <td>-0.044194</td>\n",
              "      <td>-0.028602</td>\n",
              "      <td>-0.021395</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1536 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed95de35-1a20-470f-ac65-af3dfa1f96e9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed95de35-1a20-470f-ac65-af3dfa1f96e9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed95de35-1a20-470f-ac65-af3dfa1f96e9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f44a9fc9-e60c-4417-b6a2-d671ae47d809\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f44a9fc9-e60c-4417-b6a2-d671ae47d809')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f44a9fc9-e60c-4417-b6a2-d671ae47d809 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_6b5d1b45-e482-4892-b189-3640c33288ad\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('virtue_emb_mean_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6b5d1b45-e482-4892-b189-3640c33288ad button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('virtue_emb_mean_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       courage  temperance  liberality  magnificence  magnanimity     pride  \\\n",
              "0    -0.012774   -0.004112   -0.005600     -0.008041    -0.004328  0.000002   \n",
              "1    -0.030501   -0.012391   -0.014227      0.007849    -0.016999 -0.023082   \n",
              "2    -0.017832    0.007030   -0.003365      0.018468     0.019102  0.008849   \n",
              "3    -0.012879   -0.033489   -0.040953     -0.037228    -0.030091 -0.031573   \n",
              "4     0.009165   -0.017670   -0.018043      0.005353    -0.004874  0.006235   \n",
              "...        ...         ...         ...           ...          ...       ...   \n",
              "1531  0.053979    0.035099    0.041091      0.056267     0.086881  0.045696   \n",
              "1532  0.023413    0.022259    0.000922     -0.000325    -0.001485  0.007445   \n",
              "1533 -0.020937   -0.009908   -0.025233     -0.024567    -0.031828 -0.028843   \n",
              "1534  0.002782   -0.008064    0.018306     -0.003384     0.002696  0.020715   \n",
              "1535 -0.012165   -0.027666   -0.044437     -0.028425    -0.040402 -0.015969   \n",
              "\n",
              "      patience  truthfulness  wittiness  friendliness   modesty  \\\n",
              "0    -0.004526     -0.010826  -0.008253      0.004982 -0.002438   \n",
              "1     0.000872     -0.012531  -0.008199     -0.001430 -0.011328   \n",
              "2     0.003711     -0.005146   0.001512      0.005881  0.004673   \n",
              "3    -0.029560     -0.020114  -0.007872     -0.025036 -0.017756   \n",
              "4     0.006512     -0.005239  -0.004410      0.007567 -0.022807   \n",
              "...        ...           ...        ...           ...       ...   \n",
              "1531  0.019887      0.015701   0.057476      0.037464  0.045173   \n",
              "1532  0.015902     -0.000435   0.004771      0.015217 -0.008217   \n",
              "1533 -0.008729     -0.001844  -0.019537     -0.027110 -0.005640   \n",
              "1534  0.016720      0.016369   0.004580     -0.001856  0.002448   \n",
              "1535 -0.020732     -0.031937  -0.050585     -0.044194 -0.028602   \n",
              "\n",
              "      righteous indignation  \n",
              "0                 -0.011200  \n",
              "1                 -0.016460  \n",
              "2                 -0.005365  \n",
              "3                 -0.025483  \n",
              "4                 -0.012689  \n",
              "...                     ...  \n",
              "1531               0.056259  \n",
              "1532              -0.004270  \n",
              "1533              -0.009368  \n",
              "1534               0.010565  \n",
              "1535              -0.021395  \n",
              "\n",
              "[1536 rows x 12 columns]"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "virtue_emb_mean_df = pd.DataFrame(data=virtue_emb_mean,index=mean_list).transpose()\n",
        "virtue_emb_mean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RFnKTR0Xbpow",
        "outputId": "d6304e8c-4843-4bcd-c7f6-d6e956058438"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"virtue_emb_diff_df\",\n  \"rows\": 1536,\n  \"fields\": [\n    {\n      \"column\": \"courage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004859827675219186,\n        \"min\": -0.01605613991041667,\n        \"max\": 0.01639080500555555,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.0023115495277777776,\n          0.0011994619944444443,\n          -0.001618390916666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00203512708617691,\n        \"min\": -0.006741076329890108,\n        \"max\": 0.006752098695604397,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.002630779835164837,\n          0.0017701257989010991,\n          0.0015514270467032968\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liberality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005070843566637815,\n        \"min\": -0.014864981820454544,\n        \"max\": 0.016548297604545455,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.005558813710227275,\n          -0.0009630737340909104,\n          -0.0013520678704545457\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnificence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004483535541553449,\n        \"min\": -0.017586399044444447,\n        \"max\": 0.014729722055555557,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.004499003222222222,\n          -0.006518438277777779,\n          0.003996000188888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnanimity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0027667019476895183,\n        \"min\": -0.010350999637179484,\n        \"max\": 0.009789166325641028,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.006533516871794871,\n          -0.0006419037884615387,\n          -0.0030354081705128207\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pride\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0031831501411447834,\n        \"min\": -0.011145328748833326,\n        \"max\": 0.012849447749999996,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.00017855015833333264,\n          -0.0013720935866666673,\n          -0.00011633729166666653\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"patience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0030588344509429625,\n        \"min\": -0.010137527886904762,\n        \"max\": 0.014864311602380952,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.0008527878928571433,\n          -0.0012064488011904759,\n          -0.0017380739559523799\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"truthfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003133373137831796,\n        \"min\": -0.012186014699999996,\n        \"max\": 0.01195325233333333,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.0006859675833333326,\n          0.00034409878333333313,\n          -0.0036634002916666663\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wittiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003334408858310175,\n        \"min\": -0.015760360363636372,\n        \"max\": 0.009639911760000007,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.007292534036363633,\n          -0.0012792425563636362,\n          -0.0008036777409090903\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"friendliness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0024016195806818856,\n        \"min\": -0.008237482214285713,\n        \"max\": 0.007643808765892852,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.0007770765178571427,\n          0.0011879414857142861,\n          -0.0009633993705357143\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"modesty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002267509158667594,\n        \"min\": -0.008094843593406589,\n        \"max\": 0.007119640380219778,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.0027736598186813184,\n          0.0006038274065934069,\n          -0.0023651272747252754\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"righteous indignation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004513371926579614,\n        \"min\": -0.01835201865,\n        \"max\": 0.020534593749999993,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.008627894166666665,\n          -0.003353471841666667,\n          -0.002101512608333332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "virtue_emb_diff_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-29981776-3666-4d97-9bdc-1457ba4816a1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>mean</th>\n",
              "      <th>courage</th>\n",
              "      <th>temperance</th>\n",
              "      <th>liberality</th>\n",
              "      <th>magnificence</th>\n",
              "      <th>magnanimity</th>\n",
              "      <th>pride</th>\n",
              "      <th>patience</th>\n",
              "      <th>truthfulness</th>\n",
              "      <th>wittiness</th>\n",
              "      <th>friendliness</th>\n",
              "      <th>modesty</th>\n",
              "      <th>righteous indignation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.006090</td>\n",
              "      <td>0.003987</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>-0.002930</td>\n",
              "      <td>-0.002703</td>\n",
              "      <td>-0.003718</td>\n",
              "      <td>-0.003945</td>\n",
              "      <td>-0.000537</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>-0.000519</td>\n",
              "      <td>-0.002082</td>\n",
              "      <td>-0.005421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.003991</td>\n",
              "      <td>0.002449</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>-0.004644</td>\n",
              "      <td>-0.006362</td>\n",
              "      <td>-0.000495</td>\n",
              "      <td>0.005848</td>\n",
              "      <td>-0.002428</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>-0.002792</td>\n",
              "      <td>-0.010326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.007664</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>-0.010242</td>\n",
              "      <td>-0.007068</td>\n",
              "      <td>0.003739</td>\n",
              "      <td>-0.001336</td>\n",
              "      <td>-0.003079</td>\n",
              "      <td>0.005283</td>\n",
              "      <td>-0.000721</td>\n",
              "      <td>-0.004651</td>\n",
              "      <td>0.001940</td>\n",
              "      <td>0.002773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010169</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006810</td>\n",
              "      <td>-0.004634</td>\n",
              "      <td>0.002451</td>\n",
              "      <td>-0.002528</td>\n",
              "      <td>-0.001454</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>-0.008314</td>\n",
              "      <td>-0.002707</td>\n",
              "      <td>0.002917</td>\n",
              "      <td>-0.001030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010463</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>-0.002721</td>\n",
              "      <td>-0.000434</td>\n",
              "      <td>-0.002617</td>\n",
              "      <td>-0.000494</td>\n",
              "      <td>0.004870</td>\n",
              "      <td>-0.004848</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>-0.000450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>-0.003444</td>\n",
              "      <td>-0.001823</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>-0.000353</td>\n",
              "      <td>-0.001117</td>\n",
              "      <td>0.002441</td>\n",
              "      <td>0.001912</td>\n",
              "      <td>0.004286</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>-0.000699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>0.006610</td>\n",
              "      <td>-0.000931</td>\n",
              "      <td>-0.002173</td>\n",
              "      <td>-0.005846</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>-0.002792</td>\n",
              "      <td>0.002191</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>-0.006021</td>\n",
              "      <td>-0.000557</td>\n",
              "      <td>0.004108</td>\n",
              "      <td>-0.004619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>-0.001310</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.001494</td>\n",
              "      <td>-0.001571</td>\n",
              "      <td>-0.002002</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.003828</td>\n",
              "      <td>0.000355</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>-0.001145</td>\n",
              "      <td>-0.004628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.001927</td>\n",
              "      <td>-0.003327</td>\n",
              "      <td>-0.004012</td>\n",
              "      <td>-0.003374</td>\n",
              "      <td>-0.006605</td>\n",
              "      <td>0.003321</td>\n",
              "      <td>0.007672</td>\n",
              "      <td>-0.001658</td>\n",
              "      <td>0.001891</td>\n",
              "      <td>0.002076</td>\n",
              "      <td>-0.006523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>0.003439</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>-0.000597</td>\n",
              "      <td>0.002428</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>-0.003259</td>\n",
              "      <td>-0.001511</td>\n",
              "      <td>-0.003120</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>-0.000340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1536 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29981776-3666-4d97-9bdc-1457ba4816a1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29981776-3666-4d97-9bdc-1457ba4816a1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29981776-3666-4d97-9bdc-1457ba4816a1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0a5c1f05-f4df-429f-824d-0d2c4236ede1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a5c1f05-f4df-429f-824d-0d2c4236ede1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0a5c1f05-f4df-429f-824d-0d2c4236ede1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_269b0378-85a8-41a7-9bc9-68c831826059\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('virtue_emb_diff_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_269b0378-85a8-41a7-9bc9-68c831826059 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('virtue_emb_diff_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "mean   courage  temperance  liberality  magnificence  magnanimity     pride  \\\n",
              "0    -0.006090    0.003987    0.000592     -0.002930    -0.002703 -0.003718   \n",
              "1     0.001181    0.003991    0.002449      0.002218    -0.004644 -0.006362   \n",
              "2     0.007664    0.000153   -0.010242     -0.007068     0.003739 -0.001336   \n",
              "3     0.010169   -0.002252   -0.006810     -0.004634     0.002451 -0.002528   \n",
              "4     0.010463    0.000464   -0.006486     -0.002721    -0.000434 -0.002617   \n",
              "...        ...         ...         ...           ...          ...       ...   \n",
              "1531 -0.003444   -0.001823    0.001428      0.000370    -0.000353 -0.001117   \n",
              "1532  0.006610   -0.000931   -0.002173     -0.005846     0.000746 -0.002792   \n",
              "1533 -0.001310    0.001312    0.003797      0.001494    -0.001571 -0.002002   \n",
              "1534  0.003797    0.001927   -0.003327     -0.004012    -0.003374 -0.006605   \n",
              "1535  0.003439    0.000199    0.001248     -0.000597     0.002428  0.003313   \n",
              "\n",
              "mean  patience  truthfulness  wittiness  friendliness   modesty  \\\n",
              "0    -0.003945     -0.000537  -0.000228     -0.000519 -0.002082   \n",
              "1    -0.000495      0.005848  -0.002428      0.001882 -0.002792   \n",
              "2    -0.003079      0.005283  -0.000721     -0.004651  0.001940   \n",
              "3    -0.001454      0.007776  -0.008314     -0.002707  0.002917   \n",
              "4    -0.000494      0.004870  -0.004848      0.001558  0.001072   \n",
              "...        ...           ...        ...           ...       ...   \n",
              "1531  0.002441      0.001912   0.004286      0.000947  0.001488   \n",
              "1532  0.002191      0.001349  -0.006021     -0.000557  0.004108   \n",
              "1533  0.000293      0.003828   0.000355      0.002304 -0.001145   \n",
              "1534  0.003321      0.007672  -0.001658      0.001891  0.002076   \n",
              "1535 -0.003259     -0.001511  -0.003120      0.001161  0.000454   \n",
              "\n",
              "mean  righteous indignation  \n",
              "0                 -0.005421  \n",
              "1                 -0.010326  \n",
              "2                  0.002773  \n",
              "3                 -0.001030  \n",
              "4                 -0.000450  \n",
              "...                     ...  \n",
              "1531              -0.000699  \n",
              "1532              -0.004619  \n",
              "1533              -0.004628  \n",
              "1534              -0.006523  \n",
              "1535              -0.000340  \n",
              "\n",
              "[1536 rows x 12 columns]"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "virtue_emb_diff_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2F4a1obb7Ry"
      },
      "outputs": [],
      "source": [
        "#cosine similarity between virtue embedding by mean and difference between extremes\n",
        "\n",
        "\n",
        "cosine = np.dot(virtue_emb_diff_df['courage'],virtue_emb_mean_df['courage'])/(np.linalg.norm(virtue_emb_mean_df['courage'])* np.linalg.norm(virtue_emb_mean_df['courage']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHehVC_HeNu1",
        "outputId": "2792a4d2-060f-4400-8b38-415cf1729dc3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-0.0018304056847011235"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cosine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a06W22iffcxk"
      },
      "outputs": [],
      "source": [
        "#moral value projection onto virtues (or virtues onto moral)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YXurNsKlCUWb",
        "outputId": "8ffffe7c-7be4-48c9-819c-4be1c5645fca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          8         7         6         5         4         3         2  \\\n",
            "8  1.000000  0.875951  0.888051  0.893943  0.827073  0.893327  0.861238   \n",
            "7  0.875951  1.000000  0.877011  0.954035  0.837328  0.908790  0.879508   \n",
            "6  0.888051  0.877011  1.000000  0.891153  0.819285  0.895084  0.875005   \n",
            "5  0.893943  0.954035  0.891153  1.000000  0.840427  0.920386  0.878731   \n",
            "4  0.827073  0.837328  0.819285  0.840427  1.000000  0.834542  0.851146   \n",
            "3  0.893327  0.908790  0.895084  0.920386  0.834542  1.000000  0.871189   \n",
            "2  0.861238  0.879508  0.875005  0.878731  0.851146  0.871189  1.000000   \n",
            "1  0.874518  0.846976  0.885113  0.862010  0.821599  0.879171  0.859677   \n",
            "0  0.875951  1.000000  0.877011  0.954035  0.837328  0.908790  0.879508   \n",
            "\n",
            "          1         0  \n",
            "8  0.874518  0.875951  \n",
            "7  0.846976  1.000000  \n",
            "6  0.885113  0.877011  \n",
            "5  0.862010  0.954035  \n",
            "4  0.821599  0.837328  \n",
            "3  0.879171  0.908790  \n",
            "2  0.859677  0.879508  \n",
            "1  1.000000  0.846976  \n",
            "0  0.846976  1.000000  \n",
            "          7         6         5         4         3         2         1  \\\n",
            "7  1.000000  1.000000  0.885007  0.907609  0.896101  1.000000  0.896037   \n",
            "6  1.000000  1.000000  0.885007  0.907609  0.896101  1.000000  0.896037   \n",
            "5  0.885007  0.885007  1.000000  0.879890  0.871385  0.885007  0.893373   \n",
            "4  0.907609  0.907609  0.879890  1.000000  0.883069  0.907609  0.882021   \n",
            "3  0.896101  0.896101  0.871385  0.883069  1.000000  0.896101  0.883623   \n",
            "2  1.000000  1.000000  0.885007  0.907609  0.896101  1.000000  0.896037   \n",
            "1  0.896037  0.896037  0.893373  0.882021  0.883623  0.896037  1.000000   \n",
            "0  0.845302  0.845302  0.827073  0.838587  0.857831  0.845302  0.834573   \n",
            "\n",
            "          0  \n",
            "7  0.845302  \n",
            "6  0.845302  \n",
            "5  0.827073  \n",
            "4  0.838587  \n",
            "3  0.857831  \n",
            "2  0.845302  \n",
            "1  0.834573  \n",
            "0  1.000000  \n",
            "          12        11        10        9         8         7         6   \\\n",
            "12  1.000000  0.869578  0.875731  0.865171  0.875731  0.853333  0.875731   \n",
            "11  0.869578  1.000000  0.880384  0.874651  0.880384  0.882580  0.880384   \n",
            "10  0.875731  0.880384  1.000000  0.864096  1.000000  0.900037  1.000000   \n",
            "9   0.865171  0.874651  0.864096  1.000000  0.864096  0.874635  0.864096   \n",
            "8   0.875731  0.880384  1.000000  0.864096  1.000000  0.900037  1.000000   \n",
            "7   0.853333  0.882580  0.900037  0.874635  0.900037  1.000000  0.900037   \n",
            "6   0.875731  0.880384  1.000000  0.864096  1.000000  0.900037  1.000000   \n",
            "5   0.871650  0.893451  0.882786  0.887996  0.882786  0.897277  0.882786   \n",
            "4   0.886657  0.877628  0.873908  0.890595  0.873908  0.881831  0.873908   \n",
            "3   0.860448  0.874542  0.888051  0.860621  0.888051  0.886817  0.888051   \n",
            "2   0.871650  0.893451  0.882786  0.887996  0.882786  0.897277  0.882786   \n",
            "1   0.886657  0.877628  0.873908  0.890595  0.873908  0.881831  0.873908   \n",
            "0   0.875841  0.880573  0.999996  0.864253  0.999996  0.900184  0.999996   \n",
            "\n",
            "          5         4         3         2         1         0   \n",
            "12  0.871650  0.886657  0.860448  0.871650  0.886657  0.875841  \n",
            "11  0.893451  0.877628  0.874542  0.893451  0.877628  0.880573  \n",
            "10  0.882786  0.873908  0.888051  0.882786  0.873908  0.999996  \n",
            "9   0.887996  0.890595  0.860621  0.887996  0.890595  0.864253  \n",
            "8   0.882786  0.873908  0.888051  0.882786  0.873908  0.999996  \n",
            "7   0.897277  0.881831  0.886817  0.897277  0.881831  0.900184  \n",
            "6   0.882786  0.873908  0.888051  0.882786  0.873908  0.999996  \n",
            "5   1.000000  0.907609  0.885007  1.000000  0.907609  0.882904  \n",
            "4   0.907609  1.000000  0.879890  0.907609  1.000000  0.874020  \n",
            "3   0.885007  0.879890  1.000000  0.885007  0.879890  0.888173  \n",
            "2   1.000000  0.907609  0.885007  1.000000  0.907609  0.882904  \n",
            "1   0.907609  1.000000  0.879890  0.907609  1.000000  0.874020  \n",
            "0   0.882904  0.874020  0.888173  0.882904  0.874020  1.000000  \n",
            "          13        12        11        10        9         8         7   \\\n",
            "13  1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "12  1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "11  0.885007  0.885007  1.000000  0.879890  0.885007  0.850529  0.874518   \n",
            "10  0.907609  0.907609  0.879890  1.000000  0.907609  0.844173  0.876204   \n",
            "9   1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "8   0.839237  0.839237  0.850529  0.844173  0.839237  1.000000  0.854596   \n",
            "7   0.871537  0.871537  0.874518  0.876204  0.871537  0.854596  1.000000   \n",
            "6   0.882786  0.882786  0.888051  0.873908  0.882786  0.871802  0.885113   \n",
            "5   0.893451  0.893451  0.874542  0.877628  0.893451  0.850457  0.864881   \n",
            "4   0.907609  0.907609  0.879890  1.000000  0.907609  0.844173  0.876204   \n",
            "3   0.885007  0.885007  1.000000  0.879890  0.885007  0.850529  0.874518   \n",
            "2   0.883723  0.883723  0.875951  0.867235  0.883723  0.826632  0.846976   \n",
            "1   0.882786  0.882786  0.888051  0.873908  0.882786  0.871802  0.885113   \n",
            "0   0.887996  0.887996  0.860621  0.890595  0.887996  0.843763  0.858688   \n",
            "\n",
            "          6         5         4         3         2         1         0   \n",
            "13  0.882786  0.893451  0.907609  0.885007  0.883723  0.882786  0.887996  \n",
            "12  0.882786  0.893451  0.907609  0.885007  0.883723  0.882786  0.887996  \n",
            "11  0.888051  0.874542  0.879890  1.000000  0.875951  0.888051  0.860621  \n",
            "10  0.873908  0.877628  1.000000  0.879890  0.867235  0.873908  0.890595  \n",
            "9   0.882786  0.893451  0.907609  0.885007  0.883723  0.882786  0.887996  \n",
            "8   0.871802  0.850457  0.844173  0.850529  0.826632  0.871802  0.843763  \n",
            "7   0.885113  0.864881  0.876204  0.874518  0.846976  0.885113  0.858688  \n",
            "6   1.000000  0.880384  0.873908  0.888051  0.877011  1.000000  0.864096  \n",
            "5   0.880384  1.000000  0.877628  0.874542  0.878441  0.880384  0.874651  \n",
            "4   0.873908  0.877628  1.000000  0.879890  0.867235  0.873908  0.890595  \n",
            "3   0.888051  0.874542  0.879890  1.000000  0.875951  0.888051  0.860621  \n",
            "2   0.877011  0.878441  0.867235  0.875951  1.000000  0.877011  0.866354  \n",
            "1   1.000000  0.880384  0.873908  0.888051  0.877011  1.000000  0.864096  \n",
            "0   0.864096  0.874651  0.890595  0.860621  0.866354  0.864096  1.000000  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          7         6         5         4         3         2         1  \\\n",
            "7  1.000000  1.000000  0.885007  0.907609  0.907609  0.895990  0.885007   \n",
            "6  1.000000  1.000000  0.885007  0.907609  0.907609  0.895990  0.885007   \n",
            "5  0.885007  0.885007  1.000000  0.879890  0.879890  0.893327  1.000000   \n",
            "4  0.907609  0.907609  0.879890  1.000000  1.000000  0.882006  0.879890   \n",
            "3  0.907609  0.907609  0.879890  1.000000  1.000000  0.882006  0.879890   \n",
            "2  0.895990  0.895990  0.893327  0.882006  0.882006  1.000000  0.893327   \n",
            "1  0.885007  0.885007  1.000000  0.879890  0.879890  0.893327  1.000000   \n",
            "0  0.888777  0.888777  0.862380  0.884373  0.884373  0.866493  0.862380   \n",
            "\n",
            "          0  \n",
            "7  0.888777  \n",
            "6  0.888777  \n",
            "5  0.862380  \n",
            "4  0.884373  \n",
            "3  0.884373  \n",
            "2  0.866493  \n",
            "1  0.862380  \n",
            "0  1.000000  \n",
            "          10        9         8         7         6         5         4   \\\n",
            "10  1.000000  0.869578  0.875731  0.865171  0.859033  0.863602  0.875731   \n",
            "9   0.869578  1.000000  0.880384  0.874651  0.874070  0.877200  0.880384   \n",
            "8   0.875731  0.880384  1.000000  0.864096  0.895084  0.869888  1.000000   \n",
            "7   0.865171  0.874651  0.864096  1.000000  0.871911  0.883922  0.864096   \n",
            "6   0.859033  0.874070  0.895084  0.871911  1.000000  0.876271  0.895084   \n",
            "5   0.863602  0.877200  0.869888  0.883922  0.876271  1.000000  0.869888   \n",
            "4   0.875731  0.880384  1.000000  0.864096  0.895084  0.869888  1.000000   \n",
            "3   0.853287  0.878714  0.891153  0.874804  0.920386  0.879946  0.891153   \n",
            "2   0.864186  0.864881  0.885113  0.858688  0.879171  0.870783  0.885113   \n",
            "1   0.842173  0.851836  0.819285  0.855092  0.834542  0.849807  0.819285   \n",
            "0   0.871944  0.895237  0.885897  0.883824  0.883672  0.892942  0.885897   \n",
            "\n",
            "          3         2         1         0   \n",
            "10  0.853287  0.864186  0.842173  0.871944  \n",
            "9   0.878714  0.864881  0.851836  0.895237  \n",
            "8   0.891153  0.885113  0.819285  0.885897  \n",
            "7   0.874804  0.858688  0.855092  0.883824  \n",
            "6   0.920386  0.879171  0.834542  0.883672  \n",
            "5   0.879946  0.870783  0.849807  0.892942  \n",
            "4   0.891153  0.885113  0.819285  0.885897  \n",
            "3   1.000000  0.862010  0.840427  0.886662  \n",
            "2   0.862010  1.000000  0.821599  0.875173  \n",
            "1   0.840427  0.821599  1.000000  0.855821  \n",
            "0   0.886662  0.875173  0.855821  1.000000  \n",
            "          8         7         6         5         4         3         2  \\\n",
            "8  1.000000  1.000000  0.885007  0.907609  0.882786  0.893451  0.893451   \n",
            "7  1.000000  1.000000  0.885007  0.907609  0.882786  0.893451  0.893451   \n",
            "6  0.885007  0.885007  1.000000  0.879890  0.888051  0.874542  0.874542   \n",
            "5  0.907609  0.907609  0.879890  1.000000  0.873908  0.877628  0.877628   \n",
            "4  0.882786  0.882786  0.888051  0.873908  1.000000  0.880384  0.880384   \n",
            "3  0.893451  0.893451  0.874542  0.877628  0.880384  1.000000  1.000000   \n",
            "2  0.893451  0.893451  0.874542  0.877628  0.880384  1.000000  1.000000   \n",
            "1  0.885007  0.885007  1.000000  0.879890  0.888051  0.874542  0.874542   \n",
            "0  0.901284  0.901284  0.871513  0.891997  0.885897  0.895237  0.895237   \n",
            "\n",
            "          1         0  \n",
            "8  0.885007  0.901284  \n",
            "7  0.885007  0.901284  \n",
            "6  1.000000  0.871513  \n",
            "5  0.879890  0.891997  \n",
            "4  0.888051  0.885897  \n",
            "3  0.874542  0.895237  \n",
            "2  0.874542  0.895237  \n",
            "1  1.000000  0.871513  \n",
            "0  0.871513  1.000000  \n",
            "          8         7         6         5         4         3         2  \\\n",
            "8  1.000000  0.869578  0.875731  0.842173  0.859033  0.863602  0.865171   \n",
            "7  0.869578  1.000000  0.880384  0.851836  0.874070  0.877200  0.874651   \n",
            "6  0.875731  0.880384  1.000000  0.819285  0.895084  0.869888  0.864096   \n",
            "5  0.842173  0.851836  0.819285  1.000000  0.834542  0.849807  0.855092   \n",
            "4  0.859033  0.874070  0.895084  0.834542  1.000000  0.876271  0.871911   \n",
            "3  0.863602  0.877200  0.869888  0.849807  0.876271  1.000000  0.883922   \n",
            "2  0.865171  0.874651  0.864096  0.855092  0.871911  0.883922  1.000000   \n",
            "1  0.865920  0.850457  0.871802  0.824595  0.841325  0.835753  0.843763   \n",
            "0  0.870991  0.878225  0.865930  0.862762  0.864274  0.886249  0.891346   \n",
            "\n",
            "          1         0  \n",
            "8  0.865920  0.870991  \n",
            "7  0.850457  0.878225  \n",
            "6  0.871802  0.865930  \n",
            "5  0.824595  0.862762  \n",
            "4  0.841325  0.864274  \n",
            "3  0.835753  0.886249  \n",
            "2  0.843763  0.891346  \n",
            "1  1.000000  0.851524  \n",
            "0  0.851524  1.000000  \n",
            "          12        11        10        9         8         7         6   \\\n",
            "12  1.000000  0.869578  0.875731  0.855523  0.875731  0.886657  0.859033   \n",
            "11  0.869578  1.000000  0.880384  0.874339  0.880384  0.877628  0.874070   \n",
            "10  0.875731  0.880384  1.000000  0.855505  1.000000  0.873908  0.895084   \n",
            "9   0.855523  0.874339  0.855505  1.000000  0.855505  0.884373  0.866493   \n",
            "8   0.875731  0.880384  1.000000  0.855505  1.000000  0.873908  0.895084   \n",
            "7   0.886657  0.877628  0.873908  0.884373  0.873908  1.000000  0.882006   \n",
            "6   0.859033  0.874070  0.895084  0.866493  0.895084  0.882006  1.000000   \n",
            "5   0.865171  0.874651  0.864096  0.884330  0.864096  0.890595  0.871911   \n",
            "4   0.865171  0.874651  0.864096  0.884330  0.864096  0.890595  0.871911   \n",
            "3   0.875731  0.880384  1.000000  0.855505  1.000000  0.873908  0.895084   \n",
            "2   0.871650  0.893451  0.882786  0.888777  0.882786  0.907609  0.895990   \n",
            "1   0.865920  0.850457  0.871802  0.833630  0.871802  0.844173  0.841325   \n",
            "0   0.871944  0.895237  0.885897  0.880691  0.885897  0.891997  0.883672   \n",
            "\n",
            "          5         4         3         2         1         0   \n",
            "12  0.865171  0.865171  0.875731  0.871650  0.865920  0.871944  \n",
            "11  0.874651  0.874651  0.880384  0.893451  0.850457  0.895237  \n",
            "10  0.864096  0.864096  1.000000  0.882786  0.871802  0.885897  \n",
            "9   0.884330  0.884330  0.855505  0.888777  0.833630  0.880691  \n",
            "8   0.864096  0.864096  1.000000  0.882786  0.871802  0.885897  \n",
            "7   0.890595  0.890595  0.873908  0.907609  0.844173  0.891997  \n",
            "6   0.871911  0.871911  0.895084  0.895990  0.841325  0.883672  \n",
            "5   1.000000  1.000000  0.864096  0.887996  0.843763  0.883824  \n",
            "4   1.000000  1.000000  0.864096  0.887996  0.843763  0.883824  \n",
            "3   0.864096  0.864096  1.000000  0.882786  0.871802  0.885897  \n",
            "2   0.887996  0.887996  0.882786  1.000000  0.839237  0.901284  \n",
            "1   0.843763  0.843763  0.871802  0.839237  1.000000  0.847249  \n",
            "0   0.883824  0.883824  0.885897  0.901284  0.847249  1.000000  \n",
            "          5         4         3         2         1         0\n",
            "5  1.000000  0.869578  0.875731  0.886657  0.859033  0.870991\n",
            "4  0.869578  1.000000  0.880384  0.877628  0.874070  0.878225\n",
            "3  0.875731  0.880384  1.000000  0.873908  0.895084  0.865930\n",
            "2  0.886657  0.877628  0.873908  1.000000  0.882006  0.889547\n",
            "1  0.859033  0.874070  0.895084  0.882006  1.000000  0.864274\n",
            "0  0.870991  0.878225  0.865930  0.889547  0.864274  1.000000\n",
            "          14        13        12        11        10        9         8   \\\n",
            "14  1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "13  1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "12  0.885007  0.885007  1.000000  0.879890  0.885007  0.850529  0.874518   \n",
            "11  0.907609  0.907609  0.879890  1.000000  0.907609  0.844173  0.876204   \n",
            "10  1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "9   0.839237  0.839237  0.850529  0.844173  0.839237  1.000000  0.854596   \n",
            "8   0.871537  0.871537  0.874518  0.876204  0.871537  0.854596  1.000000   \n",
            "7   0.882786  0.882786  0.888051  0.873908  0.882786  0.871802  0.885113   \n",
            "6   0.893451  0.893451  0.874542  0.877628  0.893451  0.850457  0.864881   \n",
            "5   0.882932  0.882932  0.888130  0.874061  0.882932  0.872114  0.885201   \n",
            "4   0.897277  0.897277  0.886817  0.881831  0.897277  0.833538  0.866930   \n",
            "3   0.888777  0.888777  0.862380  0.884373  0.888777  0.833630  0.843661   \n",
            "2   0.895990  0.895990  0.893327  0.882006  0.895990  0.841325  0.879171   \n",
            "1   0.907609  0.907609  0.879890  1.000000  0.907609  0.844173  0.876204   \n",
            "0   0.839274  0.839274  0.850565  0.844190  0.839274  0.999998  0.854615   \n",
            "\n",
            "          7         6         5         4         3         2         1   \\\n",
            "14  0.882786  0.893451  0.882932  0.897277  0.888777  0.895990  0.907609   \n",
            "13  0.882786  0.893451  0.882932  0.897277  0.888777  0.895990  0.907609   \n",
            "12  0.888051  0.874542  0.888130  0.886817  0.862380  0.893327  0.879890   \n",
            "11  0.873908  0.877628  0.874061  0.881831  0.884373  0.882006  1.000000   \n",
            "10  0.882786  0.893451  0.882932  0.897277  0.888777  0.895990  0.907609   \n",
            "9   0.871802  0.850457  0.872114  0.833538  0.833630  0.841325  0.844173   \n",
            "8   0.885113  0.864881  0.885201  0.866930  0.843661  0.879171  0.876204   \n",
            "7   1.000000  0.880384  0.999996  0.900037  0.855505  0.895084  0.873908   \n",
            "6   0.880384  1.000000  0.880590  0.882580  0.874339  0.874070  0.877628   \n",
            "5   0.999996  0.880590  1.000000  0.900126  0.855682  0.895243  0.874061   \n",
            "4   0.900037  0.882580  0.900126  1.000000  0.866570  0.936383  0.881831   \n",
            "3   0.855505  0.874339  0.855682  0.866570  1.000000  0.866493  0.884373   \n",
            "2   0.895084  0.874070  0.895243  0.936383  0.866493  1.000000  0.882006   \n",
            "1   0.873908  0.877628  0.874061  0.881831  0.884373  0.882006  1.000000   \n",
            "0   0.871833  0.850451  0.872144  0.833531  0.833651  0.841339  0.844190   \n",
            "\n",
            "          0   \n",
            "14  0.839274  \n",
            "13  0.839274  \n",
            "12  0.850565  \n",
            "11  0.844190  \n",
            "10  0.839274  \n",
            "9   0.999998  \n",
            "8   0.854615  \n",
            "7   0.871833  \n",
            "6   0.850451  \n",
            "5   0.872144  \n",
            "4   0.833531  \n",
            "3   0.833651  \n",
            "2   0.841339  \n",
            "1   0.844190  \n",
            "0   1.000000  \n",
            "          11        10        9         8         7         6         5   \\\n",
            "11  1.000000  0.869578  0.875731  0.886657  0.859033  0.870991  0.824581   \n",
            "10  0.869578  1.000000  0.880384  0.877628  0.874070  0.878225  0.827963   \n",
            "9   0.875731  0.880384  1.000000  0.873908  0.895084  0.865930  0.829000   \n",
            "8   0.886657  0.877628  0.873908  1.000000  0.882006  0.889547  0.848526   \n",
            "7   0.859033  0.874070  0.895084  0.882006  1.000000  0.864274  0.832180   \n",
            "6   0.870991  0.878225  0.865930  0.889547  0.864274  1.000000  0.840153   \n",
            "5   0.824581  0.827963  0.829000  0.848526  0.832180  0.840153  1.000000   \n",
            "4   1.000000  0.869578  0.875731  0.886657  0.859033  0.870991  0.824581   \n",
            "3   0.869578  1.000000  0.880384  0.877628  0.874070  0.878225  0.827963   \n",
            "2   0.871944  0.895237  0.885897  0.891997  0.883672  0.893232  0.836545   \n",
            "1   0.855523  0.874339  0.855505  0.884373  0.866493  0.877205  0.823352   \n",
            "0   0.860448  0.874542  0.888051  0.879890  0.893327  0.859087  0.840638   \n",
            "\n",
            "          4         3         2         1         0   \n",
            "11  1.000000  0.869578  0.871944  0.855523  0.860448  \n",
            "10  0.869578  1.000000  0.895237  0.874339  0.874542  \n",
            "9   0.875731  0.880384  0.885897  0.855505  0.888051  \n",
            "8   0.886657  0.877628  0.891997  0.884373  0.879890  \n",
            "7   0.859033  0.874070  0.883672  0.866493  0.893327  \n",
            "6   0.870991  0.878225  0.893232  0.877205  0.859087  \n",
            "5   0.824581  0.827963  0.836545  0.823352  0.840638  \n",
            "4   1.000000  0.869578  0.871944  0.855523  0.860448  \n",
            "3   0.869578  1.000000  0.895237  0.874339  0.874542  \n",
            "2   0.871944  0.895237  1.000000  0.880691  0.871513  \n",
            "1   0.855523  0.874339  0.880691  1.000000  0.862380  \n",
            "0   0.860448  0.874542  0.871513  0.862380  1.000000  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          13        12        11        10        9         8         7   \\\n",
            "13  1.000000  0.880384  0.851836  0.880384  0.895237  0.893451  0.827963   \n",
            "12  0.880384  1.000000  0.819285  1.000000  0.885897  0.882786  0.829000   \n",
            "11  0.851836  0.819285  1.000000  0.819285  0.855821  0.845302  0.807833   \n",
            "10  0.880384  1.000000  0.819285  1.000000  0.885897  0.882786  0.829000   \n",
            "9   0.895237  0.885897  0.855821  0.885897  1.000000  0.901284  0.836545   \n",
            "8   0.893451  0.882786  0.845302  0.882786  0.901284  1.000000  0.845763   \n",
            "7   0.827963  0.829000  0.807833  0.829000  0.836545  0.845763  1.000000   \n",
            "6   0.886070  0.882735  0.841369  0.882735  0.891682  0.898934  0.831315   \n",
            "5   0.864881  0.885113  0.821599  0.885113  0.875173  0.871537  0.839888   \n",
            "4   0.827963  0.829000  0.807833  0.829000  0.836545  0.845763  1.000000   \n",
            "3   0.869500  0.864984  0.829084  0.864984  0.882703  0.897954  0.840920   \n",
            "2   0.878441  0.877011  0.837328  0.877011  0.881331  0.883723  0.817363   \n",
            "1   0.874070  0.895084  0.834542  0.895084  0.883672  0.895990  0.832180   \n",
            "0   0.874651  0.864096  0.855092  0.864096  0.883824  0.887996  0.840919   \n",
            "\n",
            "          6         5         4         3         2         1         0   \n",
            "13  0.886070  0.864881  0.827963  0.869500  0.878441  0.874070  0.874651  \n",
            "12  0.882735  0.885113  0.829000  0.864984  0.877011  0.895084  0.864096  \n",
            "11  0.841369  0.821599  0.807833  0.829084  0.837328  0.834542  0.855092  \n",
            "10  0.882735  0.885113  0.829000  0.864984  0.877011  0.895084  0.864096  \n",
            "9   0.891682  0.875173  0.836545  0.882703  0.881331  0.883672  0.883824  \n",
            "8   0.898934  0.871537  0.845763  0.897954  0.883723  0.895990  0.887996  \n",
            "7   0.831315  0.839888  1.000000  0.840920  0.817363  0.832180  0.840919  \n",
            "6   1.000000  0.872073  0.831315  0.875688  0.891397  0.896368  0.877280  \n",
            "5   0.872073  1.000000  0.839888  0.876147  0.846976  0.879171  0.858688  \n",
            "4   0.831315  0.839888  1.000000  0.840920  0.817363  0.832180  0.840919  \n",
            "3   0.875688  0.876147  0.840920  1.000000  0.860366  0.865851  0.874712  \n",
            "2   0.891397  0.846976  0.817363  0.860366  1.000000  0.908790  0.866354  \n",
            "1   0.896368  0.879171  0.832180  0.865851  0.908790  1.000000  0.871911  \n",
            "0   0.877280  0.858688  0.840919  0.874712  0.866354  0.871911  1.000000  \n",
            "          11        10        9         8         7         6         5   \\\n",
            "11  1.000000  0.869578  0.875731  0.865171  0.875731  0.853333  0.875731   \n",
            "10  0.869578  1.000000  0.880384  0.874651  0.880384  0.882580  0.880384   \n",
            "9   0.875731  0.880384  1.000000  0.864096  1.000000  0.900037  1.000000   \n",
            "8   0.865171  0.874651  0.864096  1.000000  0.864096  0.874635  0.864096   \n",
            "7   0.875731  0.880384  1.000000  0.864096  1.000000  0.900037  1.000000   \n",
            "6   0.853333  0.882580  0.900037  0.874635  0.900037  1.000000  0.900037   \n",
            "5   0.875731  0.880384  1.000000  0.864096  1.000000  0.900037  1.000000   \n",
            "4   0.842102  0.878441  0.877011  0.866354  0.877011  0.952268  0.877011   \n",
            "3   0.871650  0.893451  0.882786  0.887996  0.882786  0.897277  0.882786   \n",
            "2   0.859033  0.874070  0.895084  0.871911  0.895084  0.936383  0.895084   \n",
            "1   0.842173  0.851836  0.819285  0.855092  0.819285  0.831205  0.819285   \n",
            "0   0.875731  0.880384  1.000000  0.864096  1.000000  0.900037  1.000000   \n",
            "\n",
            "          4         3         2         1         0   \n",
            "11  0.842102  0.871650  0.859033  0.842173  0.875731  \n",
            "10  0.878441  0.893451  0.874070  0.851836  0.880384  \n",
            "9   0.877011  0.882786  0.895084  0.819285  1.000000  \n",
            "8   0.866354  0.887996  0.871911  0.855092  0.864096  \n",
            "7   0.877011  0.882786  0.895084  0.819285  1.000000  \n",
            "6   0.952268  0.897277  0.936383  0.831205  0.900037  \n",
            "5   0.877011  0.882786  0.895084  0.819285  1.000000  \n",
            "4   1.000000  0.883723  0.908790  0.837328  0.877011  \n",
            "3   0.883723  1.000000  0.895990  0.845302  0.882786  \n",
            "2   0.908790  0.895990  1.000000  0.834542  0.895084  \n",
            "1   0.837328  0.845302  0.834542  1.000000  0.819285  \n",
            "0   0.877011  0.882786  0.895084  0.819285  1.000000  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          11        10        9         8         7         6         5   \\\n",
            "11  1.000000  0.869578  0.871650  0.860448  0.853287  0.864186  0.855523   \n",
            "10  0.869578  1.000000  0.893451  0.874542  0.878714  0.864881  0.874339   \n",
            "9   0.871650  0.893451  1.000000  0.885007  0.893102  0.871537  0.888777   \n",
            "8   0.860448  0.874542  0.885007  1.000000  0.893943  0.874518  0.862380   \n",
            "7   0.853287  0.878714  0.893102  0.893943  1.000000  0.862010  0.865116   \n",
            "6   0.864186  0.864881  0.871537  0.874518  0.862010  1.000000  0.843661   \n",
            "5   0.855523  0.874339  0.888777  0.862380  0.865116  0.843661  1.000000   \n",
            "4   0.824581  0.827963  0.845763  0.840638  0.831860  0.839888  0.823352   \n",
            "3   0.869606  0.869500  0.897954  0.863703  0.862572  0.876147  0.876375   \n",
            "2   0.842102  0.878441  0.883723  0.875951  0.954035  0.846976  0.859759   \n",
            "1   0.864186  0.864881  0.871537  0.874518  0.862010  1.000000  0.843661   \n",
            "0   0.855523  0.874339  0.888777  0.862380  0.865116  0.843661  1.000000   \n",
            "\n",
            "          4         3         2         1         0   \n",
            "11  0.824581  0.869606  0.842102  0.864186  0.855523  \n",
            "10  0.827963  0.869500  0.878441  0.864881  0.874339  \n",
            "9   0.845763  0.897954  0.883723  0.871537  0.888777  \n",
            "8   0.840638  0.863703  0.875951  0.874518  0.862380  \n",
            "7   0.831860  0.862572  0.954035  0.862010  0.865116  \n",
            "6   0.839888  0.876147  0.846976  1.000000  0.843661  \n",
            "5   0.823352  0.876375  0.859759  0.843661  1.000000  \n",
            "4   1.000000  0.840920  0.817363  0.839888  0.823352  \n",
            "3   0.840920  1.000000  0.860366  0.876147  0.876375  \n",
            "2   0.817363  0.860366  1.000000  0.846976  0.859759  \n",
            "1   0.839888  0.876147  0.846976  1.000000  0.843661  \n",
            "0   0.823352  0.876375  0.859759  0.843661  1.000000  \n",
            "          11        10        9         8         7         6         5   \\\n",
            "11  1.000000  1.000000  0.885007  0.907609  0.887996  0.839237  0.898934   \n",
            "10  1.000000  1.000000  0.885007  0.907609  0.887996  0.839237  0.898934   \n",
            "9   0.885007  0.885007  1.000000  0.879890  0.860621  0.850529  0.901244   \n",
            "8   0.907609  0.907609  0.879890  1.000000  0.890595  0.844173  0.881511   \n",
            "7   0.887996  0.887996  0.860621  0.890595  1.000000  0.843763  0.877280   \n",
            "6   0.839237  0.839237  0.850529  0.844173  0.843763  1.000000  0.842259   \n",
            "5   0.898934  0.898934  0.901244  0.881511  0.877280  0.842259  1.000000   \n",
            "4   0.893451  0.893451  0.874542  0.877628  0.874651  0.850457  0.886070   \n",
            "3   1.000000  1.000000  0.885007  0.907609  0.887996  0.839237  0.898934   \n",
            "2   0.895990  0.895990  0.893327  0.882006  0.871911  0.841325  0.896368   \n",
            "1   0.871537  0.871537  0.874518  0.876204  0.858688  0.854596  0.872073   \n",
            "0   0.897277  0.897277  0.886817  0.881831  0.874635  0.833538  0.905524   \n",
            "\n",
            "          4         3         2         1         0   \n",
            "11  0.893451  1.000000  0.895990  0.871537  0.897277  \n",
            "10  0.893451  1.000000  0.895990  0.871537  0.897277  \n",
            "9   0.874542  0.885007  0.893327  0.874518  0.886817  \n",
            "8   0.877628  0.907609  0.882006  0.876204  0.881831  \n",
            "7   0.874651  0.887996  0.871911  0.858688  0.874635  \n",
            "6   0.850457  0.839237  0.841325  0.854596  0.833538  \n",
            "5   0.886070  0.898934  0.896368  0.872073  0.905524  \n",
            "4   1.000000  0.893451  0.874070  0.864881  0.882580  \n",
            "3   0.893451  1.000000  0.895990  0.871537  0.897277  \n",
            "2   0.874070  0.895990  1.000000  0.879171  0.936383  \n",
            "1   0.864881  0.871537  0.879171  1.000000  0.866930  \n",
            "0   0.882580  0.897277  0.936383  0.866930  1.000000  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          10        9         8         7         6         5         4   \\\n",
            "10  1.000000  1.000000  0.885007  0.907609  0.896101  1.000000  0.882786   \n",
            "9   1.000000  1.000000  0.885007  0.907609  0.896101  1.000000  0.882786   \n",
            "8   0.885007  0.885007  1.000000  0.879890  0.871385  0.885007  0.888051   \n",
            "7   0.907609  0.907609  0.879890  1.000000  0.883069  0.907609  0.873908   \n",
            "6   0.896101  0.896101  0.871385  0.883069  1.000000  0.896101  0.877220   \n",
            "5   1.000000  1.000000  0.885007  0.907609  0.896101  1.000000  0.882786   \n",
            "4   0.882786  0.882786  0.888051  0.873908  0.877220  0.882786  1.000000   \n",
            "3   0.845302  0.845302  0.827073  0.838587  0.857831  0.845302  0.819285   \n",
            "2   0.871537  0.871537  0.874518  0.876204  0.865263  0.871537  0.885113   \n",
            "1   0.871537  0.871537  0.874518  0.876204  0.865263  0.871537  0.885113   \n",
            "0   0.897277  0.897277  0.886817  0.881831  0.888502  0.897277  0.900037   \n",
            "\n",
            "          3         2         1         0   \n",
            "10  0.845302  0.871537  0.871537  0.897277  \n",
            "9   0.845302  0.871537  0.871537  0.897277  \n",
            "8   0.827073  0.874518  0.874518  0.886817  \n",
            "7   0.838587  0.876204  0.876204  0.881831  \n",
            "6   0.857831  0.865263  0.865263  0.888502  \n",
            "5   0.845302  0.871537  0.871537  0.897277  \n",
            "4   0.819285  0.885113  0.885113  0.900037  \n",
            "3   1.000000  0.821599  0.821599  0.831205  \n",
            "2   0.821599  1.000000  1.000000  0.866930  \n",
            "1   0.821599  1.000000  1.000000  0.866930  \n",
            "0   0.831205  0.866930  0.866930  1.000000  \n",
            "          9         8         7         6         5         4         3  \\\n",
            "9  1.000000  0.842173  0.860448  0.886657  0.864186  0.864186  0.861286   \n",
            "8  0.842173  1.000000  0.827073  0.838587  0.821599  0.821599  0.841369   \n",
            "7  0.860448  0.827073  1.000000  0.879890  0.874518  0.874518  0.901244   \n",
            "6  0.886657  0.838587  0.879890  1.000000  0.876204  0.876204  0.881511   \n",
            "5  0.864186  0.821599  0.874518  0.876204  1.000000  1.000000  0.872073   \n",
            "4  0.864186  0.821599  0.874518  0.876204  1.000000  1.000000  0.872073   \n",
            "3  0.861286  0.841369  0.901244  0.881511  0.872073  0.872073  1.000000   \n",
            "2  0.861286  0.841369  0.901244  0.881511  0.872073  0.872073  1.000000   \n",
            "1  0.865920  0.824595  0.850529  0.844173  0.854596  0.854596  0.842259   \n",
            "0  0.853333  0.831205  0.886817  0.881831  0.866930  0.866930  0.905524   \n",
            "\n",
            "          2         1         0  \n",
            "9  0.861286  0.865920  0.853333  \n",
            "8  0.841369  0.824595  0.831205  \n",
            "7  0.901244  0.850529  0.886817  \n",
            "6  0.881511  0.844173  0.881831  \n",
            "5  0.872073  0.854596  0.866930  \n",
            "4  0.872073  0.854596  0.866930  \n",
            "3  1.000000  0.842259  0.905524  \n",
            "2  1.000000  0.842259  0.905524  \n",
            "1  0.842259  1.000000  0.833538  \n",
            "0  0.905524  0.833538  1.000000  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          15        14        13        12        11        10        9   \\\n",
            "15  1.000000  1.000000  0.885007  0.907524  1.000000  0.839237  0.871537   \n",
            "14  1.000000  1.000000  0.885007  0.907524  1.000000  0.839237  0.871537   \n",
            "13  0.885007  0.885007  1.000000  0.879837  0.885007  0.850529  0.874518   \n",
            "12  0.907524  0.907524  0.879837  1.000000  0.907524  0.844143  0.876123   \n",
            "11  1.000000  1.000000  0.885007  0.907524  1.000000  0.839237  0.871537   \n",
            "10  0.839237  0.839237  0.850529  0.844143  0.839237  1.000000  0.854596   \n",
            "9   0.871537  0.871537  0.874518  0.876123  0.871537  0.854596  1.000000   \n",
            "8   0.845302  0.845302  0.827073  0.838508  0.845302  0.824595  0.821599   \n",
            "7   0.885007  0.885007  1.000000  0.879837  0.885007  0.850529  0.874518   \n",
            "6   0.897954  0.897954  0.863703  0.895892  0.897954  0.825326  0.876147   \n",
            "5   0.907609  0.907609  0.879890  0.999999  0.907609  0.844173  0.876204   \n",
            "4   0.895990  0.895990  0.893327  0.881929  0.895990  0.841325  0.879171   \n",
            "3   0.893451  0.893451  0.874542  0.877571  0.893451  0.850457  0.864881   \n",
            "2   0.907609  0.907609  0.879890  0.999999  0.907609  0.844173  0.876204   \n",
            "1   0.895990  0.895990  0.893327  0.881929  0.895990  0.841325  0.879171   \n",
            "0   0.883723  0.883723  0.875951  0.867143  0.883723  0.826632  0.846976   \n",
            "\n",
            "          8         7         6         5         4         3         2   \\\n",
            "15  0.845302  0.885007  0.897954  0.907609  0.895990  0.893451  0.907609   \n",
            "14  0.845302  0.885007  0.897954  0.907609  0.895990  0.893451  0.907609   \n",
            "13  0.827073  1.000000  0.863703  0.879890  0.893327  0.874542  0.879890   \n",
            "12  0.838508  0.879837  0.895892  0.999999  0.881929  0.877571  0.999999   \n",
            "11  0.845302  0.885007  0.897954  0.907609  0.895990  0.893451  0.907609   \n",
            "10  0.824595  0.850529  0.825326  0.844173  0.841325  0.850457  0.844173   \n",
            "9   0.821599  0.874518  0.876147  0.876204  0.879171  0.864881  0.876204   \n",
            "8   1.000000  0.827073  0.829084  0.838587  0.834542  0.851836  0.838587   \n",
            "7   0.827073  1.000000  0.863703  0.879890  0.893327  0.874542  0.879890   \n",
            "6   0.829084  0.863703  1.000000  0.895951  0.865851  0.869500  0.895951   \n",
            "5   0.838587  0.879890  0.895951  1.000000  0.882006  0.877628  1.000000   \n",
            "4   0.834542  0.893327  0.865851  0.882006  1.000000  0.874070  0.882006   \n",
            "3   0.851836  0.874542  0.869500  0.877628  0.874070  1.000000  0.877628   \n",
            "2   0.838587  0.879890  0.895951  1.000000  0.882006  0.877628  1.000000   \n",
            "1   0.834542  0.893327  0.865851  0.882006  1.000000  0.874070  0.882006   \n",
            "0   0.837328  0.875951  0.860366  0.867235  0.908790  0.878441  0.867235   \n",
            "\n",
            "          1         0   \n",
            "15  0.895990  0.883723  \n",
            "14  0.895990  0.883723  \n",
            "13  0.893327  0.875951  \n",
            "12  0.881929  0.867143  \n",
            "11  0.895990  0.883723  \n",
            "10  0.841325  0.826632  \n",
            "9   0.879171  0.846976  \n",
            "8   0.834542  0.837328  \n",
            "7   0.893327  0.875951  \n",
            "6   0.865851  0.860366  \n",
            "5   0.882006  0.867235  \n",
            "4   1.000000  0.908790  \n",
            "3   0.874070  0.878441  \n",
            "2   0.882006  0.867235  \n",
            "1   1.000000  0.908790  \n",
            "0   0.908790  1.000000  \n",
            "          13        12        11        10        9         8         7   \\\n",
            "13  1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "12  1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "11  0.885007  0.885007  1.000000  0.879890  0.885007  0.850529  0.874518   \n",
            "10  0.907609  0.907609  0.879890  1.000000  0.907609  0.844173  0.876204   \n",
            "9   1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "8   0.839237  0.839237  0.850529  0.844173  0.839237  1.000000  0.854596   \n",
            "7   0.871537  0.871537  0.874518  0.876204  0.871537  0.854596  1.000000   \n",
            "6   0.882786  0.882786  0.888051  0.873908  0.882786  0.871802  0.885113   \n",
            "5   0.839237  0.839237  0.850529  0.844173  0.839237  1.000000  0.854596   \n",
            "4   0.879069  0.879069  0.865203  0.889019  0.879069  0.844935  0.872001   \n",
            "3   0.885007  0.885007  1.000000  0.879890  0.885007  0.850529  0.874518   \n",
            "2   1.000000  1.000000  0.885007  0.907609  1.000000  0.839237  0.871537   \n",
            "1   0.897277  0.897277  0.886817  0.881831  0.897277  0.833538  0.866930   \n",
            "0   0.871537  0.871537  0.874518  0.876204  0.871537  0.854596  1.000000   \n",
            "\n",
            "          6         5         4         3         2         1         0   \n",
            "13  0.882786  0.839237  0.879069  0.885007  1.000000  0.897277  0.871537  \n",
            "12  0.882786  0.839237  0.879069  0.885007  1.000000  0.897277  0.871537  \n",
            "11  0.888051  0.850529  0.865203  1.000000  0.885007  0.886817  0.874518  \n",
            "10  0.873908  0.844173  0.889019  0.879890  0.907609  0.881831  0.876204  \n",
            "9   0.882786  0.839237  0.879069  0.885007  1.000000  0.897277  0.871537  \n",
            "8   0.871802  1.000000  0.844935  0.850529  0.839237  0.833538  0.854596  \n",
            "7   0.885113  0.854596  0.872001  0.874518  0.871537  0.866930  1.000000  \n",
            "6   1.000000  0.871802  0.864472  0.888051  0.882786  0.900037  0.885113  \n",
            "5   0.871802  1.000000  0.844935  0.850529  0.839237  0.833538  0.854596  \n",
            "4   0.864472  0.844935  1.000000  0.865203  0.879069  0.866077  0.872001  \n",
            "3   0.888051  0.850529  0.865203  1.000000  0.885007  0.886817  0.874518  \n",
            "2   0.882786  0.839237  0.879069  0.885007  1.000000  0.897277  0.871537  \n",
            "1   0.900037  0.833538  0.866077  0.886817  0.897277  1.000000  0.866930  \n",
            "0   0.885113  0.854596  0.872001  0.874518  0.871537  0.866930  1.000000  \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n",
            "<ipython-input-10-9a4e52498519>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  vector_diff.insert(loc=0, column=this_col, value=emb_high[a] - emb_low[b])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          12        11        10        9         8         7         6   \\\n",
            "12  1.000000  1.000000  0.885007  0.907609  1.000000  1.000000  0.885007   \n",
            "11  1.000000  1.000000  0.885007  0.907609  1.000000  1.000000  0.885007   \n",
            "10  0.885007  0.885007  1.000000  0.879890  0.885007  0.885007  1.000000   \n",
            "9   0.907609  0.907609  0.879890  1.000000  0.907609  0.907609  0.879890   \n",
            "8   1.000000  1.000000  0.885007  0.907609  1.000000  1.000000  0.885007   \n",
            "7   1.000000  1.000000  0.885007  0.907609  1.000000  1.000000  0.885007   \n",
            "6   0.885007  0.885007  1.000000  0.879890  0.885007  0.885007  1.000000   \n",
            "5   0.887996  0.887996  0.860621  0.890595  0.887996  0.887996  0.860621   \n",
            "4   0.885007  0.885007  1.000000  0.879890  0.885007  0.885007  1.000000   \n",
            "3   0.888777  0.888777  0.862380  0.884373  0.888777  0.888777  0.862380   \n",
            "2   0.895990  0.895990  0.893327  0.882006  0.895990  0.895990  0.893327   \n",
            "1   0.896101  0.896101  0.871385  0.883069  0.896101  0.896101  0.871385   \n",
            "0   1.000000  1.000000  0.885007  0.907609  1.000000  1.000000  0.885007   \n",
            "\n",
            "          5         4         3         2         1         0   \n",
            "12  0.887996  0.885007  0.888777  0.895990  0.896101  1.000000  \n",
            "11  0.887996  0.885007  0.888777  0.895990  0.896101  1.000000  \n",
            "10  0.860621  1.000000  0.862380  0.893327  0.871385  0.885007  \n",
            "9   0.890595  0.879890  0.884373  0.882006  0.883069  0.907609  \n",
            "8   0.887996  0.885007  0.888777  0.895990  0.896101  1.000000  \n",
            "7   0.887996  0.885007  0.888777  0.895990  0.896101  1.000000  \n",
            "6   0.860621  1.000000  0.862380  0.893327  0.871385  0.885007  \n",
            "5   1.000000  0.860621  0.884330  0.871911  0.885337  0.887996  \n",
            "4   0.860621  1.000000  0.862380  0.893327  0.871385  0.885007  \n",
            "3   0.884330  0.862380  1.000000  0.866493  0.882035  0.888777  \n",
            "2   0.871911  0.893327  0.866493  1.000000  0.883565  0.895990  \n",
            "1   0.885337  0.871385  0.882035  0.883565  1.000000  0.896101  \n",
            "0   0.887996  0.885007  0.888777  0.895990  0.896101  1.000000  \n",
            "          6         5         4         3         2         1         0\n",
            "6  1.000000  1.000000  0.885007  0.907609  0.871650  0.896101  1.000000\n",
            "5  1.000000  1.000000  0.885007  0.907609  0.871650  0.896101  1.000000\n",
            "4  0.885007  0.885007  1.000000  0.879890  0.860448  0.871385  0.885007\n",
            "3  0.907609  0.907609  0.879890  1.000000  0.886657  0.883069  0.907609\n",
            "2  0.871650  0.871650  0.860448  0.886657  1.000000  0.878295  0.871650\n",
            "1  0.896101  0.896101  0.871385  0.883069  0.878295  1.000000  0.896101\n",
            "0  1.000000  1.000000  0.885007  0.907609  0.871650  0.896101  1.000000\n",
            "          11        10        9         8         7         6         5   \\\n",
            "11  1.000000  1.000000  0.885007  0.907609  0.887996  0.839237  0.898934   \n",
            "10  1.000000  1.000000  0.885007  0.907609  0.887996  0.839237  0.898934   \n",
            "9   0.885007  0.885007  1.000000  0.879890  0.860621  0.850529  0.901244   \n",
            "8   0.907609  0.907609  0.879890  1.000000  0.890595  0.844173  0.881511   \n",
            "7   0.887996  0.887996  0.860621  0.890595  1.000000  0.843763  0.877280   \n",
            "6   0.839237  0.839237  0.850529  0.844173  0.843763  1.000000  0.842259   \n",
            "5   0.898934  0.898934  0.901244  0.881511  0.877280  0.842259  1.000000   \n",
            "4   0.885007  0.885007  1.000000  0.879890  0.860621  0.850529  0.901244   \n",
            "3   0.893451  0.893451  0.874542  0.877628  0.874651  0.850457  0.886070   \n",
            "2   0.882786  0.882786  0.888051  0.873908  0.864096  0.871802  0.882735   \n",
            "1   0.901284  0.901284  0.871513  0.891997  0.883824  0.847249  0.891682   \n",
            "0   1.000000  1.000000  0.885007  0.907609  0.887996  0.839237  0.898934   \n",
            "\n",
            "          4         3         2         1         0   \n",
            "11  0.885007  0.893451  0.882786  0.901284  1.000000  \n",
            "10  0.885007  0.893451  0.882786  0.901284  1.000000  \n",
            "9   1.000000  0.874542  0.888051  0.871513  0.885007  \n",
            "8   0.879890  0.877628  0.873908  0.891997  0.907609  \n",
            "7   0.860621  0.874651  0.864096  0.883824  0.887996  \n",
            "6   0.850529  0.850457  0.871802  0.847249  0.839237  \n",
            "5   0.901244  0.886070  0.882735  0.891682  0.898934  \n",
            "4   1.000000  0.874542  0.888051  0.871513  0.885007  \n",
            "3   0.874542  1.000000  0.880384  0.895237  0.893451  \n",
            "2   0.888051  0.880384  1.000000  0.885897  0.882786  \n",
            "1   0.871513  0.895237  0.885897  1.000000  0.901284  \n",
            "0   0.885007  0.893451  0.882786  0.901284  1.000000  \n",
            "          3         2         1         0\n",
            "3  1.000000  0.870991  0.886657  0.860448\n",
            "2  0.870991  1.000000  0.889547  0.859087\n",
            "1  0.886657  0.889547  1.000000  0.879890\n",
            "0  0.860448  0.859087  0.879890  1.000000\n"
          ]
        }
      ],
      "source": [
        "#using virtue vectors as space to project action embeddings on\n",
        "# Initializing lists\n",
        "mean_list = virtue_data['mean'].tolist()\n",
        "excess_list = virtue_data['excess'].tolist()\n",
        "deficiency_list = virtue_data['deficiency'].tolist()\n",
        "\n",
        "# Construct variables of list elements\n",
        "# using dict() + zip()\n",
        "mean_excess_dict = dict(zip(mean_list,excess_list))\n",
        "mean_deficiency_dict = dict(zip(mean_list, deficiency_list))\n",
        "\n",
        "virtue_emb_diffs = []\n",
        "for virtue in mean_list:\n",
        "  temp_excess_list = mean_excess_dict[virtue]\n",
        "  temp_deficiency_list = mean_deficiency_dict[virtue]\n",
        "  emb_diff = return_embeddings_diff(temp_excess_list, temp_deficiency_list)\n",
        "  virtue_emb_diffs.append(emb_diff)\n",
        "\n",
        "virtue_emb_mean = []\n",
        "for virtue in mean_list:\n",
        "  virtue_emb = getEmbeddings(virtue)[\"data\"][0][\"embedding\"]\n",
        "  virtue_emb_mean.append(virtue_emb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9wNnuNeoUyQ7",
        "outputId": "69aa9c08-1540-4a9f-c27e-5ca8a07078d2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"virtue_emb_diff_df\",\n  \"rows\": 1536,\n  \"fields\": [\n    {\n      \"column\": \"courage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004859827675219186,\n        \"min\": -0.01605613991041667,\n        \"max\": 0.01639080500555555,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.0023115495277777776,\n          0.0011994619944444443,\n          -0.001618390916666667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.00203512708617691,\n        \"min\": -0.006741076329890108,\n        \"max\": 0.006752098695604397,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.002630779835164837,\n          0.0017701257989010991,\n          0.0015514270467032968\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liberality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005070843566637815,\n        \"min\": -0.014864981820454544,\n        \"max\": 0.016548297604545455,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.005558813710227275,\n          -0.0009630737340909104,\n          -0.0013520678704545457\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnificence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004483535541553449,\n        \"min\": -0.017586399044444447,\n        \"max\": 0.014729722055555557,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.004499003222222222,\n          -0.006518438277777779,\n          0.003996000188888889\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnanimity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0027667019476895183,\n        \"min\": -0.010350999637179484,\n        \"max\": 0.009789166325641028,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.006533516871794871,\n          -0.0006419037884615387,\n          -0.0030354081705128207\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pride\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0031831501411447834,\n        \"min\": -0.011145328748833326,\n        \"max\": 0.012849447749999996,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.00017855015833333264,\n          -0.0013720935866666673,\n          -0.00011633729166666653\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"patience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0030588344509429625,\n        \"min\": -0.010137527886904762,\n        \"max\": 0.014864311602380952,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.0008527878928571433,\n          -0.0012064488011904759,\n          -0.0017380739559523799\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"truthfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003133373137831796,\n        \"min\": -0.012186014699999996,\n        \"max\": 0.01195325233333333,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          -0.0006859675833333326,\n          0.00034409878333333313,\n          -0.0036634002916666663\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wittiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.003334408858310175,\n        \"min\": -0.015760360363636372,\n        \"max\": 0.009639911760000007,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.007292534036363633,\n          -0.0012792425563636362,\n          -0.0008036777409090903\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"friendliness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0024016195806818856,\n        \"min\": -0.008237482214285713,\n        \"max\": 0.007643808765892852,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.0007770765178571427,\n          0.0011879414857142861,\n          -0.0009633993705357143\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"modesty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002267509158667594,\n        \"min\": -0.008094843593406589,\n        \"max\": 0.007119640380219778,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.0027736598186813184,\n          0.0006038274065934069,\n          -0.0023651272747252754\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"righteous indignation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.004513371926579614,\n        \"min\": -0.01835201865,\n        \"max\": 0.020534593749999993,\n        \"num_unique_values\": 1536,\n        \"samples\": [\n          0.008627894166666665,\n          -0.003353471841666667,\n          -0.002101512608333332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "virtue_emb_diff_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dc66c078-25a3-45db-99bf-ae38a3faf869\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>mean</th>\n",
              "      <th>courage</th>\n",
              "      <th>temperance</th>\n",
              "      <th>liberality</th>\n",
              "      <th>magnificence</th>\n",
              "      <th>magnanimity</th>\n",
              "      <th>pride</th>\n",
              "      <th>patience</th>\n",
              "      <th>truthfulness</th>\n",
              "      <th>wittiness</th>\n",
              "      <th>friendliness</th>\n",
              "      <th>modesty</th>\n",
              "      <th>righteous indignation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.006090</td>\n",
              "      <td>0.003987</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>-0.002930</td>\n",
              "      <td>-0.002703</td>\n",
              "      <td>-0.003718</td>\n",
              "      <td>-0.003945</td>\n",
              "      <td>-0.000537</td>\n",
              "      <td>-0.000228</td>\n",
              "      <td>-0.000519</td>\n",
              "      <td>-0.002082</td>\n",
              "      <td>-0.005421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.003991</td>\n",
              "      <td>0.002449</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>-0.004644</td>\n",
              "      <td>-0.006362</td>\n",
              "      <td>-0.000495</td>\n",
              "      <td>0.005848</td>\n",
              "      <td>-0.002428</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>-0.002792</td>\n",
              "      <td>-0.010326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.007664</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>-0.010242</td>\n",
              "      <td>-0.007068</td>\n",
              "      <td>0.003739</td>\n",
              "      <td>-0.001336</td>\n",
              "      <td>-0.003079</td>\n",
              "      <td>0.005283</td>\n",
              "      <td>-0.000721</td>\n",
              "      <td>-0.004651</td>\n",
              "      <td>0.001940</td>\n",
              "      <td>0.002773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.010169</td>\n",
              "      <td>-0.002252</td>\n",
              "      <td>-0.006810</td>\n",
              "      <td>-0.004634</td>\n",
              "      <td>0.002451</td>\n",
              "      <td>-0.002528</td>\n",
              "      <td>-0.001454</td>\n",
              "      <td>0.007776</td>\n",
              "      <td>-0.008314</td>\n",
              "      <td>-0.002707</td>\n",
              "      <td>0.002917</td>\n",
              "      <td>-0.001030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.010463</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>-0.006486</td>\n",
              "      <td>-0.002721</td>\n",
              "      <td>-0.000434</td>\n",
              "      <td>-0.002617</td>\n",
              "      <td>-0.000494</td>\n",
              "      <td>0.004870</td>\n",
              "      <td>-0.004848</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>-0.000450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>-0.003444</td>\n",
              "      <td>-0.001823</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>-0.000353</td>\n",
              "      <td>-0.001117</td>\n",
              "      <td>0.002441</td>\n",
              "      <td>0.001912</td>\n",
              "      <td>0.004286</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>-0.000699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>0.006610</td>\n",
              "      <td>-0.000931</td>\n",
              "      <td>-0.002173</td>\n",
              "      <td>-0.005846</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>-0.002792</td>\n",
              "      <td>0.002191</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>-0.006021</td>\n",
              "      <td>-0.000557</td>\n",
              "      <td>0.004108</td>\n",
              "      <td>-0.004619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>-0.001310</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.001494</td>\n",
              "      <td>-0.001571</td>\n",
              "      <td>-0.002002</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.003828</td>\n",
              "      <td>0.000355</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>-0.001145</td>\n",
              "      <td>-0.004628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.001927</td>\n",
              "      <td>-0.003327</td>\n",
              "      <td>-0.004012</td>\n",
              "      <td>-0.003374</td>\n",
              "      <td>-0.006605</td>\n",
              "      <td>0.003321</td>\n",
              "      <td>0.007672</td>\n",
              "      <td>-0.001658</td>\n",
              "      <td>0.001891</td>\n",
              "      <td>0.002076</td>\n",
              "      <td>-0.006523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>0.003439</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>-0.000597</td>\n",
              "      <td>0.002428</td>\n",
              "      <td>0.003313</td>\n",
              "      <td>-0.003259</td>\n",
              "      <td>-0.001511</td>\n",
              "      <td>-0.003120</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>-0.000340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1536 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc66c078-25a3-45db-99bf-ae38a3faf869')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc66c078-25a3-45db-99bf-ae38a3faf869 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc66c078-25a3-45db-99bf-ae38a3faf869');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-491f0e25-941b-46ba-b86a-6ed50f2734eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-491f0e25-941b-46ba-b86a-6ed50f2734eb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-491f0e25-941b-46ba-b86a-6ed50f2734eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_524f0864-4583-4108-8b1e-d044d635fda5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('virtue_emb_diff_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_524f0864-4583-4108-8b1e-d044d635fda5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('virtue_emb_diff_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "mean   courage  temperance  liberality  magnificence  magnanimity     pride  \\\n",
              "0    -0.006090    0.003987    0.000592     -0.002930    -0.002703 -0.003718   \n",
              "1     0.001181    0.003991    0.002449      0.002218    -0.004644 -0.006362   \n",
              "2     0.007664    0.000153   -0.010242     -0.007068     0.003739 -0.001336   \n",
              "3     0.010169   -0.002252   -0.006810     -0.004634     0.002451 -0.002528   \n",
              "4     0.010463    0.000464   -0.006486     -0.002721    -0.000434 -0.002617   \n",
              "...        ...         ...         ...           ...          ...       ...   \n",
              "1531 -0.003444   -0.001823    0.001428      0.000370    -0.000353 -0.001117   \n",
              "1532  0.006610   -0.000931   -0.002173     -0.005846     0.000746 -0.002792   \n",
              "1533 -0.001310    0.001312    0.003797      0.001494    -0.001571 -0.002002   \n",
              "1534  0.003797    0.001927   -0.003327     -0.004012    -0.003374 -0.006605   \n",
              "1535  0.003439    0.000199    0.001248     -0.000597     0.002428  0.003313   \n",
              "\n",
              "mean  patience  truthfulness  wittiness  friendliness   modesty  \\\n",
              "0    -0.003945     -0.000537  -0.000228     -0.000519 -0.002082   \n",
              "1    -0.000495      0.005848  -0.002428      0.001882 -0.002792   \n",
              "2    -0.003079      0.005283  -0.000721     -0.004651  0.001940   \n",
              "3    -0.001454      0.007776  -0.008314     -0.002707  0.002917   \n",
              "4    -0.000494      0.004870  -0.004848      0.001558  0.001072   \n",
              "...        ...           ...        ...           ...       ...   \n",
              "1531  0.002441      0.001912   0.004286      0.000947  0.001488   \n",
              "1532  0.002191      0.001349  -0.006021     -0.000557  0.004108   \n",
              "1533  0.000293      0.003828   0.000355      0.002304 -0.001145   \n",
              "1534  0.003321      0.007672  -0.001658      0.001891  0.002076   \n",
              "1535 -0.003259     -0.001511  -0.003120      0.001161  0.000454   \n",
              "\n",
              "mean  righteous indignation  \n",
              "0                 -0.005421  \n",
              "1                 -0.010326  \n",
              "2                  0.002773  \n",
              "3                 -0.001030  \n",
              "4                 -0.000450  \n",
              "...                     ...  \n",
              "1531              -0.000699  \n",
              "1532              -0.004619  \n",
              "1533              -0.004628  \n",
              "1534              -0.006523  \n",
              "1535              -0.000340  \n",
              "\n",
              "[1536 rows x 12 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "virtue_emb_diff_df = pd.DataFrame(np.array(virtue_emb_diffs).T, columns=virtue_data['mean'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 511
        },
        "id": "Op4JryXbYaaV",
        "outputId": "7c46ba26-7e1e-4550-ffc5-876eea7eaca4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-10cc239e-e910-4234-b185-1e4c19503676\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>running a marathon for charity</th>\n",
              "      <th>giving my employees a day off to go to the spa</th>\n",
              "      <th>serving food to unhoused people in a soup kitchen</th>\n",
              "      <th>teaching English to refugees</th>\n",
              "      <th>cleaning up litter in a rough neighborhood</th>\n",
              "      <th>serving on a jury</th>\n",
              "      <th>running a marathon for fitness</th>\n",
              "      <th>winning millions in the lottery</th>\n",
              "      <th>mowing the lawn</th>\n",
              "      <th>sitting in a chair</th>\n",
              "      <th>...</th>\n",
              "      <th>going swing dancing</th>\n",
              "      <th>watching your favorite movie</th>\n",
              "      <th>painting a wall</th>\n",
              "      <th>listening to local news</th>\n",
              "      <th>punching myself in the face</th>\n",
              "      <th>dropping and breaking my phone</th>\n",
              "      <th>taking candy from a baby</th>\n",
              "      <th>lying on my taxes to get a higher return</th>\n",
              "      <th>punching my friend in the face</th>\n",
              "      <th>losing borrowed book</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.015271</td>\n",
              "      <td>-0.015827</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>-0.001945</td>\n",
              "      <td>0.008113</td>\n",
              "      <td>-0.016537</td>\n",
              "      <td>-0.016121</td>\n",
              "      <td>-0.015166</td>\n",
              "      <td>-0.010816</td>\n",
              "      <td>-0.006440</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.031146</td>\n",
              "      <td>-0.013596</td>\n",
              "      <td>-0.005681</td>\n",
              "      <td>-0.015399</td>\n",
              "      <td>-0.036107</td>\n",
              "      <td>-0.037175</td>\n",
              "      <td>-0.013059</td>\n",
              "      <td>-0.022622</td>\n",
              "      <td>-0.033868</td>\n",
              "      <td>-0.016733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.024870</td>\n",
              "      <td>-0.000501</td>\n",
              "      <td>-0.030625</td>\n",
              "      <td>-0.019591</td>\n",
              "      <td>0.008490</td>\n",
              "      <td>-0.032151</td>\n",
              "      <td>-0.017443</td>\n",
              "      <td>-0.010569</td>\n",
              "      <td>0.003959</td>\n",
              "      <td>-0.007232</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.017363</td>\n",
              "      <td>-0.026425</td>\n",
              "      <td>0.022816</td>\n",
              "      <td>0.014974</td>\n",
              "      <td>0.006485</td>\n",
              "      <td>-0.004561</td>\n",
              "      <td>-0.024290</td>\n",
              "      <td>-0.025695</td>\n",
              "      <td>-0.009288</td>\n",
              "      <td>-0.015275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.009293</td>\n",
              "      <td>0.004827</td>\n",
              "      <td>-0.009774</td>\n",
              "      <td>-0.018411</td>\n",
              "      <td>-0.012074</td>\n",
              "      <td>-0.012781</td>\n",
              "      <td>0.001669</td>\n",
              "      <td>0.012544</td>\n",
              "      <td>-0.006759</td>\n",
              "      <td>-0.019028</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005996</td>\n",
              "      <td>0.003505</td>\n",
              "      <td>-0.006162</td>\n",
              "      <td>0.019232</td>\n",
              "      <td>0.003962</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.001298</td>\n",
              "      <td>-0.003259</td>\n",
              "      <td>0.008841</td>\n",
              "      <td>-0.007696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.027639</td>\n",
              "      <td>-0.042665</td>\n",
              "      <td>-0.003279</td>\n",
              "      <td>-0.033269</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>-0.025562</td>\n",
              "      <td>-0.016540</td>\n",
              "      <td>-0.021508</td>\n",
              "      <td>-0.018645</td>\n",
              "      <td>-0.016064</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009940</td>\n",
              "      <td>-0.043693</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>-0.030665</td>\n",
              "      <td>-0.001913</td>\n",
              "      <td>-0.012275</td>\n",
              "      <td>-0.038043</td>\n",
              "      <td>-0.049125</td>\n",
              "      <td>-0.012182</td>\n",
              "      <td>-0.017913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.017308</td>\n",
              "      <td>-0.009868</td>\n",
              "      <td>-0.004894</td>\n",
              "      <td>-0.006939</td>\n",
              "      <td>-0.010381</td>\n",
              "      <td>-0.029023</td>\n",
              "      <td>-0.014260</td>\n",
              "      <td>-0.049753</td>\n",
              "      <td>-0.021075</td>\n",
              "      <td>-0.000484</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004378</td>\n",
              "      <td>-0.016091</td>\n",
              "      <td>-0.014326</td>\n",
              "      <td>-0.006134</td>\n",
              "      <td>-0.011438</td>\n",
              "      <td>-0.019704</td>\n",
              "      <td>-0.037816</td>\n",
              "      <td>-0.026060</td>\n",
              "      <td>-0.004273</td>\n",
              "      <td>-0.030404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>0.012994</td>\n",
              "      <td>0.010790</td>\n",
              "      <td>0.029835</td>\n",
              "      <td>0.008172</td>\n",
              "      <td>0.004347</td>\n",
              "      <td>0.032535</td>\n",
              "      <td>-0.002197</td>\n",
              "      <td>0.037810</td>\n",
              "      <td>0.006854</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>...</td>\n",
              "      <td>0.023164</td>\n",
              "      <td>0.027474</td>\n",
              "      <td>0.002895</td>\n",
              "      <td>0.027880</td>\n",
              "      <td>0.018512</td>\n",
              "      <td>0.009169</td>\n",
              "      <td>0.031337</td>\n",
              "      <td>0.011591</td>\n",
              "      <td>0.018798</td>\n",
              "      <td>0.014422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>-0.000037</td>\n",
              "      <td>-0.004301</td>\n",
              "      <td>-0.030230</td>\n",
              "      <td>-0.013507</td>\n",
              "      <td>0.007075</td>\n",
              "      <td>0.002407</td>\n",
              "      <td>0.012506</td>\n",
              "      <td>0.003590</td>\n",
              "      <td>0.016619</td>\n",
              "      <td>0.009854</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016502</td>\n",
              "      <td>0.007265</td>\n",
              "      <td>-0.011042</td>\n",
              "      <td>-0.001266</td>\n",
              "      <td>-0.033925</td>\n",
              "      <td>0.005855</td>\n",
              "      <td>0.006076</td>\n",
              "      <td>0.005691</td>\n",
              "      <td>-0.007774</td>\n",
              "      <td>0.001638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>-0.016975</td>\n",
              "      <td>-0.003616</td>\n",
              "      <td>-0.012274</td>\n",
              "      <td>-0.024230</td>\n",
              "      <td>-0.013740</td>\n",
              "      <td>-0.019498</td>\n",
              "      <td>-0.019804</td>\n",
              "      <td>-0.026237</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>-0.005428</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028427</td>\n",
              "      <td>0.006651</td>\n",
              "      <td>-0.005717</td>\n",
              "      <td>-0.003303</td>\n",
              "      <td>-0.002740</td>\n",
              "      <td>-0.000981</td>\n",
              "      <td>0.005789</td>\n",
              "      <td>-0.000544</td>\n",
              "      <td>-0.010968</td>\n",
              "      <td>-0.018525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>-0.017334</td>\n",
              "      <td>0.005476</td>\n",
              "      <td>-0.010800</td>\n",
              "      <td>-0.004593</td>\n",
              "      <td>-0.015697</td>\n",
              "      <td>-0.010781</td>\n",
              "      <td>0.000584</td>\n",
              "      <td>-0.002570</td>\n",
              "      <td>0.020303</td>\n",
              "      <td>0.009205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002495</td>\n",
              "      <td>0.011179</td>\n",
              "      <td>0.020278</td>\n",
              "      <td>-0.018675</td>\n",
              "      <td>0.024811</td>\n",
              "      <td>-0.012427</td>\n",
              "      <td>-0.013941</td>\n",
              "      <td>0.005864</td>\n",
              "      <td>-0.001617</td>\n",
              "      <td>0.010319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>-0.015058</td>\n",
              "      <td>-0.022813</td>\n",
              "      <td>-0.002302</td>\n",
              "      <td>-0.019007</td>\n",
              "      <td>-0.020643</td>\n",
              "      <td>-0.016396</td>\n",
              "      <td>-0.013174</td>\n",
              "      <td>-0.023159</td>\n",
              "      <td>-0.002933</td>\n",
              "      <td>-0.012496</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023447</td>\n",
              "      <td>-0.018674</td>\n",
              "      <td>-0.008183</td>\n",
              "      <td>-0.014749</td>\n",
              "      <td>-0.012220</td>\n",
              "      <td>-0.009839</td>\n",
              "      <td>-0.009164</td>\n",
              "      <td>-0.043863</td>\n",
              "      <td>-0.022513</td>\n",
              "      <td>-0.034252</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1536 rows Ã— 52 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-10cc239e-e910-4234-b185-1e4c19503676')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-10cc239e-e910-4234-b185-1e4c19503676 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-10cc239e-e910-4234-b185-1e4c19503676');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8908b152-31ed-4df1-9a93-545f44d99205\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8908b152-31ed-4df1-9a93-545f44d99205')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8908b152-31ed-4df1-9a93-545f44d99205 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      running a marathon for charity  \\\n",
              "0                          -0.015271   \n",
              "1                          -0.024870   \n",
              "2                          -0.009293   \n",
              "3                          -0.027639   \n",
              "4                          -0.017308   \n",
              "...                              ...   \n",
              "1531                        0.012994   \n",
              "1532                       -0.000037   \n",
              "1533                       -0.016975   \n",
              "1534                       -0.017334   \n",
              "1535                       -0.015058   \n",
              "\n",
              "      giving my employees a day off to go to the spa  \\\n",
              "0                                          -0.015827   \n",
              "1                                          -0.000501   \n",
              "2                                           0.004827   \n",
              "3                                          -0.042665   \n",
              "4                                          -0.009868   \n",
              "...                                              ...   \n",
              "1531                                        0.010790   \n",
              "1532                                       -0.004301   \n",
              "1533                                       -0.003616   \n",
              "1534                                        0.005476   \n",
              "1535                                       -0.022813   \n",
              "\n",
              "      serving food to unhoused people in a soup kitchen  \\\n",
              "0                                              0.000996   \n",
              "1                                             -0.030625   \n",
              "2                                             -0.009774   \n",
              "3                                             -0.003279   \n",
              "4                                             -0.004894   \n",
              "...                                                 ...   \n",
              "1531                                           0.029835   \n",
              "1532                                          -0.030230   \n",
              "1533                                          -0.012274   \n",
              "1534                                          -0.010800   \n",
              "1535                                          -0.002302   \n",
              "\n",
              "      teaching English to refugees  \\\n",
              "0                        -0.001945   \n",
              "1                        -0.019591   \n",
              "2                        -0.018411   \n",
              "3                        -0.033269   \n",
              "4                        -0.006939   \n",
              "...                            ...   \n",
              "1531                      0.008172   \n",
              "1532                     -0.013507   \n",
              "1533                     -0.024230   \n",
              "1534                     -0.004593   \n",
              "1535                     -0.019007   \n",
              "\n",
              "      cleaning up litter in a rough neighborhood  serving on a jury  \\\n",
              "0                                       0.008113          -0.016537   \n",
              "1                                       0.008490          -0.032151   \n",
              "2                                      -0.012074          -0.012781   \n",
              "3                                       0.003670          -0.025562   \n",
              "4                                      -0.010381          -0.029023   \n",
              "...                                          ...                ...   \n",
              "1531                                    0.004347           0.032535   \n",
              "1532                                    0.007075           0.002407   \n",
              "1533                                   -0.013740          -0.019498   \n",
              "1534                                   -0.015697          -0.010781   \n",
              "1535                                   -0.020643          -0.016396   \n",
              "\n",
              "      running a marathon for fitness  winning millions in the lottery  \\\n",
              "0                          -0.016121                        -0.015166   \n",
              "1                          -0.017443                        -0.010569   \n",
              "2                           0.001669                         0.012544   \n",
              "3                          -0.016540                        -0.021508   \n",
              "4                          -0.014260                        -0.049753   \n",
              "...                              ...                              ...   \n",
              "1531                       -0.002197                         0.037810   \n",
              "1532                        0.012506                         0.003590   \n",
              "1533                       -0.019804                        -0.026237   \n",
              "1534                        0.000584                        -0.002570   \n",
              "1535                       -0.013174                        -0.023159   \n",
              "\n",
              "      mowing the lawn  sitting in a chair  ...  going swing dancing  \\\n",
              "0           -0.010816           -0.006440  ...            -0.031146   \n",
              "1            0.003959           -0.007232  ...            -0.017363   \n",
              "2           -0.006759           -0.019028  ...             0.005996   \n",
              "3           -0.018645           -0.016064  ...            -0.009940   \n",
              "4           -0.021075           -0.000484  ...            -0.004378   \n",
              "...               ...                 ...  ...                  ...   \n",
              "1531         0.006854            0.003165  ...             0.023164   \n",
              "1532         0.016619            0.009854  ...             0.016502   \n",
              "1533         0.001254           -0.005428  ...            -0.028427   \n",
              "1534         0.020303            0.009205  ...             0.002495   \n",
              "1535        -0.002933           -0.012496  ...            -0.023447   \n",
              "\n",
              "      watching your favorite movie  painting a wall  listening to local news  \\\n",
              "0                        -0.013596        -0.005681                -0.015399   \n",
              "1                        -0.026425         0.022816                 0.014974   \n",
              "2                         0.003505        -0.006162                 0.019232   \n",
              "3                        -0.043693         0.004700                -0.030665   \n",
              "4                        -0.016091        -0.014326                -0.006134   \n",
              "...                            ...              ...                      ...   \n",
              "1531                      0.027474         0.002895                 0.027880   \n",
              "1532                      0.007265        -0.011042                -0.001266   \n",
              "1533                      0.006651        -0.005717                -0.003303   \n",
              "1534                      0.011179         0.020278                -0.018675   \n",
              "1535                     -0.018674        -0.008183                -0.014749   \n",
              "\n",
              "      punching myself in the face  dropping and breaking my phone  \\\n",
              "0                       -0.036107                       -0.037175   \n",
              "1                        0.006485                       -0.004561   \n",
              "2                        0.003962                        0.001783   \n",
              "3                       -0.001913                       -0.012275   \n",
              "4                       -0.011438                       -0.019704   \n",
              "...                           ...                             ...   \n",
              "1531                     0.018512                        0.009169   \n",
              "1532                    -0.033925                        0.005855   \n",
              "1533                    -0.002740                       -0.000981   \n",
              "1534                     0.024811                       -0.012427   \n",
              "1535                    -0.012220                       -0.009839   \n",
              "\n",
              "      taking candy from a baby  lying on my taxes to get a higher return  \\\n",
              "0                    -0.013059                                 -0.022622   \n",
              "1                    -0.024290                                 -0.025695   \n",
              "2                     0.001298                                 -0.003259   \n",
              "3                    -0.038043                                 -0.049125   \n",
              "4                    -0.037816                                 -0.026060   \n",
              "...                        ...                                       ...   \n",
              "1531                  0.031337                                  0.011591   \n",
              "1532                  0.006076                                  0.005691   \n",
              "1533                  0.005789                                 -0.000544   \n",
              "1534                 -0.013941                                  0.005864   \n",
              "1535                 -0.009164                                 -0.043863   \n",
              "\n",
              "      punching my friend in the face  losing borrowed book  \n",
              "0                          -0.033868             -0.016733  \n",
              "1                          -0.009288             -0.015275  \n",
              "2                           0.008841             -0.007696  \n",
              "3                          -0.012182             -0.017913  \n",
              "4                          -0.004273             -0.030404  \n",
              "...                              ...                   ...  \n",
              "1531                        0.018798              0.014422  \n",
              "1532                       -0.007774              0.001638  \n",
              "1533                       -0.010968             -0.018525  \n",
              "1534                       -0.001617              0.010319  \n",
              "1535                       -0.022513             -0.034252  \n",
              "\n",
              "[1536 rows x 52 columns]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# compute df of action embeddings so i don't have to keep calling! sheesh\n",
        "action_embeddings = []\n",
        "\n",
        "for a in action_list_all:\n",
        "  this_em = getEmbeddings(a)[\"data\"][0][\"embedding\"]\n",
        "  action_embeddings.append(this_em)\n",
        "\n",
        "action_embeddings_df = pd.DataFrame(action_embeddings, index=action_list_all)\n",
        "action_embeddings_df.transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MVbuJG66i93"
      },
      "outputs": [],
      "source": [
        "virtues_list = list(virtue_emb_diff_df.columns.values)\n",
        "\n",
        "def get_projections(stim_list, virtues):\n",
        "  #construct dataframe to save items\n",
        "  projection_df_ctl = pd.DataFrame(index=action_list_all, columns=virtues_list, data=0)\n",
        "\n",
        "  for a in action_list_all:\n",
        "    for virtue in virtues_list:\n",
        "      this_em = getEmbeddings(a)[\"data\"][0][\"embedding\"]\n",
        "      projection_temp = np.inner(np.array(this_em),np.array(virtue_emb_diff_df[virtue]))\n",
        "      projection_df_ctl.loc[a, virtue] = projection_temp\n",
        "\n",
        "  return projection_df_ctl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "BhWxyIKx-irL",
        "outputId": "829eb8fb-2f82-4531-ea31-bb4cbd1f4549"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"virtue_emb_mean_df\",\n  \"rows\": 1536,\n  \"fields\": [\n    {\n      \"column\": \"courage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02551301436135368,\n        \"min\": -0.6900963,\n        \"max\": 0.20491165,\n        \"num_unique_values\": 1417,\n        \"samples\": [\n          -0.0061840345,\n          0.0063412557,\n          -0.02142139\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025512349755059453,\n        \"min\": -0.6775013,\n        \"max\": 0.1895544,\n        \"num_unique_values\": 1426,\n        \"samples\": [\n          0.0046221325,\n          -0.007842196,\n          -0.002366411\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liberality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025512352845558174,\n        \"min\": -0.6742681,\n        \"max\": 0.16945189,\n        \"num_unique_values\": 1420,\n        \"samples\": [\n          0.008212665,\n          -0.017946195,\n          -0.032021098\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnificence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513054443564782,\n        \"min\": -0.6868657,\n        \"max\": 0.16577688,\n        \"num_unique_values\": 1419,\n        \"samples\": [\n          8.265477e-05,\n          0.023241114,\n          -0.007782392\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnanimity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513130020846687,\n        \"min\": -0.67204237,\n        \"max\": 0.18505043,\n        \"num_unique_values\": 1428,\n        \"samples\": [\n          -0.018328674,\n          -0.0095238555,\n          -0.034079667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pride\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513482159077117,\n        \"min\": -0.6949289,\n        \"max\": 0.19016917,\n        \"num_unique_values\": 1413,\n        \"samples\": [\n          -0.030325117,\n          -0.0053868694,\n          -0.00402471\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"patience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513313683963153,\n        \"min\": -0.6908585,\n        \"max\": 0.19752388,\n        \"num_unique_values\": 1414,\n        \"samples\": [\n          -0.022130381,\n          0.00065528427,\n          -0.006674085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"truthfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025512779954808232,\n        \"min\": -0.686021,\n        \"max\": 0.17760368,\n        \"num_unique_values\": 1393,\n        \"samples\": [\n          -0.019940292,\n          -0.023176745,\n          0.001433501\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wittiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02551261411464276,\n        \"min\": -0.69911015,\n        \"max\": 0.17242722,\n        \"num_unique_values\": 1424,\n        \"samples\": [\n          -0.018735703,\n          -0.0071176975,\n          0.02174036\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"friendliness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025511509333260216,\n        \"min\": -0.6952347,\n        \"max\": 0.16809563,\n        \"num_unique_values\": 1404,\n        \"samples\": [\n          0.017345162,\n          -0.012703305,\n          0.023264214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"modesty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02551250311568573,\n        \"min\": -0.67740786,\n        \"max\": 0.19643947,\n        \"num_unique_values\": 1418,\n        \"samples\": [\n          0.0057292553,\n          -0.011038709,\n          0.011988424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"righteous indignation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.025513492612569477,\n        \"min\": -0.6799826,\n        \"max\": 0.20757252,\n        \"num_unique_values\": 1416,\n        \"samples\": [\n          -0.010254252,\n          -0.020296803,\n          0.012754966\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "virtue_emb_mean_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-03e06865-8738-4d19-9948-845fa7b5b453\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>courage</th>\n",
              "      <th>temperance</th>\n",
              "      <th>liberality</th>\n",
              "      <th>magnificence</th>\n",
              "      <th>magnanimity</th>\n",
              "      <th>pride</th>\n",
              "      <th>patience</th>\n",
              "      <th>truthfulness</th>\n",
              "      <th>wittiness</th>\n",
              "      <th>friendliness</th>\n",
              "      <th>modesty</th>\n",
              "      <th>righteous indignation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.012774</td>\n",
              "      <td>-0.004112</td>\n",
              "      <td>-0.005600</td>\n",
              "      <td>-0.008041</td>\n",
              "      <td>-0.004328</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>-0.004526</td>\n",
              "      <td>-0.010826</td>\n",
              "      <td>-0.008253</td>\n",
              "      <td>0.004982</td>\n",
              "      <td>-0.002438</td>\n",
              "      <td>-0.011200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.030501</td>\n",
              "      <td>-0.012391</td>\n",
              "      <td>-0.014227</td>\n",
              "      <td>0.007849</td>\n",
              "      <td>-0.016999</td>\n",
              "      <td>-0.023082</td>\n",
              "      <td>0.000872</td>\n",
              "      <td>-0.012531</td>\n",
              "      <td>-0.008199</td>\n",
              "      <td>-0.001430</td>\n",
              "      <td>-0.011328</td>\n",
              "      <td>-0.016460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.017832</td>\n",
              "      <td>0.007030</td>\n",
              "      <td>-0.003365</td>\n",
              "      <td>0.018468</td>\n",
              "      <td>0.019102</td>\n",
              "      <td>0.008849</td>\n",
              "      <td>0.003711</td>\n",
              "      <td>-0.005146</td>\n",
              "      <td>0.001512</td>\n",
              "      <td>0.005881</td>\n",
              "      <td>0.004673</td>\n",
              "      <td>-0.005365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.012879</td>\n",
              "      <td>-0.033489</td>\n",
              "      <td>-0.040953</td>\n",
              "      <td>-0.037228</td>\n",
              "      <td>-0.030091</td>\n",
              "      <td>-0.031573</td>\n",
              "      <td>-0.029560</td>\n",
              "      <td>-0.020114</td>\n",
              "      <td>-0.007872</td>\n",
              "      <td>-0.025036</td>\n",
              "      <td>-0.017756</td>\n",
              "      <td>-0.025483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.009165</td>\n",
              "      <td>-0.017670</td>\n",
              "      <td>-0.018043</td>\n",
              "      <td>0.005353</td>\n",
              "      <td>-0.004874</td>\n",
              "      <td>0.006235</td>\n",
              "      <td>0.006512</td>\n",
              "      <td>-0.005239</td>\n",
              "      <td>-0.004410</td>\n",
              "      <td>0.007567</td>\n",
              "      <td>-0.022807</td>\n",
              "      <td>-0.012689</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>0.053979</td>\n",
              "      <td>0.035099</td>\n",
              "      <td>0.041091</td>\n",
              "      <td>0.056267</td>\n",
              "      <td>0.086881</td>\n",
              "      <td>0.045696</td>\n",
              "      <td>0.019887</td>\n",
              "      <td>0.015701</td>\n",
              "      <td>0.057476</td>\n",
              "      <td>0.037464</td>\n",
              "      <td>0.045173</td>\n",
              "      <td>0.056259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>0.023413</td>\n",
              "      <td>0.022259</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>-0.000325</td>\n",
              "      <td>-0.001485</td>\n",
              "      <td>0.007445</td>\n",
              "      <td>0.015902</td>\n",
              "      <td>-0.000435</td>\n",
              "      <td>0.004771</td>\n",
              "      <td>0.015217</td>\n",
              "      <td>-0.008217</td>\n",
              "      <td>-0.004270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>-0.020937</td>\n",
              "      <td>-0.009908</td>\n",
              "      <td>-0.025233</td>\n",
              "      <td>-0.024567</td>\n",
              "      <td>-0.031828</td>\n",
              "      <td>-0.028843</td>\n",
              "      <td>-0.008729</td>\n",
              "      <td>-0.001844</td>\n",
              "      <td>-0.019537</td>\n",
              "      <td>-0.027110</td>\n",
              "      <td>-0.005640</td>\n",
              "      <td>-0.009368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>0.002782</td>\n",
              "      <td>-0.008064</td>\n",
              "      <td>0.018306</td>\n",
              "      <td>-0.003384</td>\n",
              "      <td>0.002696</td>\n",
              "      <td>0.020715</td>\n",
              "      <td>0.016720</td>\n",
              "      <td>0.016369</td>\n",
              "      <td>0.004580</td>\n",
              "      <td>-0.001856</td>\n",
              "      <td>0.002448</td>\n",
              "      <td>0.010565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>-0.012165</td>\n",
              "      <td>-0.027666</td>\n",
              "      <td>-0.044437</td>\n",
              "      <td>-0.028425</td>\n",
              "      <td>-0.040402</td>\n",
              "      <td>-0.015969</td>\n",
              "      <td>-0.020732</td>\n",
              "      <td>-0.031937</td>\n",
              "      <td>-0.050585</td>\n",
              "      <td>-0.044194</td>\n",
              "      <td>-0.028602</td>\n",
              "      <td>-0.021395</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1536 rows Ã— 12 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03e06865-8738-4d19-9948-845fa7b5b453')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03e06865-8738-4d19-9948-845fa7b5b453 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03e06865-8738-4d19-9948-845fa7b5b453');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-010fc010-5b59-4382-8d11-e105b0c2e895\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-010fc010-5b59-4382-8d11-e105b0c2e895')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-010fc010-5b59-4382-8d11-e105b0c2e895 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_be96541f-df3c-44f4-82f8-8a6088c0e951\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('virtue_emb_mean_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_be96541f-df3c-44f4-82f8-8a6088c0e951 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('virtue_emb_mean_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "       courage  temperance  liberality  magnificence  magnanimity     pride  \\\n",
              "0    -0.012774   -0.004112   -0.005600     -0.008041    -0.004328  0.000002   \n",
              "1    -0.030501   -0.012391   -0.014227      0.007849    -0.016999 -0.023082   \n",
              "2    -0.017832    0.007030   -0.003365      0.018468     0.019102  0.008849   \n",
              "3    -0.012879   -0.033489   -0.040953     -0.037228    -0.030091 -0.031573   \n",
              "4     0.009165   -0.017670   -0.018043      0.005353    -0.004874  0.006235   \n",
              "...        ...         ...         ...           ...          ...       ...   \n",
              "1531  0.053979    0.035099    0.041091      0.056267     0.086881  0.045696   \n",
              "1532  0.023413    0.022259    0.000922     -0.000325    -0.001485  0.007445   \n",
              "1533 -0.020937   -0.009908   -0.025233     -0.024567    -0.031828 -0.028843   \n",
              "1534  0.002782   -0.008064    0.018306     -0.003384     0.002696  0.020715   \n",
              "1535 -0.012165   -0.027666   -0.044437     -0.028425    -0.040402 -0.015969   \n",
              "\n",
              "      patience  truthfulness  wittiness  friendliness   modesty  \\\n",
              "0    -0.004526     -0.010826  -0.008253      0.004982 -0.002438   \n",
              "1     0.000872     -0.012531  -0.008199     -0.001430 -0.011328   \n",
              "2     0.003711     -0.005146   0.001512      0.005881  0.004673   \n",
              "3    -0.029560     -0.020114  -0.007872     -0.025036 -0.017756   \n",
              "4     0.006512     -0.005239  -0.004410      0.007567 -0.022807   \n",
              "...        ...           ...        ...           ...       ...   \n",
              "1531  0.019887      0.015701   0.057476      0.037464  0.045173   \n",
              "1532  0.015902     -0.000435   0.004771      0.015217 -0.008217   \n",
              "1533 -0.008729     -0.001844  -0.019537     -0.027110 -0.005640   \n",
              "1534  0.016720      0.016369   0.004580     -0.001856  0.002448   \n",
              "1535 -0.020732     -0.031937  -0.050585     -0.044194 -0.028602   \n",
              "\n",
              "      righteous indignation  \n",
              "0                 -0.011200  \n",
              "1                 -0.016460  \n",
              "2                 -0.005365  \n",
              "3                 -0.025483  \n",
              "4                 -0.012689  \n",
              "...                     ...  \n",
              "1531               0.056259  \n",
              "1532              -0.004270  \n",
              "1533              -0.009368  \n",
              "1534               0.010565  \n",
              "1535              -0.021395  \n",
              "\n",
              "[1536 rows x 12 columns]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "action_embeddings_df = pd.DataFrame(action_embeddings, index=action_list_all).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60_9_thXpo7r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuF0Pr2XLpO1"
      },
      "outputs": [],
      "source": [
        "#compare embeddings of mean virtues versus embeddings of extreme differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMO76rBtL_22"
      },
      "outputs": [],
      "source": [
        "#compare graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPJL7lrtMgUz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ref2L9NU9liQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "daVUA-Tn9mS8",
        "outputId": "4253713c-4fb2-4c8a-86f9-9c29eb735332"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"action_df_all_virtues\",\n  \"rows\": 52,\n  \"fields\": [\n    {\n      \"column\": \"courage\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001980936304218572,\n        \"min\": -0.005496594694482157,\n        \"max\": 0.004677712813713006,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -0.0029089757441771277,\n          -0.00018148195842774455,\n          -0.0009815827596111031\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0008813970343672079,\n        \"min\": -0.0030439108111850544,\n        \"max\": 0.0011752608459000514,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -0.0003924198906519197,\n          -0.0016682246615146415,\n          -0.0009872326902142878\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liberality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002306766320509046,\n        \"min\": -0.0020397117741636924,\n        \"max\": 0.008972062755004732,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.003926962013771734,\n          0.0010958140570986696,\n          0.002959615056118981\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnificence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0020978359291335337,\n        \"min\": -0.004569986912193994,\n        \"max\": 0.0030278656589453126,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.0015574593691750262,\n          -0.003567701719228158,\n          0.0008403648791313854\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"magnanimity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0015053286866696766,\n        \"min\": -0.0005028615718418944,\n        \"max\": 0.0071823229369204706,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.0035114148594479607,\n          0.003232705028076275,\n          0.0025046530490685655\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pride\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0015555384037023302,\n        \"min\": -0.003676357316592195,\n        \"max\": 0.0034535628046446654,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -0.00019586807816629297,\n          -0.0010857528324325987,\n          0.0010762500895866041\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"patience\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0017861212804545798,\n        \"min\": -0.0029335769498130752,\n        \"max\": 0.004791395597982036,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.0008649410967151465,\n          0.003342385936876127,\n          -0.00031600398021755256\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"truthfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0020851127566492724,\n        \"min\": -0.004476737120573791,\n        \"max\": 0.0048224367421212284,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -1.078993519232829e-05,\n          0.0029177050082337622,\n          -0.002520705288492772\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wittiness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001517543441864433,\n        \"min\": -0.008038353728893785,\n        \"max\": -0.00042975778739060217,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -0.0032137494690609022,\n          -0.00042975778739060217,\n          -0.0015093466536148336\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"friendliness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001253380664620193,\n        \"min\": -0.0038922129564737088,\n        \"max\": 0.001366983419603281,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -0.0029372079308316633,\n          0.0008806714533856453,\n          -0.0007825260643101481\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"modesty\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0011886973587348628,\n        \"min\": -0.0029072506490409227,\n        \"max\": 0.003421336532309062,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.0007015949582708733,\n          0.0023059973278309947,\n          0.0025465749893289515\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"righteous indignation\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002075388457112047,\n        \"min\": -0.00395968799381066,\n        \"max\": 0.005639693403151062,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.0003978410644108956,\n          0.0007643100724678679,\n          0.0028118190090478633\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "action_df_all_virtues"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-eaef2a94-8e38-4ea2-b636-47c364c4f9ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>courage</th>\n",
              "      <th>temperance</th>\n",
              "      <th>liberality</th>\n",
              "      <th>magnificence</th>\n",
              "      <th>magnanimity</th>\n",
              "      <th>pride</th>\n",
              "      <th>patience</th>\n",
              "      <th>truthfulness</th>\n",
              "      <th>wittiness</th>\n",
              "      <th>friendliness</th>\n",
              "      <th>modesty</th>\n",
              "      <th>righteous indignation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>running a marathon for charity</th>\n",
              "      <td>0.001065</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>-0.002761</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.001662</td>\n",
              "      <td>-0.004166</td>\n",
              "      <td>-0.001892</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>-0.002514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>giving my employees a day off to go to the spa</th>\n",
              "      <td>-0.002030</td>\n",
              "      <td>-0.001505</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>-0.001397</td>\n",
              "      <td>0.003993</td>\n",
              "      <td>0.002242</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>-0.000529</td>\n",
              "      <td>-0.003010</td>\n",
              "      <td>-0.002290</td>\n",
              "      <td>0.000812</td>\n",
              "      <td>0.002880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>serving food to unhoused people in a soup kitchen</th>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>-0.000093</td>\n",
              "      <td>-0.000363</td>\n",
              "      <td>0.003499</td>\n",
              "      <td>-0.001246</td>\n",
              "      <td>-0.001621</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>-0.004210</td>\n",
              "      <td>-0.001834</td>\n",
              "      <td>0.000553</td>\n",
              "      <td>-0.000276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>teaching English to refugees</th>\n",
              "      <td>-0.001724</td>\n",
              "      <td>0.001175</td>\n",
              "      <td>-0.000701</td>\n",
              "      <td>-0.004318</td>\n",
              "      <td>0.003527</td>\n",
              "      <td>-0.000773</td>\n",
              "      <td>-0.000387</td>\n",
              "      <td>0.001634</td>\n",
              "      <td>-0.001717</td>\n",
              "      <td>-0.001550</td>\n",
              "      <td>-0.000706</td>\n",
              "      <td>-0.000279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cleaning up litter in a rough neighborhood</th>\n",
              "      <td>-0.002128</td>\n",
              "      <td>-0.000536</td>\n",
              "      <td>0.003132</td>\n",
              "      <td>0.002127</td>\n",
              "      <td>0.002106</td>\n",
              "      <td>-0.001418</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.002503</td>\n",
              "      <td>-0.001581</td>\n",
              "      <td>-0.002973</td>\n",
              "      <td>-0.000766</td>\n",
              "      <td>-0.000445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>serving on a jury</th>\n",
              "      <td>0.000879</td>\n",
              "      <td>-0.000618</td>\n",
              "      <td>-0.001575</td>\n",
              "      <td>0.001746</td>\n",
              "      <td>0.004394</td>\n",
              "      <td>-0.001545</td>\n",
              "      <td>0.002442</td>\n",
              "      <td>0.004226</td>\n",
              "      <td>-0.002881</td>\n",
              "      <td>-0.003009</td>\n",
              "      <td>0.000818</td>\n",
              "      <td>0.002469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running a marathon for fitness</th>\n",
              "      <td>0.000138</td>\n",
              "      <td>-0.000894</td>\n",
              "      <td>-0.000521</td>\n",
              "      <td>-0.002778</td>\n",
              "      <td>0.002456</td>\n",
              "      <td>-0.003430</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>0.002297</td>\n",
              "      <td>-0.004158</td>\n",
              "      <td>-0.001956</td>\n",
              "      <td>0.000407</td>\n",
              "      <td>-0.003648</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>winning millions in the lottery</th>\n",
              "      <td>-0.003691</td>\n",
              "      <td>-0.000883</td>\n",
              "      <td>0.003697</td>\n",
              "      <td>0.002264</td>\n",
              "      <td>0.001622</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>0.001834</td>\n",
              "      <td>-0.001502</td>\n",
              "      <td>-0.002951</td>\n",
              "      <td>-0.002152</td>\n",
              "      <td>0.000540</td>\n",
              "      <td>0.000939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mowing the lawn</th>\n",
              "      <td>-0.003585</td>\n",
              "      <td>-0.001994</td>\n",
              "      <td>-0.000487</td>\n",
              "      <td>0.001966</td>\n",
              "      <td>-0.000344</td>\n",
              "      <td>-0.001966</td>\n",
              "      <td>0.004155</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>-0.003177</td>\n",
              "      <td>-0.003892</td>\n",
              "      <td>-0.002907</td>\n",
              "      <td>-0.001609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sitting in a chair</th>\n",
              "      <td>0.004678</td>\n",
              "      <td>-0.001247</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>-0.001456</td>\n",
              "      <td>0.001339</td>\n",
              "      <td>-0.003676</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>0.004822</td>\n",
              "      <td>-0.008038</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.001746</td>\n",
              "      <td>-0.003960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>falling off a bridge</th>\n",
              "      <td>-0.001575</td>\n",
              "      <td>-0.000185</td>\n",
              "      <td>0.002046</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.002984</td>\n",
              "      <td>-0.001238</td>\n",
              "      <td>-0.001549</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>-0.001243</td>\n",
              "      <td>-0.002658</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>-0.000521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>losing my wallet</th>\n",
              "      <td>-0.002874</td>\n",
              "      <td>-0.002217</td>\n",
              "      <td>0.003113</td>\n",
              "      <td>0.001517</td>\n",
              "      <td>0.002009</td>\n",
              "      <td>-0.000192</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>-0.001105</td>\n",
              "      <td>-0.003224</td>\n",
              "      <td>-0.000231</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.001662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stealing someone's wallet</th>\n",
              "      <td>0.001444</td>\n",
              "      <td>-0.000769</td>\n",
              "      <td>-0.001473</td>\n",
              "      <td>-0.004314</td>\n",
              "      <td>0.003732</td>\n",
              "      <td>-0.002385</td>\n",
              "      <td>0.000811</td>\n",
              "      <td>0.001837</td>\n",
              "      <td>-0.006782</td>\n",
              "      <td>-0.000130</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>-0.000813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>revealing state secrets for personal gain</th>\n",
              "      <td>-0.003633</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.001822</td>\n",
              "      <td>-0.003261</td>\n",
              "      <td>0.002278</td>\n",
              "      <td>-0.001828</td>\n",
              "      <td>0.002085</td>\n",
              "      <td>0.000266</td>\n",
              "      <td>-0.002448</td>\n",
              "      <td>-0.001508</td>\n",
              "      <td>-0.000091</td>\n",
              "      <td>-0.003250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pushing a girl off a bridge</th>\n",
              "      <td>0.003256</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.002553</td>\n",
              "      <td>0.001404</td>\n",
              "      <td>0.002636</td>\n",
              "      <td>-0.000912</td>\n",
              "      <td>-0.002302</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>-0.002702</td>\n",
              "      <td>-0.000991</td>\n",
              "      <td>0.002746</td>\n",
              "      <td>0.000635</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forgetting my mom's birthday</th>\n",
              "      <td>-0.002419</td>\n",
              "      <td>-0.002305</td>\n",
              "      <td>0.004786</td>\n",
              "      <td>-0.000934</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>-0.000744</td>\n",
              "      <td>0.002510</td>\n",
              "      <td>-0.003649</td>\n",
              "      <td>-0.003947</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>0.001795</td>\n",
              "      <td>-0.001656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>working as a suicide bomber</th>\n",
              "      <td>-0.001396</td>\n",
              "      <td>-0.000864</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>-0.001714</td>\n",
              "      <td>0.002454</td>\n",
              "      <td>-0.001572</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>0.003036</td>\n",
              "      <td>-0.002188</td>\n",
              "      <td>-0.001724</td>\n",
              "      <td>-0.000297</td>\n",
              "      <td>-0.002208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thinking about harming myself</th>\n",
              "      <td>0.000457</td>\n",
              "      <td>-0.000864</td>\n",
              "      <td>0.004478</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.002117</td>\n",
              "      <td>0.001742</td>\n",
              "      <td>-0.002127</td>\n",
              "      <td>-0.002036</td>\n",
              "      <td>-0.003801</td>\n",
              "      <td>-0.000441</td>\n",
              "      <td>0.002254</td>\n",
              "      <td>-0.000331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>walking dogs for a local animal shelter</th>\n",
              "      <td>-0.001682</td>\n",
              "      <td>-0.000821</td>\n",
              "      <td>0.003269</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.002644</td>\n",
              "      <td>-0.001314</td>\n",
              "      <td>0.000623</td>\n",
              "      <td>0.001158</td>\n",
              "      <td>-0.003612</td>\n",
              "      <td>-0.002474</td>\n",
              "      <td>-0.000459</td>\n",
              "      <td>-0.001532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>donating money to charity</th>\n",
              "      <td>-0.002909</td>\n",
              "      <td>-0.000392</td>\n",
              "      <td>0.003927</td>\n",
              "      <td>0.001557</td>\n",
              "      <td>0.003511</td>\n",
              "      <td>-0.000196</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>-0.003214</td>\n",
              "      <td>-0.002937</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.000398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>helping someone change their flat tire on the road</th>\n",
              "      <td>-0.000430</td>\n",
              "      <td>-0.000433</td>\n",
              "      <td>0.001954</td>\n",
              "      <td>-0.000370</td>\n",
              "      <td>0.004309</td>\n",
              "      <td>-0.001781</td>\n",
              "      <td>0.001068</td>\n",
              "      <td>0.002871</td>\n",
              "      <td>-0.001828</td>\n",
              "      <td>-0.001254</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>0.001228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>volunteering at a voting location</th>\n",
              "      <td>-0.001733</td>\n",
              "      <td>-0.000397</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.001825</td>\n",
              "      <td>0.007114</td>\n",
              "      <td>0.000915</td>\n",
              "      <td>0.000546</td>\n",
              "      <td>0.002766</td>\n",
              "      <td>-0.001058</td>\n",
              "      <td>-0.002830</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.005640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rescuing refugees from a sinking life raft</th>\n",
              "      <td>-0.001036</td>\n",
              "      <td>-0.000017</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>-0.000511</td>\n",
              "      <td>0.004909</td>\n",
              "      <td>-0.000741</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>-0.004567</td>\n",
              "      <td>-0.002756</td>\n",
              "      <td>0.000629</td>\n",
              "      <td>0.001529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>paying taxes</th>\n",
              "      <td>-0.000918</td>\n",
              "      <td>-0.001757</td>\n",
              "      <td>0.002312</td>\n",
              "      <td>-0.004570</td>\n",
              "      <td>0.002886</td>\n",
              "      <td>-0.001663</td>\n",
              "      <td>0.003150</td>\n",
              "      <td>0.002365</td>\n",
              "      <td>-0.003865</td>\n",
              "      <td>-0.000798</td>\n",
              "      <td>0.003421</td>\n",
              "      <td>-0.001788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hiking in a beautiful place</th>\n",
              "      <td>0.002027</td>\n",
              "      <td>-0.000786</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>0.002255</td>\n",
              "      <td>0.004221</td>\n",
              "      <td>-0.000798</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.001649</td>\n",
              "      <td>-0.004918</td>\n",
              "      <td>-0.003182</td>\n",
              "      <td>0.001377</td>\n",
              "      <td>0.001017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>taking a day off to go to the spa</th>\n",
              "      <td>0.001262</td>\n",
              "      <td>-0.002093</td>\n",
              "      <td>0.001928</td>\n",
              "      <td>-0.002538</td>\n",
              "      <td>0.005837</td>\n",
              "      <td>0.003454</td>\n",
              "      <td>0.000603</td>\n",
              "      <td>-0.000705</td>\n",
              "      <td>-0.003872</td>\n",
              "      <td>-0.002907</td>\n",
              "      <td>0.002385</td>\n",
              "      <td>0.003889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>opening a door</th>\n",
              "      <td>-0.001295</td>\n",
              "      <td>-0.001264</td>\n",
              "      <td>0.004155</td>\n",
              "      <td>0.002547</td>\n",
              "      <td>0.003347</td>\n",
              "      <td>-0.001138</td>\n",
              "      <td>0.000904</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>-0.003139</td>\n",
              "      <td>-0.002397</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.000102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thinking about the day's plans</th>\n",
              "      <td>-0.002188</td>\n",
              "      <td>-0.002371</td>\n",
              "      <td>0.005221</td>\n",
              "      <td>-0.002387</td>\n",
              "      <td>0.003034</td>\n",
              "      <td>0.002153</td>\n",
              "      <td>0.002493</td>\n",
              "      <td>-0.002085</td>\n",
              "      <td>-0.005023</td>\n",
              "      <td>-0.000917</td>\n",
              "      <td>0.002189</td>\n",
              "      <td>0.000331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>falling down the stairs</th>\n",
              "      <td>-0.002184</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>-0.000409</td>\n",
              "      <td>-0.000861</td>\n",
              "      <td>0.002844</td>\n",
              "      <td>-0.003674</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.002191</td>\n",
              "      <td>-0.001103</td>\n",
              "      <td>-0.001180</td>\n",
              "      <td>0.000792</td>\n",
              "      <td>-0.000323</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forgetting my own birthday</th>\n",
              "      <td>-0.002358</td>\n",
              "      <td>-0.002267</td>\n",
              "      <td>0.004628</td>\n",
              "      <td>-0.000516</td>\n",
              "      <td>0.002241</td>\n",
              "      <td>-0.000047</td>\n",
              "      <td>0.001980</td>\n",
              "      <td>-0.003593</td>\n",
              "      <td>-0.003625</td>\n",
              "      <td>0.001367</td>\n",
              "      <td>0.002954</td>\n",
              "      <td>0.001726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheating on my partner</th>\n",
              "      <td>-0.003489</td>\n",
              "      <td>-0.001700</td>\n",
              "      <td>0.001908</td>\n",
              "      <td>-0.002567</td>\n",
              "      <td>0.003897</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>-0.002667</td>\n",
              "      <td>-0.001549</td>\n",
              "      <td>-0.000386</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.003085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>watching a rival sports team lose badly</th>\n",
              "      <td>-0.000182</td>\n",
              "      <td>-0.000730</td>\n",
              "      <td>0.003446</td>\n",
              "      <td>0.001630</td>\n",
              "      <td>0.003593</td>\n",
              "      <td>0.001519</td>\n",
              "      <td>-0.002040</td>\n",
              "      <td>-0.002177</td>\n",
              "      <td>-0.004572</td>\n",
              "      <td>-0.002869</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>0.001160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kicking a baby</th>\n",
              "      <td>0.001282</td>\n",
              "      <td>-0.001361</td>\n",
              "      <td>0.001992</td>\n",
              "      <td>0.001388</td>\n",
              "      <td>0.004126</td>\n",
              "      <td>-0.001150</td>\n",
              "      <td>-0.000863</td>\n",
              "      <td>-0.000421</td>\n",
              "      <td>-0.002691</td>\n",
              "      <td>-0.002231</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.001550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breaking someone's heart</th>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.004473</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.001944</td>\n",
              "      <td>-0.001884</td>\n",
              "      <td>0.001428</td>\n",
              "      <td>0.002226</td>\n",
              "      <td>-0.005145</td>\n",
              "      <td>-0.001827</td>\n",
              "      <td>0.002215</td>\n",
              "      <td>-0.002583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>burning down my neighborhood</th>\n",
              "      <td>-0.002628</td>\n",
              "      <td>-0.000746</td>\n",
              "      <td>0.002970</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.003414</td>\n",
              "      <td>-0.001419</td>\n",
              "      <td>0.001596</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.000826</td>\n",
              "      <td>-0.000571</td>\n",
              "      <td>0.002794</td>\n",
              "      <td>0.002066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>having impure thoughts about a family member</th>\n",
              "      <td>-0.005497</td>\n",
              "      <td>-0.001932</td>\n",
              "      <td>0.007423</td>\n",
              "      <td>-0.000186</td>\n",
              "      <td>0.001274</td>\n",
              "      <td>-0.000530</td>\n",
              "      <td>0.001865</td>\n",
              "      <td>-0.000115</td>\n",
              "      <td>-0.002051</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.001142</td>\n",
              "      <td>-0.002860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reading stories to neighborhood children</th>\n",
              "      <td>-0.001839</td>\n",
              "      <td>-0.001082</td>\n",
              "      <td>0.003599</td>\n",
              "      <td>0.000699</td>\n",
              "      <td>0.004943</td>\n",
              "      <td>-0.001608</td>\n",
              "      <td>0.004791</td>\n",
              "      <td>0.002367</td>\n",
              "      <td>-0.002800</td>\n",
              "      <td>-0.003055</td>\n",
              "      <td>0.001220</td>\n",
              "      <td>0.000990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>being a pen pal for a child in a developing country</th>\n",
              "      <td>0.000005</td>\n",
              "      <td>-0.001191</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>-0.001201</td>\n",
              "      <td>0.002384</td>\n",
              "      <td>-0.002296</td>\n",
              "      <td>0.002708</td>\n",
              "      <td>0.003003</td>\n",
              "      <td>-0.003087</td>\n",
              "      <td>-0.000656</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>-0.000846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carrying groceries for an elderly person</th>\n",
              "      <td>-0.000875</td>\n",
              "      <td>-0.000831</td>\n",
              "      <td>-0.002040</td>\n",
              "      <td>-0.003855</td>\n",
              "      <td>0.003846</td>\n",
              "      <td>-0.001786</td>\n",
              "      <td>0.001155</td>\n",
              "      <td>0.001666</td>\n",
              "      <td>-0.004314</td>\n",
              "      <td>-0.002007</td>\n",
              "      <td>-0.000232</td>\n",
              "      <td>-0.000544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>writing letters to the local government to support a homeless shelter</th>\n",
              "      <td>-0.003561</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.003489</td>\n",
              "      <td>0.002772</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>-0.000494</td>\n",
              "      <td>0.001421</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>-0.000699</td>\n",
              "      <td>-0.002411</td>\n",
              "      <td>0.000862</td>\n",
              "      <td>0.000436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running into a burning building to save a precious antique</th>\n",
              "      <td>-0.000641</td>\n",
              "      <td>-0.000421</td>\n",
              "      <td>0.002309</td>\n",
              "      <td>0.001609</td>\n",
              "      <td>0.004813</td>\n",
              "      <td>-0.000234</td>\n",
              "      <td>0.000712</td>\n",
              "      <td>0.001195</td>\n",
              "      <td>-0.002631</td>\n",
              "      <td>-0.001432</td>\n",
              "      <td>0.001524</td>\n",
              "      <td>0.000921</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>buying carbon credits</th>\n",
              "      <td>-0.000181</td>\n",
              "      <td>-0.001668</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>-0.003568</td>\n",
              "      <td>0.003233</td>\n",
              "      <td>-0.001086</td>\n",
              "      <td>0.003342</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>-0.000430</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.000764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>going swing dancing</th>\n",
              "      <td>0.000608</td>\n",
              "      <td>-0.001156</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>-0.002417</td>\n",
              "      <td>0.007182</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>-0.000211</td>\n",
              "      <td>-0.000496</td>\n",
              "      <td>-0.005031</td>\n",
              "      <td>-0.000706</td>\n",
              "      <td>0.002815</td>\n",
              "      <td>0.004986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>watching your favorite movie</th>\n",
              "      <td>-0.003138</td>\n",
              "      <td>-0.001925</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>0.001295</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>-0.000570</td>\n",
              "      <td>-0.001881</td>\n",
              "      <td>-0.000536</td>\n",
              "      <td>0.002594</td>\n",
              "      <td>0.001462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>painting a wall</th>\n",
              "      <td>-0.000932</td>\n",
              "      <td>-0.000545</td>\n",
              "      <td>0.002355</td>\n",
              "      <td>0.001532</td>\n",
              "      <td>0.003509</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>-0.000568</td>\n",
              "      <td>0.002122</td>\n",
              "      <td>-0.004291</td>\n",
              "      <td>-0.003189</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.001279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>listening to local news</th>\n",
              "      <td>0.001009</td>\n",
              "      <td>-0.000599</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.002699</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>0.000684</td>\n",
              "      <td>-0.002753</td>\n",
              "      <td>-0.003076</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>punching myself in the face</th>\n",
              "      <td>-0.003999</td>\n",
              "      <td>-0.001701</td>\n",
              "      <td>0.006512</td>\n",
              "      <td>-0.001412</td>\n",
              "      <td>-0.000503</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>-0.002934</td>\n",
              "      <td>-0.004477</td>\n",
              "      <td>-0.003038</td>\n",
              "      <td>0.000551</td>\n",
              "      <td>0.001603</td>\n",
              "      <td>-0.002736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dropping and breaking my phone</th>\n",
              "      <td>-0.000982</td>\n",
              "      <td>-0.000987</td>\n",
              "      <td>0.002960</td>\n",
              "      <td>0.000840</td>\n",
              "      <td>0.002505</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>-0.000316</td>\n",
              "      <td>-0.002521</td>\n",
              "      <td>-0.001509</td>\n",
              "      <td>-0.000783</td>\n",
              "      <td>0.002547</td>\n",
              "      <td>0.002812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>taking candy from a baby</th>\n",
              "      <td>-0.000096</td>\n",
              "      <td>-0.000661</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>-0.001333</td>\n",
              "      <td>0.004051</td>\n",
              "      <td>-0.002192</td>\n",
              "      <td>0.002787</td>\n",
              "      <td>-0.000440</td>\n",
              "      <td>-0.004723</td>\n",
              "      <td>-0.002092</td>\n",
              "      <td>0.001890</td>\n",
              "      <td>0.001205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lying on my taxes to get a higher return</th>\n",
              "      <td>-0.003934</td>\n",
              "      <td>-0.003044</td>\n",
              "      <td>0.008972</td>\n",
              "      <td>0.002453</td>\n",
              "      <td>0.003034</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.004573</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>-0.001355</td>\n",
              "      <td>-0.001345</td>\n",
              "      <td>0.003255</td>\n",
              "      <td>-0.000228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>punching my friend in the face</th>\n",
              "      <td>-0.002460</td>\n",
              "      <td>-0.001986</td>\n",
              "      <td>0.004444</td>\n",
              "      <td>-0.002446</td>\n",
              "      <td>0.001168</td>\n",
              "      <td>0.001476</td>\n",
              "      <td>-0.002275</td>\n",
              "      <td>-0.002968</td>\n",
              "      <td>-0.002669</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>-0.001079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>losing borrowed book</th>\n",
              "      <td>-0.000937</td>\n",
              "      <td>-0.002654</td>\n",
              "      <td>0.006319</td>\n",
              "      <td>0.003028</td>\n",
              "      <td>0.002862</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.003073</td>\n",
              "      <td>0.001696</td>\n",
              "      <td>-0.003027</td>\n",
              "      <td>-0.000754</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.001921</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eaef2a94-8e38-4ea2-b636-47c364c4f9ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eaef2a94-8e38-4ea2-b636-47c364c4f9ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eaef2a94-8e38-4ea2-b636-47c364c4f9ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-84582dd5-7292-41f3-ad06-89fba7129f04\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84582dd5-7292-41f3-ad06-89fba7129f04')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-84582dd5-7292-41f3-ad06-89fba7129f04 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7998a315-17a8-4888-b92f-36ef44f2fdd4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('action_df_all_virtues')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7998a315-17a8-4888-b92f-36ef44f2fdd4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('action_df_all_virtues');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                     courage  temperance  \\\n",
              "running a marathon for charity                      0.001065    0.000348   \n",
              "giving my employees a day off to go to the spa     -0.002030   -0.001505   \n",
              "serving food to unhoused people in a soup kitchen   0.000642    0.000295   \n",
              "teaching English to refugees                       -0.001724    0.001175   \n",
              "cleaning up litter in a rough neighborhood         -0.002128   -0.000536   \n",
              "serving on a jury                                   0.000879   -0.000618   \n",
              "running a marathon for fitness                      0.000138   -0.000894   \n",
              "winning millions in the lottery                    -0.003691   -0.000883   \n",
              "mowing the lawn                                    -0.003585   -0.001994   \n",
              "sitting in a chair                                  0.004678   -0.001247   \n",
              "falling off a bridge                               -0.001575   -0.000185   \n",
              "losing my wallet                                   -0.002874   -0.002217   \n",
              "stealing someone's wallet                           0.001444   -0.000769   \n",
              "revealing state secrets for personal gain          -0.003633    0.000089   \n",
              "pushing a girl off a bridge                         0.003256    0.000143   \n",
              "forgetting my mom's birthday                       -0.002419   -0.002305   \n",
              "working as a suicide bomber                        -0.001396   -0.000864   \n",
              "thinking about harming myself                       0.000457   -0.000864   \n",
              "walking dogs for a local animal shelter            -0.001682   -0.000821   \n",
              "donating money to charity                          -0.002909   -0.000392   \n",
              "helping someone change their flat tire on the road -0.000430   -0.000433   \n",
              "volunteering at a voting location                  -0.001733   -0.000397   \n",
              "rescuing refugees from a sinking life raft         -0.001036   -0.000017   \n",
              "paying taxes                                       -0.000918   -0.001757   \n",
              "hiking in a beautiful place                         0.002027   -0.000786   \n",
              "taking a day off to go to the spa                   0.001262   -0.002093   \n",
              "opening a door                                     -0.001295   -0.001264   \n",
              "thinking about the day's plans                     -0.002188   -0.002371   \n",
              "falling down the stairs                            -0.002184    0.000110   \n",
              "forgetting my own birthday                         -0.002358   -0.002267   \n",
              "cheating on my partner                             -0.003489   -0.001700   \n",
              "watching a rival sports team lose badly            -0.000182   -0.000730   \n",
              "kicking a baby                                      0.001282   -0.001361   \n",
              "breaking someone's heart                            0.000074    0.000033   \n",
              "burning down my neighborhood                       -0.002628   -0.000746   \n",
              "having impure thoughts about a family member       -0.005497   -0.001932   \n",
              "reading stories to neighborhood children           -0.001839   -0.001082   \n",
              "being a pen pal for a child in a developing cou...  0.000005   -0.001191   \n",
              "carrying groceries for an elderly person           -0.000875   -0.000831   \n",
              "writing letters to the local government to supp... -0.003561    0.000157   \n",
              "running into a burning building to save a preci... -0.000641   -0.000421   \n",
              "buying carbon credits                              -0.000181   -0.001668   \n",
              "going swing dancing                                 0.000608   -0.001156   \n",
              "watching your favorite movie                       -0.003138   -0.001925   \n",
              "painting a wall                                    -0.000932   -0.000545   \n",
              "listening to local news                             0.001009   -0.000599   \n",
              "punching myself in the face                        -0.003999   -0.001701   \n",
              "dropping and breaking my phone                     -0.000982   -0.000987   \n",
              "taking candy from a baby                           -0.000096   -0.000661   \n",
              "lying on my taxes to get a higher return           -0.003934   -0.003044   \n",
              "punching my friend in the face                     -0.002460   -0.001986   \n",
              "losing borrowed book                               -0.000937   -0.002654   \n",
              "\n",
              "                                                    liberality  magnificence  \\\n",
              "running a marathon for charity                        0.001058      0.000040   \n",
              "giving my employees a day off to go to the spa        0.003233     -0.001397   \n",
              "serving food to unhoused people in a soup kitchen    -0.000093     -0.000363   \n",
              "teaching English to refugees                         -0.000701     -0.004318   \n",
              "cleaning up litter in a rough neighborhood            0.003132      0.002127   \n",
              "serving on a jury                                    -0.001575      0.001746   \n",
              "running a marathon for fitness                       -0.000521     -0.002778   \n",
              "winning millions in the lottery                       0.003697      0.002264   \n",
              "mowing the lawn                                      -0.000487      0.001966   \n",
              "sitting in a chair                                    0.001029     -0.001456   \n",
              "falling off a bridge                                  0.002046      0.001149   \n",
              "losing my wallet                                      0.003113      0.001517   \n",
              "stealing someone's wallet                            -0.001473     -0.004314   \n",
              "revealing state secrets for personal gain             0.001822     -0.003261   \n",
              "pushing a girl off a bridge                           0.002553      0.001404   \n",
              "forgetting my mom's birthday                          0.004786     -0.000934   \n",
              "working as a suicide bomber                           0.000153     -0.001714   \n",
              "thinking about harming myself                         0.004478      0.000101   \n",
              "walking dogs for a local animal shelter               0.003269      0.001101   \n",
              "donating money to charity                             0.003927      0.001557   \n",
              "helping someone change their flat tire on the road    0.001954     -0.000370   \n",
              "volunteering at a voting location                     0.000491      0.001825   \n",
              "rescuing refugees from a sinking life raft            0.000636     -0.000511   \n",
              "paying taxes                                          0.002312     -0.004570   \n",
              "hiking in a beautiful place                           0.000922      0.002255   \n",
              "taking a day off to go to the spa                     0.001928     -0.002538   \n",
              "opening a door                                        0.004155      0.002547   \n",
              "thinking about the day's plans                        0.005221     -0.002387   \n",
              "falling down the stairs                              -0.000409     -0.000861   \n",
              "forgetting my own birthday                            0.004628     -0.000516   \n",
              "cheating on my partner                                0.001908     -0.002567   \n",
              "watching a rival sports team lose badly               0.003446      0.001630   \n",
              "kicking a baby                                        0.001992      0.001388   \n",
              "breaking someone's heart                              0.004473      0.000302   \n",
              "burning down my neighborhood                          0.002970      0.000937   \n",
              "having impure thoughts about a family member          0.007423     -0.000186   \n",
              "reading stories to neighborhood children              0.003599      0.000699   \n",
              "being a pen pal for a child in a developing cou...    0.002036     -0.001201   \n",
              "carrying groceries for an elderly person             -0.002040     -0.003855   \n",
              "writing letters to the local government to supp...    0.003489      0.002772   \n",
              "running into a burning building to save a preci...    0.002309      0.001609   \n",
              "buying carbon credits                                 0.001096     -0.003568   \n",
              "going swing dancing                                   0.000060     -0.002417   \n",
              "watching your favorite movie                          0.003800      0.001295   \n",
              "painting a wall                                       0.002355      0.001532   \n",
              "listening to local news                               0.000962      0.000600   \n",
              "punching myself in the face                           0.006512     -0.001412   \n",
              "dropping and breaking my phone                        0.002960      0.000840   \n",
              "taking candy from a baby                              0.000425     -0.001333   \n",
              "lying on my taxes to get a higher return              0.008972      0.002453   \n",
              "punching my friend in the face                        0.004444     -0.002446   \n",
              "losing borrowed book                                  0.006319      0.003028   \n",
              "\n",
              "                                                    magnanimity     pride  \\\n",
              "running a marathon for charity                         0.002318 -0.002761   \n",
              "giving my employees a day off to go to the spa         0.003993  0.002242   \n",
              "serving food to unhoused people in a soup kitchen      0.003499 -0.001246   \n",
              "teaching English to refugees                           0.003527 -0.000773   \n",
              "cleaning up litter in a rough neighborhood             0.002106 -0.001418   \n",
              "serving on a jury                                      0.004394 -0.001545   \n",
              "running a marathon for fitness                         0.002456 -0.003430   \n",
              "winning millions in the lottery                        0.001622 -0.000028   \n",
              "mowing the lawn                                       -0.000344 -0.001966   \n",
              "sitting in a chair                                     0.001339 -0.003676   \n",
              "falling off a bridge                                   0.002984 -0.001238   \n",
              "losing my wallet                                       0.002009 -0.000192   \n",
              "stealing someone's wallet                              0.003732 -0.002385   \n",
              "revealing state secrets for personal gain              0.002278 -0.001828   \n",
              "pushing a girl off a bridge                            0.002636 -0.000912   \n",
              "forgetting my mom's birthday                           0.000247 -0.000744   \n",
              "working as a suicide bomber                            0.002454 -0.001572   \n",
              "thinking about harming myself                          0.002117  0.001742   \n",
              "walking dogs for a local animal shelter                0.002644 -0.001314   \n",
              "donating money to charity                              0.003511 -0.000196   \n",
              "helping someone change their flat tire on the road     0.004309 -0.001781   \n",
              "volunteering at a voting location                      0.007114  0.000915   \n",
              "rescuing refugees from a sinking life raft             0.004909 -0.000741   \n",
              "paying taxes                                           0.002886 -0.001663   \n",
              "hiking in a beautiful place                            0.004221 -0.000798   \n",
              "taking a day off to go to the spa                      0.005837  0.003454   \n",
              "opening a door                                         0.003347 -0.001138   \n",
              "thinking about the day's plans                         0.003034  0.002153   \n",
              "falling down the stairs                                0.002844 -0.003674   \n",
              "forgetting my own birthday                             0.002241 -0.000047   \n",
              "cheating on my partner                                 0.003897  0.001333   \n",
              "watching a rival sports team lose badly                0.003593  0.001519   \n",
              "kicking a baby                                         0.004126 -0.001150   \n",
              "breaking someone's heart                               0.001944 -0.001884   \n",
              "burning down my neighborhood                           0.003414 -0.001419   \n",
              "having impure thoughts about a family member           0.001274 -0.000530   \n",
              "reading stories to neighborhood children               0.004943 -0.001608   \n",
              "being a pen pal for a child in a developing cou...     0.002384 -0.002296   \n",
              "carrying groceries for an elderly person               0.003846 -0.001786   \n",
              "writing letters to the local government to supp...     0.002528 -0.000494   \n",
              "running into a burning building to save a preci...     0.004813 -0.000234   \n",
              "buying carbon credits                                  0.003233 -0.001086   \n",
              "going swing dancing                                    0.007182  0.000380   \n",
              "watching your favorite movie                           0.004203  0.000774   \n",
              "painting a wall                                        0.003509  0.000231   \n",
              "listening to local news                                0.002699  0.000687   \n",
              "punching myself in the face                           -0.000503  0.001020   \n",
              "dropping and breaking my phone                         0.002505  0.001076   \n",
              "taking candy from a baby                               0.004051 -0.002192   \n",
              "lying on my taxes to get a higher return               0.003034  0.001097   \n",
              "punching my friend in the face                         0.001168  0.001476   \n",
              "losing borrowed book                                   0.002862  0.000216   \n",
              "\n",
              "                                                    patience  truthfulness  \\\n",
              "running a marathon for charity                      0.000239      0.001662   \n",
              "giving my employees a day off to go to the spa      0.001123     -0.000529   \n",
              "serving food to unhoused people in a soup kitchen  -0.001621      0.001076   \n",
              "teaching English to refugees                       -0.000387      0.001634   \n",
              "cleaning up litter in a rough neighborhood          0.000763      0.002503   \n",
              "serving on a jury                                   0.002442      0.004226   \n",
              "running a marathon for fitness                      0.002385      0.002297   \n",
              "winning millions in the lottery                     0.001834     -0.001502   \n",
              "mowing the lawn                                     0.004155      0.000529   \n",
              "sitting in a chair                                  0.003233      0.004822   \n",
              "falling off a bridge                               -0.001549      0.000828   \n",
              "losing my wallet                                    0.000430     -0.001105   \n",
              "stealing someone's wallet                           0.000811      0.001837   \n",
              "revealing state secrets for personal gain           0.002085      0.000266   \n",
              "pushing a girl off a bridge                        -0.002302      0.001613   \n",
              "forgetting my mom's birthday                        0.002510     -0.003649   \n",
              "working as a suicide bomber                         0.001455      0.003036   \n",
              "thinking about harming myself                      -0.002127     -0.002036   \n",
              "walking dogs for a local animal shelter             0.000623      0.001158   \n",
              "donating money to charity                           0.000865     -0.000011   \n",
              "helping someone change their flat tire on the road  0.001068      0.002871   \n",
              "volunteering at a voting location                   0.000546      0.002766   \n",
              "rescuing refugees from a sinking life raft          0.000248      0.000112   \n",
              "paying taxes                                        0.003150      0.002365   \n",
              "hiking in a beautiful place                         0.000868      0.001649   \n",
              "taking a day off to go to the spa                   0.000603     -0.000705   \n",
              "opening a door                                      0.000904      0.001745   \n",
              "thinking about the day's plans                      0.002493     -0.002085   \n",
              "falling down the stairs                             0.000100      0.002191   \n",
              "forgetting my own birthday                          0.001980     -0.003593   \n",
              "cheating on my partner                              0.000040     -0.002667   \n",
              "watching a rival sports team lose badly            -0.002040     -0.002177   \n",
              "kicking a baby                                     -0.000863     -0.000421   \n",
              "breaking someone's heart                            0.001428      0.002226   \n",
              "burning down my neighborhood                        0.001596      0.000005   \n",
              "having impure thoughts about a family member        0.001865     -0.000115   \n",
              "reading stories to neighborhood children            0.004791      0.002367   \n",
              "being a pen pal for a child in a developing cou...  0.002708      0.003003   \n",
              "carrying groceries for an elderly person            0.001155      0.001666   \n",
              "writing letters to the local government to supp...  0.001421      0.001109   \n",
              "running into a burning building to save a preci...  0.000712      0.001195   \n",
              "buying carbon credits                               0.003342      0.002918   \n",
              "going swing dancing                                -0.000211     -0.000496   \n",
              "watching your favorite movie                        0.002022     -0.000570   \n",
              "painting a wall                                    -0.000568      0.002122   \n",
              "listening to local news                            -0.000246      0.000684   \n",
              "punching myself in the face                        -0.002934     -0.004477   \n",
              "dropping and breaking my phone                     -0.000316     -0.002521   \n",
              "taking candy from a baby                            0.002787     -0.000440   \n",
              "lying on my taxes to get a higher return            0.004573      0.000544   \n",
              "punching my friend in the face                     -0.002275     -0.002968   \n",
              "losing borrowed book                                0.003073      0.001696   \n",
              "\n",
              "                                                    wittiness  friendliness  \\\n",
              "running a marathon for charity                      -0.004166     -0.001892   \n",
              "giving my employees a day off to go to the spa      -0.003010     -0.002290   \n",
              "serving food to unhoused people in a soup kitchen   -0.004210     -0.001834   \n",
              "teaching English to refugees                        -0.001717     -0.001550   \n",
              "cleaning up litter in a rough neighborhood          -0.001581     -0.002973   \n",
              "serving on a jury                                   -0.002881     -0.003009   \n",
              "running a marathon for fitness                      -0.004158     -0.001956   \n",
              "winning millions in the lottery                     -0.002951     -0.002152   \n",
              "mowing the lawn                                     -0.003177     -0.003892   \n",
              "sitting in a chair                                  -0.008038      0.000018   \n",
              "falling off a bridge                                -0.001243     -0.002658   \n",
              "losing my wallet                                    -0.003224     -0.000231   \n",
              "stealing someone's wallet                           -0.006782     -0.000130   \n",
              "revealing state secrets for personal gain           -0.002448     -0.001508   \n",
              "pushing a girl off a bridge                         -0.002702     -0.000991   \n",
              "forgetting my mom's birthday                        -0.003947      0.001003   \n",
              "working as a suicide bomber                         -0.002188     -0.001724   \n",
              "thinking about harming myself                       -0.003801     -0.000441   \n",
              "walking dogs for a local animal shelter             -0.003612     -0.002474   \n",
              "donating money to charity                           -0.003214     -0.002937   \n",
              "helping someone change their flat tire on the road  -0.001828     -0.001254   \n",
              "volunteering at a voting location                   -0.001058     -0.002830   \n",
              "rescuing refugees from a sinking life raft          -0.004567     -0.002756   \n",
              "paying taxes                                        -0.003865     -0.000798   \n",
              "hiking in a beautiful place                         -0.004918     -0.003182   \n",
              "taking a day off to go to the spa                   -0.003872     -0.002907   \n",
              "opening a door                                      -0.003139     -0.002397   \n",
              "thinking about the day's plans                      -0.005023     -0.000917   \n",
              "falling down the stairs                             -0.001103     -0.001180   \n",
              "forgetting my own birthday                          -0.003625      0.001367   \n",
              "cheating on my partner                              -0.001549     -0.000386   \n",
              "watching a rival sports team lose badly             -0.004572     -0.002869   \n",
              "kicking a baby                                      -0.002691     -0.002231   \n",
              "breaking someone's heart                            -0.005145     -0.001827   \n",
              "burning down my neighborhood                        -0.000826     -0.000571   \n",
              "having impure thoughts about a family member        -0.002051      0.000217   \n",
              "reading stories to neighborhood children            -0.002800     -0.003055   \n",
              "being a pen pal for a child in a developing cou...  -0.003087     -0.000656   \n",
              "carrying groceries for an elderly person            -0.004314     -0.002007   \n",
              "writing letters to the local government to supp...  -0.000699     -0.002411   \n",
              "running into a burning building to save a preci...  -0.002631     -0.001432   \n",
              "buying carbon credits                               -0.000430      0.000881   \n",
              "going swing dancing                                 -0.005031     -0.000706   \n",
              "watching your favorite movie                        -0.001881     -0.000536   \n",
              "painting a wall                                     -0.004291     -0.003189   \n",
              "listening to local news                             -0.002753     -0.003076   \n",
              "punching myself in the face                         -0.003038      0.000551   \n",
              "dropping and breaking my phone                      -0.001509     -0.000783   \n",
              "taking candy from a baby                            -0.004723     -0.002092   \n",
              "lying on my taxes to get a higher return            -0.001355     -0.001345   \n",
              "punching my friend in the face                      -0.002669      0.000054   \n",
              "losing borrowed book                                -0.003027     -0.000754   \n",
              "\n",
              "                                                     modesty  \\\n",
              "running a marathon for charity                      0.000511   \n",
              "giving my employees a day off to go to the spa      0.000812   \n",
              "serving food to unhoused people in a soup kitchen   0.000553   \n",
              "teaching English to refugees                       -0.000706   \n",
              "cleaning up litter in a rough neighborhood         -0.000766   \n",
              "serving on a jury                                   0.000818   \n",
              "running a marathon for fitness                      0.000407   \n",
              "winning millions in the lottery                     0.000540   \n",
              "mowing the lawn                                    -0.002907   \n",
              "sitting in a chair                                  0.001746   \n",
              "falling off a bridge                                0.000867   \n",
              "losing my wallet                                    0.000709   \n",
              "stealing someone's wallet                           0.000587   \n",
              "revealing state secrets for personal gain          -0.000091   \n",
              "pushing a girl off a bridge                         0.002746   \n",
              "forgetting my mom's birthday                        0.001795   \n",
              "working as a suicide bomber                        -0.000297   \n",
              "thinking about harming myself                       0.002254   \n",
              "walking dogs for a local animal shelter            -0.000459   \n",
              "donating money to charity                           0.000702   \n",
              "helping someone change their flat tire on the road  0.001130   \n",
              "volunteering at a voting location                   0.000174   \n",
              "rescuing refugees from a sinking life raft          0.000629   \n",
              "paying taxes                                        0.003421   \n",
              "hiking in a beautiful place                         0.001377   \n",
              "taking a day off to go to the spa                   0.002385   \n",
              "opening a door                                      0.001221   \n",
              "thinking about the day's plans                      0.002189   \n",
              "falling down the stairs                             0.000792   \n",
              "forgetting my own birthday                          0.002954   \n",
              "cheating on my partner                              0.001001   \n",
              "watching a rival sports team lose badly             0.001839   \n",
              "kicking a baby                                      0.002083   \n",
              "breaking someone's heart                            0.002215   \n",
              "burning down my neighborhood                        0.002794   \n",
              "having impure thoughts about a family member        0.001142   \n",
              "reading stories to neighborhood children            0.001220   \n",
              "being a pen pal for a child in a developing cou...  0.001784   \n",
              "carrying groceries for an elderly person           -0.000232   \n",
              "writing letters to the local government to supp...  0.000862   \n",
              "running into a burning building to save a preci...  0.001524   \n",
              "buying carbon credits                               0.002306   \n",
              "going swing dancing                                 0.002815   \n",
              "watching your favorite movie                        0.002594   \n",
              "painting a wall                                     0.000573   \n",
              "listening to local news                             0.000053   \n",
              "punching myself in the face                         0.001603   \n",
              "dropping and breaking my phone                      0.002547   \n",
              "taking candy from a baby                            0.001890   \n",
              "lying on my taxes to get a higher return            0.003255   \n",
              "punching my friend in the face                      0.001216   \n",
              "losing borrowed book                                0.001526   \n",
              "\n",
              "                                                    righteous indignation  \n",
              "running a marathon for charity                                  -0.002514  \n",
              "giving my employees a day off to go to the spa                   0.002880  \n",
              "serving food to unhoused people in a soup kitchen               -0.000276  \n",
              "teaching English to refugees                                    -0.000279  \n",
              "cleaning up litter in a rough neighborhood                      -0.000445  \n",
              "serving on a jury                                                0.002469  \n",
              "running a marathon for fitness                                  -0.003648  \n",
              "winning millions in the lottery                                  0.000939  \n",
              "mowing the lawn                                                 -0.001609  \n",
              "sitting in a chair                                              -0.003960  \n",
              "falling off a bridge                                            -0.000521  \n",
              "losing my wallet                                                 0.001662  \n",
              "stealing someone's wallet                                       -0.000813  \n",
              "revealing state secrets for personal gain                       -0.003250  \n",
              "pushing a girl off a bridge                                      0.000635  \n",
              "forgetting my mom's birthday                                    -0.001656  \n",
              "working as a suicide bomber                                     -0.002208  \n",
              "thinking about harming myself                                   -0.000331  \n",
              "walking dogs for a local animal shelter                         -0.001532  \n",
              "donating money to charity                                        0.000398  \n",
              "helping someone change their flat tire on the road               0.001228  \n",
              "volunteering at a voting location                                0.005640  \n",
              "rescuing refugees from a sinking life raft                       0.001529  \n",
              "paying taxes                                                    -0.001788  \n",
              "hiking in a beautiful place                                      0.001017  \n",
              "taking a day off to go to the spa                                0.003889  \n",
              "opening a door                                                   0.000102  \n",
              "thinking about the day's plans                                   0.000331  \n",
              "falling down the stairs                                         -0.000323  \n",
              "forgetting my own birthday                                       0.001726  \n",
              "cheating on my partner                                           0.003085  \n",
              "watching a rival sports team lose badly                          0.001160  \n",
              "kicking a baby                                                   0.001550  \n",
              "breaking someone's heart                                        -0.002583  \n",
              "burning down my neighborhood                                     0.002066  \n",
              "having impure thoughts about a family member                    -0.002860  \n",
              "reading stories to neighborhood children                         0.000990  \n",
              "being a pen pal for a child in a developing cou...              -0.000846  \n",
              "carrying groceries for an elderly person                        -0.000544  \n",
              "writing letters to the local government to supp...               0.000436  \n",
              "running into a burning building to save a preci...               0.000921  \n",
              "buying carbon credits                                            0.000764  \n",
              "going swing dancing                                              0.004986  \n",
              "watching your favorite movie                                     0.001462  \n",
              "painting a wall                                                  0.001279  \n",
              "listening to local news                                          0.000343  \n",
              "punching myself in the face                                     -0.002736  \n",
              "dropping and breaking my phone                                   0.002812  \n",
              "taking candy from a baby                                         0.001205  \n",
              "lying on my taxes to get a higher return                        -0.000228  \n",
              "punching my friend in the face                                  -0.001079  \n",
              "losing borrowed book                                             0.001921  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "action_df_all_virtues"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "DgX0a4Mn9x7F",
        "outputId": "f464d09c-6b4a-473c-a97e-a1d2da9ed909"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# @title temperance vs liberality\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "action_df_all_virtues.plot(kind='scatter', x='pride', y='modesty', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMV9zPcNVUrK",
        "outputId": "97d32c46-3eff-4562-e7b1-2a28a7fede72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0035439539589627216,\n",
              " 0.013041784158429437,\n",
              " 0.021794424374211488,\n",
              " 0.028239504437976536,\n",
              " 0.02773665137212591,\n",
              " 0.022511865275231187,\n",
              " 0.011248391555450297,\n",
              " 0.013351995527582728,\n",
              " 0.0208307033528823,\n",
              " 0.026486677169619416,\n",
              " -0.002107214478687697,\n",
              " 0.019922502444395866]"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mean_moral_projection = []\n",
        "for virtue in mean_list:\n",
        "  projection_moral = np.inner(np.array(virtue_emb_mean_df[virtue]),np.array(moral_v))\n",
        "  mean_moral_projection.append(projection_moral)\n",
        "\n",
        "mean_hedonic_projection = []\n",
        "for virtue in mean_list:\n",
        "  projection_hedonic = np.inner(np.array(virtue_emb_mean_df[virtue]),np.array(hedonic_v))\n",
        "  mean_hedonic_projection.append(projection_hedonic)\n",
        "\n",
        "mean_hedonic_projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "eWIjw5WcjIvt",
        "outputId": "95a1c71c-a187-41e3-8d40-2cd3bb3e49e7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"virtue_moral_hedonic_df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"moral\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006723932911183401,\n        \"min\": 0.01846678236938067,\n        \"max\": 0.03879042383139522,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.020543191567226265,\n          0.032362323156967146,\n          0.033241998479837095\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hedonic\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009609772572147108,\n        \"min\": -0.002107214478687697,\n        \"max\": 0.028239504437976536,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          -0.002107214478687697,\n          0.026486677169619416,\n          0.0035439539589627216\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "virtue_moral_hedonic_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-69ae10cc-5ed0-430a-b8a6-6a6cc7e4ca81\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>moral</th>\n",
              "      <th>hedonic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>courage</th>\n",
              "      <td>0.033242</td>\n",
              "      <td>0.003544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>temperance</th>\n",
              "      <td>0.030927</td>\n",
              "      <td>0.013042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>liberality</th>\n",
              "      <td>0.028638</td>\n",
              "      <td>0.021794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>magnificence</th>\n",
              "      <td>0.037886</td>\n",
              "      <td>0.028240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>magnanimity</th>\n",
              "      <td>0.038790</td>\n",
              "      <td>0.027737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pride</th>\n",
              "      <td>0.033119</td>\n",
              "      <td>0.022512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>patience</th>\n",
              "      <td>0.026655</td>\n",
              "      <td>0.011248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>truthfulness</th>\n",
              "      <td>0.033198</td>\n",
              "      <td>0.013352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wittiness</th>\n",
              "      <td>0.020617</td>\n",
              "      <td>0.020831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>friendliness</th>\n",
              "      <td>0.032362</td>\n",
              "      <td>0.026487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>modesty</th>\n",
              "      <td>0.020543</td>\n",
              "      <td>-0.002107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>righteous indignation</th>\n",
              "      <td>0.018467</td>\n",
              "      <td>0.019923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69ae10cc-5ed0-430a-b8a6-6a6cc7e4ca81')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-69ae10cc-5ed0-430a-b8a6-6a6cc7e4ca81 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-69ae10cc-5ed0-430a-b8a6-6a6cc7e4ca81');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-64ba3d5c-0bdd-47eb-9d47-d7786d6b0b5b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-64ba3d5c-0bdd-47eb-9d47-d7786d6b0b5b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-64ba3d5c-0bdd-47eb-9d47-d7786d6b0b5b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_348ede91-0a23-4fb3-9608-0780bde282b9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('virtue_moral_hedonic_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_348ede91-0a23-4fb3-9608-0780bde282b9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('virtue_moral_hedonic_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                          moral   hedonic\n",
              "courage                0.033242  0.003544\n",
              "temperance             0.030927  0.013042\n",
              "liberality             0.028638  0.021794\n",
              "magnificence           0.037886  0.028240\n",
              "magnanimity            0.038790  0.027737\n",
              "pride                  0.033119  0.022512\n",
              "patience               0.026655  0.011248\n",
              "truthfulness           0.033198  0.013352\n",
              "wittiness              0.020617  0.020831\n",
              "friendliness           0.032362  0.026487\n",
              "modesty                0.020543 -0.002107\n",
              "righteous indignation  0.018467  0.019923"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#virtue_moral_hedonic_df = pd.DataFrame(index=mean_list, columns=['moral_v','hedonic_v'], data=[[mean_moral_projection],[mean_hedonic_projection]])\n",
        "#virtue_moral_hedonic_df\n",
        "virtue_moral_hedonic_df = pd.DataFrame(\n",
        "    data= {'moral': mean_moral_projection,\n",
        "     'hedonic': mean_hedonic_projection,\n",
        "    },\n",
        "    index=mean_list)\n",
        "\n",
        "virtue_moral_hedonic_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "QOETmoWopsNq",
        "outputId": "37276f13-1e54-426d-af1a-ac31d6abc6f0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "virtue_moral_hedonic_df.plot(kind='scatter', x='moral', y='hedonic', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-x9ZDszV8lMO"
      },
      "outputs": [],
      "source": [
        "def project_(stim_list, courage_v, temperance_v, liberality_v):\n",
        "  '''\n",
        "  get projections for list of items\n",
        "  '''\n",
        "  #construct dataframe to save items\n",
        "  projection_df_ctl = pd.DataFrame(index=action_list_all, columns=list(virtue_emb_diff_df.columns.values), data=0)\n",
        "\n",
        "  #loop through items to get embeddings in moral, hedonic, and movement vector directions\n",
        "\n",
        "  for a in action_list_all:\n",
        "    this_em = getEmbeddings(a)[\"data\"][0][\"embedding\"]\n",
        "    projection_moral = np.inner(np.array(this_em),np.array(courage_v))\n",
        "    projection_hedonic = np.inner(np.array(this_em),np.array(temperance_v))\n",
        "    projection_movement = np.inner(np.array(this_em),np.array(liberality_v))\n",
        "    projection_df_ctl.loc[a, \"courage_v\"] = projection_moral\n",
        "    projection_df_ctl.loc[a, \"temperance_v\"] = projection_hedonic\n",
        "    projection_df_ctl.loc[a, \"liberality_v\"] = projection_movement\n",
        "\n",
        "  return projection_df_ctl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cn6w25YfVFqf",
        "outputId": "199f67aa-181e-4454-8750-dca72b0ca399"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'temperance'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(virtue_emb_diff_df.columns.values)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u88QyZg5WXyf",
        "outputId": "3c0383c9-3f96-4a54-d31e-e4fd8f91b000"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-17-f8bfc06de9e3>:19: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.001066092301173007' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df_ctl.loc[a, \"courage_v\"] = projection_moral\n",
            "<ipython-input-17-f8bfc06de9e3>:20: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.00036522485563000363' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df_ctl.loc[a, \"temperance_v\"] = projection_hedonic\n",
            "<ipython-input-17-f8bfc06de9e3>:21: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.0010577199321673955' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df_ctl.loc[a, \"liberality_v\"] = projection_movement\n"
          ]
        }
      ],
      "source": [
        "action_df = get_projections(action_list_all, courage_v, temperance_v, liberality_v)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9kXTpQeubcrz",
        "outputId": "808225c6-f75e-4159-f75d-8aecd9cfa502"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"action_df\",\n  \"rows\": 52,\n  \"fields\": [\n    {\n      \"column\": \"courage_v\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001979919137851643,\n        \"min\": -0.005494849692349854,\n        \"max\": 0.004676758653282323,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -0.0029084521859463757,\n          -0.00017994859323461907,\n          -0.000979267759660831\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"temperance_v\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0008819863947111199,\n        \"min\": -0.0030255863445624344,\n        \"max\": 0.0011944060439537628,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          -0.0003792620022252871,\n          -0.0016514364675781457,\n          -0.0009710027451546262\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"liberality_v\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0023067184564838647,\n        \"min\": -0.0020397117741636924,\n        \"max\": 0.008972062755004732,\n        \"num_unique_values\": 52,\n        \"samples\": [\n          0.003926962013771734,\n          0.0010958140570986696,\n          0.002959615056118981\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "action_df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-dd9cbd48-a1dd-4c56-8a93-306c95ea8d02\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>courage_v</th>\n",
              "      <th>temperance_v</th>\n",
              "      <th>liberality_v</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>running a marathon for charity</th>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.001058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>giving my employees a day off to go to the spa</th>\n",
              "      <td>-0.002025</td>\n",
              "      <td>-0.001487</td>\n",
              "      <td>0.003233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>serving food to unhoused people in a soup kitchen</th>\n",
              "      <td>0.000642</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>-0.000093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>teaching English to refugees</th>\n",
              "      <td>-0.001726</td>\n",
              "      <td>0.001194</td>\n",
              "      <td>-0.000701</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cleaning up litter in a rough neighborhood</th>\n",
              "      <td>-0.002128</td>\n",
              "      <td>-0.000520</td>\n",
              "      <td>0.003132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>serving on a jury</th>\n",
              "      <td>0.000876</td>\n",
              "      <td>-0.000601</td>\n",
              "      <td>-0.001575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running a marathon for fitness</th>\n",
              "      <td>0.000139</td>\n",
              "      <td>-0.000878</td>\n",
              "      <td>-0.000521</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>winning millions in the lottery</th>\n",
              "      <td>-0.003688</td>\n",
              "      <td>-0.000867</td>\n",
              "      <td>0.003697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mowing the lawn</th>\n",
              "      <td>-0.003590</td>\n",
              "      <td>-0.001975</td>\n",
              "      <td>-0.000487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sitting in a chair</th>\n",
              "      <td>0.004677</td>\n",
              "      <td>-0.001235</td>\n",
              "      <td>0.001029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>falling off a bridge</th>\n",
              "      <td>-0.001575</td>\n",
              "      <td>-0.000170</td>\n",
              "      <td>0.002046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>losing my wallet</th>\n",
              "      <td>-0.002871</td>\n",
              "      <td>-0.002203</td>\n",
              "      <td>0.003113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>stealing someone's wallet</th>\n",
              "      <td>0.001428</td>\n",
              "      <td>-0.000753</td>\n",
              "      <td>-0.001473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>revealing state secrets for personal gain</th>\n",
              "      <td>-0.003631</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.001822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pushing a girl off a bridge</th>\n",
              "      <td>0.003255</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.002553</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forgetting my mom's birthday</th>\n",
              "      <td>-0.002420</td>\n",
              "      <td>-0.002295</td>\n",
              "      <td>0.004786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>working as a suicide bomber</th>\n",
              "      <td>-0.001397</td>\n",
              "      <td>-0.000848</td>\n",
              "      <td>0.000153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thinking about harming myself</th>\n",
              "      <td>0.000454</td>\n",
              "      <td>-0.000851</td>\n",
              "      <td>0.004478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>walking dogs for a local animal shelter</th>\n",
              "      <td>-0.001682</td>\n",
              "      <td>-0.000800</td>\n",
              "      <td>0.003269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>donating money to charity</th>\n",
              "      <td>-0.002908</td>\n",
              "      <td>-0.000379</td>\n",
              "      <td>0.003927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>helping someone change their flat tire on the road</th>\n",
              "      <td>-0.000429</td>\n",
              "      <td>-0.000417</td>\n",
              "      <td>0.001970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>volunteering at a voting location</th>\n",
              "      <td>-0.001734</td>\n",
              "      <td>-0.000379</td>\n",
              "      <td>0.000491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rescuing refugees from a sinking life raft</th>\n",
              "      <td>-0.001033</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000636</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>paying taxes</th>\n",
              "      <td>-0.000911</td>\n",
              "      <td>-0.001744</td>\n",
              "      <td>0.002312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hiking in a beautiful place</th>\n",
              "      <td>0.002023</td>\n",
              "      <td>-0.000769</td>\n",
              "      <td>0.000922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>taking a day off to go to the spa</th>\n",
              "      <td>0.001264</td>\n",
              "      <td>-0.002073</td>\n",
              "      <td>0.001928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>opening a door</th>\n",
              "      <td>-0.001294</td>\n",
              "      <td>-0.001249</td>\n",
              "      <td>0.004155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thinking about the day's plans</th>\n",
              "      <td>-0.002187</td>\n",
              "      <td>-0.002353</td>\n",
              "      <td>0.005221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>falling down the stairs</th>\n",
              "      <td>-0.002183</td>\n",
              "      <td>0.000123</td>\n",
              "      <td>-0.000409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>forgetting my own birthday</th>\n",
              "      <td>-0.002358</td>\n",
              "      <td>-0.002258</td>\n",
              "      <td>0.004628</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cheating on my partner</th>\n",
              "      <td>-0.003489</td>\n",
              "      <td>-0.001684</td>\n",
              "      <td>0.001908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>watching a rival sports team lose badly</th>\n",
              "      <td>-0.000184</td>\n",
              "      <td>-0.000716</td>\n",
              "      <td>0.003446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>kicking a baby</th>\n",
              "      <td>0.001278</td>\n",
              "      <td>-0.001347</td>\n",
              "      <td>0.001992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>breaking someone's heart</th>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.004473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>burning down my neighborhood</th>\n",
              "      <td>-0.002628</td>\n",
              "      <td>-0.000730</td>\n",
              "      <td>0.002970</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>having impure thoughts about a family member</th>\n",
              "      <td>-0.005495</td>\n",
              "      <td>-0.001922</td>\n",
              "      <td>0.007423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reading stories to neighborhood children</th>\n",
              "      <td>-0.001840</td>\n",
              "      <td>-0.001062</td>\n",
              "      <td>0.003599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>being a pen pal for a child in a developing country</th>\n",
              "      <td>0.000006</td>\n",
              "      <td>-0.001177</td>\n",
              "      <td>0.002036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carrying groceries for an elderly person</th>\n",
              "      <td>-0.000874</td>\n",
              "      <td>-0.000812</td>\n",
              "      <td>-0.002040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>writing letters to the local government to support a homeless shelter</th>\n",
              "      <td>-0.003559</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.003489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>running into a burning building to save a precious antique</th>\n",
              "      <td>-0.000646</td>\n",
              "      <td>-0.000405</td>\n",
              "      <td>0.002309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>buying carbon credits</th>\n",
              "      <td>-0.000180</td>\n",
              "      <td>-0.001651</td>\n",
              "      <td>0.001096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>going swing dancing</th>\n",
              "      <td>0.000608</td>\n",
              "      <td>-0.001141</td>\n",
              "      <td>0.000060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>watching your favorite movie</th>\n",
              "      <td>-0.003145</td>\n",
              "      <td>-0.001908</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>painting a wall</th>\n",
              "      <td>-0.000942</td>\n",
              "      <td>-0.000527</td>\n",
              "      <td>0.002355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>listening to local news</th>\n",
              "      <td>0.001013</td>\n",
              "      <td>-0.000580</td>\n",
              "      <td>0.000962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>punching myself in the face</th>\n",
              "      <td>-0.003999</td>\n",
              "      <td>-0.001688</td>\n",
              "      <td>0.006512</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dropping and breaking my phone</th>\n",
              "      <td>-0.000979</td>\n",
              "      <td>-0.000971</td>\n",
              "      <td>0.002960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>taking candy from a baby</th>\n",
              "      <td>-0.000100</td>\n",
              "      <td>-0.000651</td>\n",
              "      <td>0.000425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lying on my taxes to get a higher return</th>\n",
              "      <td>-0.003926</td>\n",
              "      <td>-0.003026</td>\n",
              "      <td>0.008972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>punching my friend in the face</th>\n",
              "      <td>-0.002462</td>\n",
              "      <td>-0.001970</td>\n",
              "      <td>0.004444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>losing borrowed book</th>\n",
              "      <td>-0.000932</td>\n",
              "      <td>-0.002639</td>\n",
              "      <td>0.006319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd9cbd48-a1dd-4c56-8a93-306c95ea8d02')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd9cbd48-a1dd-4c56-8a93-306c95ea8d02 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd9cbd48-a1dd-4c56-8a93-306c95ea8d02');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5fcb90b7-335c-41f5-83b8-679eb21fa84e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5fcb90b7-335c-41f5-83b8-679eb21fa84e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5fcb90b7-335c-41f5-83b8-679eb21fa84e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_975112fb-9bb1-41b6-a8cb-3511009c42eb\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('action_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_975112fb-9bb1-41b6-a8cb-3511009c42eb button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('action_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                                    courage_v  temperance_v  \\\n",
              "running a marathon for charity                       0.001066      0.000365   \n",
              "giving my employees a day off to go to the spa      -0.002025     -0.001487   \n",
              "serving food to unhoused people in a soup kitchen    0.000642      0.000310   \n",
              "teaching English to refugees                        -0.001726      0.001194   \n",
              "cleaning up litter in a rough neighborhood          -0.002128     -0.000520   \n",
              "serving on a jury                                    0.000876     -0.000601   \n",
              "running a marathon for fitness                       0.000139     -0.000878   \n",
              "winning millions in the lottery                     -0.003688     -0.000867   \n",
              "mowing the lawn                                     -0.003590     -0.001975   \n",
              "sitting in a chair                                   0.004677     -0.001235   \n",
              "falling off a bridge                                -0.001575     -0.000170   \n",
              "losing my wallet                                    -0.002871     -0.002203   \n",
              "stealing someone's wallet                            0.001428     -0.000753   \n",
              "revealing state secrets for personal gain           -0.003631      0.000106   \n",
              "pushing a girl off a bridge                          0.003255      0.000158   \n",
              "forgetting my mom's birthday                        -0.002420     -0.002295   \n",
              "working as a suicide bomber                         -0.001397     -0.000848   \n",
              "thinking about harming myself                        0.000454     -0.000851   \n",
              "walking dogs for a local animal shelter             -0.001682     -0.000800   \n",
              "donating money to charity                           -0.002908     -0.000379   \n",
              "helping someone change their flat tire on the road  -0.000429     -0.000417   \n",
              "volunteering at a voting location                   -0.001734     -0.000379   \n",
              "rescuing refugees from a sinking life raft          -0.001033      0.000003   \n",
              "paying taxes                                        -0.000911     -0.001744   \n",
              "hiking in a beautiful place                          0.002023     -0.000769   \n",
              "taking a day off to go to the spa                    0.001264     -0.002073   \n",
              "opening a door                                      -0.001294     -0.001249   \n",
              "thinking about the day's plans                      -0.002187     -0.002353   \n",
              "falling down the stairs                             -0.002183      0.000123   \n",
              "forgetting my own birthday                          -0.002358     -0.002258   \n",
              "cheating on my partner                              -0.003489     -0.001684   \n",
              "watching a rival sports team lose badly             -0.000184     -0.000716   \n",
              "kicking a baby                                       0.001278     -0.001347   \n",
              "breaking someone's heart                             0.000073      0.000048   \n",
              "burning down my neighborhood                        -0.002628     -0.000730   \n",
              "having impure thoughts about a family member        -0.005495     -0.001922   \n",
              "reading stories to neighborhood children            -0.001840     -0.001062   \n",
              "being a pen pal for a child in a developing cou...   0.000006     -0.001177   \n",
              "carrying groceries for an elderly person            -0.000874     -0.000812   \n",
              "writing letters to the local government to supp...  -0.003559      0.000175   \n",
              "running into a burning building to save a preci...  -0.000646     -0.000405   \n",
              "buying carbon credits                               -0.000180     -0.001651   \n",
              "going swing dancing                                  0.000608     -0.001141   \n",
              "watching your favorite movie                        -0.003145     -0.001908   \n",
              "painting a wall                                     -0.000942     -0.000527   \n",
              "listening to local news                              0.001013     -0.000580   \n",
              "punching myself in the face                         -0.003999     -0.001688   \n",
              "dropping and breaking my phone                      -0.000979     -0.000971   \n",
              "taking candy from a baby                            -0.000100     -0.000651   \n",
              "lying on my taxes to get a higher return            -0.003926     -0.003026   \n",
              "punching my friend in the face                      -0.002462     -0.001970   \n",
              "losing borrowed book                                -0.000932     -0.002639   \n",
              "\n",
              "                                                    liberality_v  \n",
              "running a marathon for charity                          0.001058  \n",
              "giving my employees a day off to go to the spa          0.003233  \n",
              "serving food to unhoused people in a soup kitchen      -0.000093  \n",
              "teaching English to refugees                           -0.000701  \n",
              "cleaning up litter in a rough neighborhood              0.003132  \n",
              "serving on a jury                                      -0.001575  \n",
              "running a marathon for fitness                         -0.000521  \n",
              "winning millions in the lottery                         0.003697  \n",
              "mowing the lawn                                        -0.000487  \n",
              "sitting in a chair                                      0.001029  \n",
              "falling off a bridge                                    0.002046  \n",
              "losing my wallet                                        0.003113  \n",
              "stealing someone's wallet                              -0.001473  \n",
              "revealing state secrets for personal gain               0.001822  \n",
              "pushing a girl off a bridge                             0.002553  \n",
              "forgetting my mom's birthday                            0.004786  \n",
              "working as a suicide bomber                             0.000153  \n",
              "thinking about harming myself                           0.004478  \n",
              "walking dogs for a local animal shelter                 0.003269  \n",
              "donating money to charity                               0.003927  \n",
              "helping someone change their flat tire on the road      0.001970  \n",
              "volunteering at a voting location                       0.000491  \n",
              "rescuing refugees from a sinking life raft              0.000636  \n",
              "paying taxes                                            0.002312  \n",
              "hiking in a beautiful place                             0.000922  \n",
              "taking a day off to go to the spa                       0.001928  \n",
              "opening a door                                          0.004155  \n",
              "thinking about the day's plans                          0.005221  \n",
              "falling down the stairs                                -0.000409  \n",
              "forgetting my own birthday                              0.004628  \n",
              "cheating on my partner                                  0.001908  \n",
              "watching a rival sports team lose badly                 0.003446  \n",
              "kicking a baby                                          0.001992  \n",
              "breaking someone's heart                                0.004473  \n",
              "burning down my neighborhood                            0.002970  \n",
              "having impure thoughts about a family member            0.007423  \n",
              "reading stories to neighborhood children                0.003599  \n",
              "being a pen pal for a child in a developing cou...      0.002036  \n",
              "carrying groceries for an elderly person               -0.002040  \n",
              "writing letters to the local government to supp...      0.003489  \n",
              "running into a burning building to save a preci...      0.002309  \n",
              "buying carbon credits                                   0.001096  \n",
              "going swing dancing                                     0.000060  \n",
              "watching your favorite movie                            0.003800  \n",
              "painting a wall                                         0.002355  \n",
              "listening to local news                                 0.000962  \n",
              "punching myself in the face                             0.006512  \n",
              "dropping and breaking my phone                          0.002960  \n",
              "taking candy from a baby                                0.000425  \n",
              "lying on my taxes to get a higher return                0.008972  \n",
              "punching my friend in the face                          0.004444  \n",
              "losing borrowed book                                    0.006319  "
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "action_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7lEVYzPY5qd"
      },
      "outputs": [],
      "source": [
        "def get_projections(stim_list, moral_v, hedonic_v, movement_v):\n",
        "  '''\n",
        "  get projections for list of items\n",
        "  '''\n",
        "  #construct dataframe to save items\n",
        "  projection_df = pd.DataFrame(index=stim_list, columns=['courage_v','temperance_v','liberality_v'], data=0)\n",
        "\n",
        "  #loop through items to get embeddings in moral, hedonic, and movement vector directions\n",
        "\n",
        "  for a in stim_list:\n",
        "    this_em = getEmbeddings(a)[\"data\"][0][\"embedding\"]\n",
        "    projection_moral = np.inner(np.array(this_em),np.array(moral_v))\n",
        "    projection_hedonic = np.inner(np.array(this_em),np.array(hedonic_v))\n",
        "    projection_movement = np.inner(np.array(this_em),np.array(movement_v))\n",
        "    projection_df.loc[a, \"courage_v\"] = projection_moral\n",
        "    projection_df.loc[a, \"temperance_v\"] = projection_hedonic\n",
        "    projection_df.loc[a, \"liberality_v\"] = projection_movement\n",
        "\n",
        "  return projection_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhkEnTfaoiJ-",
        "outputId": "341dc24b-d697-48c2-b788-9061d9c16797"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          2         1         0\n",
            "2  1.000000  0.849759  0.875315\n",
            "1  0.849759  1.000000  0.893319\n",
            "0  0.875315  0.893319  1.000000\n",
            "          2         1         0\n",
            "2  1.000000  0.833950  0.891547\n",
            "1  0.833950  1.000000  0.831318\n",
            "0  0.891547  0.831318  1.000000\n",
            "0       0.013077\n",
            "1      -0.004362\n",
            "2      -0.004571\n",
            "3       0.002126\n",
            "4       0.019575\n",
            "          ...   \n",
            "1531    0.003848\n",
            "1532    0.012578\n",
            "1533   -0.001828\n",
            "1534   -0.002859\n",
            "1535   -0.000449\n",
            "Length: 1536, dtype: float64\n",
            "          2         1         0\n",
            "2  1.000000  0.856273  0.806584\n",
            "1  0.856273  1.000000  0.804478\n",
            "0  0.806584  0.804478  1.000000\n",
            "          2         1         0\n",
            "2  1.000000  0.881714  0.822901\n",
            "1  0.881714  1.000000  0.844764\n",
            "0  0.822901  0.844764  1.000000\n"
          ]
        }
      ],
      "source": [
        "# get moral direction in GPT embeddings\n",
        "attributes_morality_high = ['morally virtuous','ethical', 'high moral value']\n",
        "attributes_morality_low = ['morally wrong','unethical', 'low moral value']\n",
        "moral_v = return_embeddings_diff(attributes_morality_high, attributes_morality_low)\n",
        "\n",
        "print(moral_v)\n",
        "\n",
        "# get hedonic direction in GPT embeddings\n",
        "attributes_hedonic_high = ['personally rewarding','pleasurable for me', 'high hedonic value for me']\n",
        "attributes_hedonic_low = ['personally costly','unpleasurable for me', 'low hedonic value for me']\n",
        "hedonic_v = return_embeddings_diff(attributes_hedonic_high, attributes_hedonic_low)\n",
        "\n",
        "# get physicality direction in GPT embeddings\n",
        "#attributes_movement_high = ['physical','bodily action', 'high movement']\n",
        "#attributes_movement_low = ['mental','minimally active', 'low movement']\n",
        "#movement_v = return_embeddings_diff(attributes_movement_high, attributes_movement_low)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFiUyHmPpajF",
        "outputId": "a9c52340-9de6-433a-eb25-3854e36b4252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding Correlations:\n",
            "r(courage, temperance) = 0.16\n",
            "r(courage, liberality) = -0.57\n",
            "r(temperance, liberality) = -0.4\n"
          ]
        }
      ],
      "source": [
        "# Report correlation among attribute embeddings\n",
        "print('Embedding Correlations:')\n",
        "print('r(courage, temperance) = '+ str(round(np.corrcoef(courage_v, temperance_v)[0, 1], 2)))\n",
        "print('r(courage, liberality) = '+ str(round(np.corrcoef(courage_v, liberality_v)[0, 1], 2)))\n",
        "print('r(temperance, liberality) = '+ str(round(np.corrcoef(temperance_v, liberality_v)[0, 1], 2)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXKONMKpy_k"
      },
      "source": [
        "## generate projections on embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTifrfhUp4Gg",
        "outputId": "100b2ddd-f3b4-4c95-a181-31165a2bdedf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-81-9a4e52498519>:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.008128498181515299' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"moral_v\"] = projection_moral\n",
            "<ipython-input-81-9a4e52498519>:78: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.00906591482654647' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"hedonic_v\"] = projection_hedonic\n",
            "<ipython-input-81-9a4e52498519>:79: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.02234547954340919' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"movement_v\"] = projection_movement\n",
            "<ipython-input-81-9a4e52498519>:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.01597811301763228' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"moral_v\"] = projection_moral\n",
            "<ipython-input-81-9a4e52498519>:78: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.019875303414387153' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"hedonic_v\"] = projection_hedonic\n",
            "<ipython-input-81-9a4e52498519>:79: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '-0.006088716791096466' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"movement_v\"] = projection_movement\n",
            "<ipython-input-81-9a4e52498519>:77: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.03322264972859719' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"moral_v\"] = projection_moral\n",
            "<ipython-input-81-9a4e52498519>:78: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.003661206904315091' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"hedonic_v\"] = projection_hedonic\n",
            "<ipython-input-81-9a4e52498519>:79: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.012110283904437328' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  projection_df.loc[a, \"movement_v\"] = projection_movement\n"
          ]
        }
      ],
      "source": [
        "# get projections of stimuli onto each vector direction\n",
        "#item_projections = get_projections(action_list_all, moral_v, hedonic_v, movement_v, social_v)\n",
        "excess_projections = get_projections(excess_list, moral_v, hedonic_v, movement_v)\n",
        "deficiency_projections = get_projections(deficiency_list, moral_v, hedonic_v, movement_v)\n",
        "mean_projections = get_projections(mean_list, moral_v, hedonic_v, movement_v)\n",
        "\n",
        "item_projections[\"item\"] = item_projections.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LsmoT02gdnb"
      },
      "outputs": [],
      "source": [
        "data = {'excess': virtue_data['excess'],\n",
        "        'excess value': excess_projections['moral_v'].tolist(),\n",
        "        'mean': virtue_data['mean'],\n",
        "        'mean value': mean_projections['moral_v'].tolist(),\n",
        "        'deficiency': virtue_data['deficiency'],\n",
        "        'deficiency value': deficiency_projections['moral_v'].tolist()\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "38HrSs51bhl5",
        "outputId": "734a7be9-a5a6-47bb-bfe5-e21b29ee2c67"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"excess\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"shyness\",\n          \"obsequiousness\",\n          \"rashness\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"excess value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.017109562874937242,\n        \"min\": -0.02438107085947572,\n        \"max\": 0.028848077849008426,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.01140009755973077,\n          0.01719929304125062,\n          -0.008128498181515299\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"modesty\",\n          \"friendliness\",\n          \"courage\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006758989473082272,\n        \"min\": 0.018419422930845527,\n        \"max\": 0.03904452315825714,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          0.02047220337567853,\n          0.03225844511196365,\n          0.03322264972859719\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deficiency\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"shamelessness\",\n          \"cantankerousness\",\n          \"cowardice\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"deficiency value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011921011695096828,\n        \"min\": -0.021524111762055504,\n        \"max\": 0.013066045091375624,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          -0.013275769186875704,\n          0.008617280579750938,\n          -0.01597811301763228\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-09d6eada-4e6d-4ad5-9e7b-b6726dedcb25\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>excess</th>\n",
              "      <th>excess value</th>\n",
              "      <th>mean</th>\n",
              "      <th>mean value</th>\n",
              "      <th>deficiency</th>\n",
              "      <th>deficiency value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rashness</td>\n",
              "      <td>-0.008128</td>\n",
              "      <td>courage</td>\n",
              "      <td>0.033223</td>\n",
              "      <td>cowardice</td>\n",
              "      <td>-0.015978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>licentiousness</td>\n",
              "      <td>-0.013024</td>\n",
              "      <td>temperance</td>\n",
              "      <td>0.030826</td>\n",
              "      <td>insensibility</td>\n",
              "      <td>-0.020047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>prodigality</td>\n",
              "      <td>0.002157</td>\n",
              "      <td>liberality</td>\n",
              "      <td>0.028609</td>\n",
              "      <td>meanness</td>\n",
              "      <td>-0.017838</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vulgarity</td>\n",
              "      <td>-0.020713</td>\n",
              "      <td>magnificence</td>\n",
              "      <td>0.037803</td>\n",
              "      <td>pettiness</td>\n",
              "      <td>-0.018463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>vanity</td>\n",
              "      <td>0.014390</td>\n",
              "      <td>magnanimity</td>\n",
              "      <td>0.039045</td>\n",
              "      <td>pusillanimity</td>\n",
              "      <td>0.013066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ambition</td>\n",
              "      <td>0.022900</td>\n",
              "      <td>pride</td>\n",
              "      <td>0.033071</td>\n",
              "      <td>unambitiousness</td>\n",
              "      <td>-0.008197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>irascibility</td>\n",
              "      <td>0.007597</td>\n",
              "      <td>patience</td>\n",
              "      <td>0.026559</td>\n",
              "      <td>unirascibility</td>\n",
              "      <td>0.005336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>boastfulness</td>\n",
              "      <td>0.028848</td>\n",
              "      <td>truthfulness</td>\n",
              "      <td>0.033105</td>\n",
              "      <td>understatement</td>\n",
              "      <td>-0.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>buffoonery</td>\n",
              "      <td>-0.024381</td>\n",
              "      <td>wittiness</td>\n",
              "      <td>0.020567</td>\n",
              "      <td>boorishness</td>\n",
              "      <td>-0.012465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>obsequiousness</td>\n",
              "      <td>0.017199</td>\n",
              "      <td>friendliness</td>\n",
              "      <td>0.032258</td>\n",
              "      <td>cantankerousness</td>\n",
              "      <td>0.008617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>shyness</td>\n",
              "      <td>0.011400</td>\n",
              "      <td>modesty</td>\n",
              "      <td>0.020472</td>\n",
              "      <td>shamelessness</td>\n",
              "      <td>-0.013276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>envy</td>\n",
              "      <td>-0.003298</td>\n",
              "      <td>righteous indignation</td>\n",
              "      <td>0.018419</td>\n",
              "      <td>spitefulness</td>\n",
              "      <td>-0.003023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09d6eada-4e6d-4ad5-9e7b-b6726dedcb25')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-09d6eada-4e6d-4ad5-9e7b-b6726dedcb25 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-09d6eada-4e6d-4ad5-9e7b-b6726dedcb25');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4731cc5e-88d1-4a5c-abd9-d2d1459af9a2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4731cc5e-88d1-4a5c-abd9-d2d1459af9a2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4731cc5e-88d1-4a5c-abd9-d2d1459af9a2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_103f3bb6-d2d3-48cf-8767-b959d7728cae\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_103f3bb6-d2d3-48cf-8767-b959d7728cae button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "            excess  excess value                   mean  mean value  \\\n",
              "0         rashness     -0.008128                courage    0.033223   \n",
              "1   licentiousness     -0.013024             temperance    0.030826   \n",
              "2      prodigality      0.002157             liberality    0.028609   \n",
              "3        vulgarity     -0.020713           magnificence    0.037803   \n",
              "4           vanity      0.014390            magnanimity    0.039045   \n",
              "5         ambition      0.022900                  pride    0.033071   \n",
              "6     irascibility      0.007597               patience    0.026559   \n",
              "7     boastfulness      0.028848           truthfulness    0.033105   \n",
              "8       buffoonery     -0.024381              wittiness    0.020567   \n",
              "9   obsequiousness      0.017199           friendliness    0.032258   \n",
              "10         shyness      0.011400                modesty    0.020472   \n",
              "11            envy     -0.003298  righteous indignation    0.018419   \n",
              "\n",
              "          deficiency  deficiency value  \n",
              "0          cowardice         -0.015978  \n",
              "1      insensibility         -0.020047  \n",
              "2           meanness         -0.017838  \n",
              "3          pettiness         -0.018463  \n",
              "4      pusillanimity          0.013066  \n",
              "5    unambitiousness         -0.008197  \n",
              "6     unirascibility          0.005336  \n",
              "7     understatement         -0.021524  \n",
              "8        boorishness         -0.012465  \n",
              "9   cantankerousness          0.008617  \n",
              "10     shamelessness         -0.013276  \n",
              "11      spitefulness         -0.003023  "
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.DataFrame(data)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WBsN8VYp9yJ",
        "outputId": "cd222f28-86cf-4afe-ab80-7b0481d8de7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Item Embedding Projection Correlations:\n",
            "r(moral, hedonic) = 0.81\n",
            "r(moral, movement) = 0.185\n",
            "r(hedonic, movement) = 0.499\n"
          ]
        }
      ],
      "source": [
        "print('Item Embedding Projection Correlations:')\n",
        "print('r(moral, hedonic) = '+ str(round(np.corrcoef(item_projections[\"moral_v\"], item_projections[\"hedonic_v\"])[0, 1], 3)))\n",
        "print('r(moral, movement) = '+ str(round(np.corrcoef(item_projections[\"moral_v\"], item_projections[\"movement_v\"])[0, 1], 3)))\n",
        "print('r(hedonic, movement) = '+ str(round(np.corrcoef(item_projections[\"hedonic_v\"], item_projections[\"movement_v\"])[0, 1], 3)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "id": "PFhEzfGiqCqP",
        "outputId": "b03d49a4-b4ac-4baf-c7ee-bc2edb107f94"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-36cdd83c0fbc>:17: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  ax.annotate(plot_i, (item_projections_1['hedonic_v'][i]+.0005, item_projections_1['moral_v'][i]-.0001))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x78f5a7ee4940>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot Embeddings for Moral x Hedonic\n",
        "\n",
        "# sort projections by morality before plotting\n",
        "item_projections_1 = item_projections.sort_values('moral_v', ascending=False)\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "cmap = plt.get_cmap(\"tab20\")\n",
        "\n",
        "plot_i = 1\n",
        "for i in range(len(item_projections_1)):\n",
        "    x = item_projections_1['hedonic_v'].iloc[i]\n",
        "    y = item_projections_1['moral_v'].iloc[i]\n",
        "    colors = cmap(i/len(item_projections_1))\n",
        "    l = str(plot_i) + '. ' + item_projections_1.index[i].lstrip('[').rstrip(']')\n",
        "    ax.scatter([x],[y], label=l)\n",
        "    ax.annotate(plot_i, (item_projections_1['hedonic_v'][i]+.0005, item_projections_1['moral_v'][i]-.0001))\n",
        "    plot_i += 1\n",
        "\n",
        "ax.set_xlabel('Hedonic')\n",
        "ax.set_ylabel('Moral')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCmO3srU1Z78"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJgs-SgKqGP_"
      },
      "source": [
        "# Sort-Rate Prompting (GPT-3.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7BmdfO1qO72"
      },
      "outputs": [],
      "source": [
        "# @title define functions\n",
        "\n",
        "\n",
        "def promptGPT(prompt_message_list, gpt_temperature=0):\n",
        "  gpt_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "  gpt_headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": userdata.get('OAI')\n",
        "    }\n",
        "  gpt_data = {\n",
        "        \"model\": \"gpt-3.5-turbo-1106\",\n",
        "        \"response_format\": {\"type\": \"json_object\"}, # only works on 3.5-turbo-1106, 4 and above\n",
        "        \"temperature\": gpt_temperature,\n",
        "        \"messages\": prompt_message_list,\n",
        "    }\n",
        "  response = requests.post(gpt_url, headers=gpt_headers, json=gpt_data)\n",
        "  output = response.json()['choices'][0]['message']['content']\n",
        "\n",
        "  return output\n",
        "\n",
        "def prompt_sort(action_list, dimension, context=\"\"):\n",
        "  \"\"\"\n",
        "  Given a list of actions to sort, and a dimension to sort them on, craft a prompt\n",
        "  for GPT, with example calls and responses.\n",
        "\n",
        "  Optionally, include context regarding the hypothetical rater (Ziv),\n",
        "  which may influence the ranking results.\n",
        "\n",
        "  Returns a list of dicts to pass on to GPT as prompt messages\n",
        "  \"\"\"\n",
        "\n",
        "  #randomize the order of the list\n",
        "  random.shuffle(action_list)\n",
        "\n",
        "  #turn list into formatted string\n",
        "  action_string = (', ').join(action_list)\n",
        "\n",
        "  # Switch by sorting dimension\n",
        "  if dimension == 'moral':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the relative moral virtues of different actions.\n",
        "Moral actions are those which humans consider virtuous, that consider others' wellbeing and happiness, and that are guided by principles of ethics.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "You must sort them in terms of their relative moral virtue according to Ziv.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: an unsorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. ranking - Array<string>: A sorted list of actions, starting with the most morally wrong, and ending with the most morally virtuous from Ziv's perpsective.\"\"\"\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"walk a dog\", \"rob a bank\", \"rescue a cat\", \"steal food\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"ranking\": [\"rob a bank\", \"steal food\", \"walk a dog\", \"rescue a cat\"]})\n",
        "    }\n",
        "\n",
        "  elif dimension == 'hedonic':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the relative hedonic reward of different actions.\n",
        "Hedonically rewarding actions are those which humans consider pleasant, make them feel happy, and benefit their own wellbeing.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "You must sort them in terms of their relative hedonic reward for Ziv.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: an unsorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. ranking - Array<string>: A sorted list of actions, starting with the\n",
        "worst and least hedonically rewarding, and ending with the best and most hedonically rewarding from Ziv's perpsective.\"\"\"\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"miss the bus\", \"win a sweepstakes\", \"eat a meal\", \"lose my wallet\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"ranking\": [\"lose my wallet\", \"miss the bus\", \"eat a meal\", \"win a sweepstakes\"]})\n",
        "    }\n",
        "\n",
        "  elif dimension == 'movement':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the physical body movement\n",
        "of different actions. Physically active actions are those which humans consider to involve substantial movement of the body.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "You must sort them in terms of how much physical movement they involve.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: an unsorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "\n",
        "1. ranking - Array<string>: A sorted list of actions, starting with the\n",
        "least physical body movement, and ending with the most physical body movement.\"\"\"\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"play soccer\", \"listen to music\", \"pack boxes\", \"order coffee at a cafe\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"ranking\": [\"listen to music\", \"order coffee at a cafe\", \"pack boxes\", \"play soccer\"]})\n",
        "    }\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Specify a rating dimension: moral, hedonic, movement\")\n",
        "\n",
        "\n",
        "  # List actions to sort\n",
        "  user_prompt = {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": json.dumps({\"actions\": action_list,\n",
        "                             \"context\": context}) # Optional context, defaults to empty string\n",
        "  }\n",
        "\n",
        "  return [system_prompt, user_example, assistant_example, user_prompt]\n",
        "\n",
        "\n",
        "def prompt_rate_sorted(action_list, dimension, context=\"\"):\n",
        "  \"\"\"\n",
        "  Given a sorted list of actions to rate, and a dimension to sort them on, craft a prompt\n",
        "  for GPT, with example calls and responses.\n",
        "\n",
        "  Optionally, include context regarding the hypothetical rater (Ziv),\n",
        "  which may influence the rating results.\n",
        "\n",
        "  Returns a list of dicts to pass on to GPT as prompt messages\n",
        "  \"\"\"\n",
        "\n",
        "  #turn list into formatted string\n",
        "  action_string = (', ').join(action_list)\n",
        "\n",
        "  # Switch by sorting dimension\n",
        "  if dimension == 'moral':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the relative moral virtues of different actions.\n",
        "Moral actions are those which humans consider virtuous, that consider others' wellbeing and happiness, and that are guided by principles of ethics.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "They are already sorted in terms of their relative moral virtue according to Ziv.\n",
        "You will assign a number to each adjacent pair, rating 0 if the actions are very similar in terms of moral virtue, and 10 if the actions are very different in terms of moral virtue.\n",
        "You will give the first action a rating of 0. You will give the second action a rating in comparison to the first action, and so on.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: a sorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. ranking - Array: A list of lists, each list contains 2 elements:\n",
        "1. action from the original list\n",
        "2. similarity rating from 0 to 10 between this action and the previous action in the list\"\"\"\n",
        "\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"rob a bank\", \"steal food\", \"walk a dog\", \"rescue a cat\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"ranking\": [[\"rob a bank\", 0], [\"steal food\", 3], [\"walk a dog\", 10], [\"rescue a cat\", 6]]})\n",
        "    }\n",
        "\n",
        "  elif dimension == 'hedonic':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the relative hedonic reward\n",
        "of different actions. Hedonically rewarding actions are those which humans consider pleasant, make them feel happy, and benefit their own wellbeing.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "They are already sorted in terms of their relative hedonic reward for Ziv.\n",
        "You will assign a number to each adjacent pair, rating 0 if the actions are very similar\n",
        "in terms of hedonic reward, and 10 if the actions are very different in terms of hedonic reward.\n",
        "You will give the first action a rating of 0. You will give the second action a rating in comparison to the first action, and so on.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: a sorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. ranking - Array: A list of lists, each list contains 2 elements:\n",
        "1. action from the original list\n",
        "2. similarity rating from 0 to 10 between this action and the previous action in the list\"\"\"\n",
        "\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"lose my wallet\", \"miss the bus\", \"eat a meal\", \"win a sweepstakes\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"ranking\": [[\"lose my wallet\", 0], [\"miss the bus\", 2], [\"eat a meal\", 9], [\"win a sweepstakes\", 8]]})\n",
        "    }\n",
        "\n",
        "  elif dimension == 'movement':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the physical body movement of different actions.\n",
        "Physically active actions are those which humans consider to involve substantial movement of the body.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "They are already sorted in terms of how much physical movement they involve.\n",
        "You will assign a number to each adjacent pair, rating 0 if the actions are very similar\n",
        "in terms of amount of physical movement, and 10 if the actions are very different in terms of amount of physical movement.\n",
        "You will give the first action a rating of 0. You will give the second action a rating in comparison to the first action, and so on.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: a sorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. ranking - Array: A list of lists, each list contains 2 elements:\n",
        "1. action from the original list\n",
        "2. similarity rating from 0 to 10 between this action and the previous action in the list\"\"\"\n",
        "\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"listen to music\", \"order coffee at a cafe\", \"pack boxes\", \"play soccer\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"ranking\": [[\"listen to music\", 0], [\"order coffee at a cafe\", 2], [\"pack boxes\", 6], [\"play soccer\", 9]]})\n",
        "    }\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Specify a rating dimension: moral, hedonic, movement\")\n",
        "\n",
        "\n",
        "  # List actions to sort\n",
        "  user_prompt = {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": json.dumps({\"actions\": action_list,\n",
        "                             \"context\": context}) # Optional context, defaults to empty string\n",
        "  }\n",
        "\n",
        "  return [system_prompt, user_example, assistant_example, user_prompt]\n",
        "\n",
        "def sort_rate(action_list, dimension):\n",
        "\n",
        "  # Sort actions\n",
        "  resp = promptGPT(prompt_sort(action_list, dimension))\n",
        "  resp_sorted_list = json.loads(resp)['ranking']\n",
        "\n",
        "  # Rate distance between sorted actions\n",
        "  resp = promptGPT(prompt_rate_sorted(resp_sorted_list, dimension))\n",
        "  resp_rated_list = json.loads(resp)['ranking']\n",
        "\n",
        "  # parse the ratings into a dataframe\n",
        "  resp_df = pd.DataFrame(resp_rated_list, columns=['item', 'rating'])\n",
        "  resp_df['rating'] = pd.to_numeric(resp_df['rating'])\n",
        "  resp_df['rating_sum'] = resp_df['rating'].cumsum()\n",
        "  resp_df['rescored_' + dimension] = resp_df['rating_sum'] / resp_df['rating_sum'].max() * 100\n",
        "  resp_df = resp_df.drop(columns=['rating', 'rating_sum'])\n",
        "\n",
        "  return resp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HuBalu7gqk7Q",
        "outputId": "343a6f79-841d-4751-cfb0-61b4a30ea945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "False\n",
            "1\n",
            "False\n",
            "2\n",
            "False\n",
            "3\n",
            "False\n",
            "4\n",
            "False\n",
            "5\n",
            "False\n",
            "6\n",
            "False\n",
            "7\n",
            "False\n",
            "8\n",
            "False\n",
            "9\n",
            "False\n",
            "10\n",
            "False\n",
            "11\n",
            "False\n",
            "12\n",
            "False\n",
            "13\n",
            "False\n",
            "14\n",
            "False\n",
            "15\n",
            "False\n",
            "16\n",
            "False\n",
            "17\n",
            "False\n",
            "18\n",
            "False\n",
            "19\n",
            "False\n",
            "20\n",
            "False\n",
            "21\n",
            "False\n",
            "22\n",
            "False\n",
            "23\n",
            "False\n",
            "24\n",
            "False\n",
            "25\n",
            "False\n",
            "26\n",
            "False\n",
            "27\n",
            "False\n",
            "28\n",
            "False\n",
            "29\n",
            "False\n",
            "30\n",
            "False\n",
            "31\n",
            "False\n",
            "32\n",
            "False\n",
            "33\n",
            "False\n",
            "34\n",
            "False\n",
            "35\n",
            "False\n",
            "36\n",
            "False\n",
            "37\n",
            "False\n",
            "38\n",
            "False\n",
            "39\n",
            "False\n",
            "40\n",
            "False\n",
            "41\n",
            "False\n",
            "42\n",
            "False\n",
            "43\n",
            "False\n",
            "44\n",
            "False\n",
            "45\n",
            "False\n",
            "46\n",
            "False\n",
            "47\n",
            "False\n",
            "48\n",
            "False\n",
            "49\n",
            "False\n",
            "50\n",
            "False\n",
            "51\n",
            "False\n",
            "52\n",
            "False\n",
            "53\n",
            "False\n",
            "54\n",
            "False\n",
            "55\n",
            "False\n",
            "56\n",
            "False\n",
            "57\n",
            "False\n",
            "58\n",
            "False\n",
            "59\n",
            "False\n",
            "60\n",
            "False\n",
            "61\n",
            "False\n",
            "62\n",
            "False\n",
            "63\n",
            "False\n",
            "64\n",
            "False\n",
            "65\n",
            "False\n",
            "66\n",
            "False\n",
            "67\n",
            "False\n",
            "68\n",
            "False\n",
            "69\n",
            "False\n"
          ]
        },
        {
          "ename": "JSONDecodeError",
          "evalue": "Unterminated string starting at: line 1 column 17562 (char 17561)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-eca9abd3198a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msort_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_list_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-81b6724ea311>\u001b[0m in \u001b[0;36msort_rate\u001b[0;34m(action_list, dimension)\u001b[0m\n\u001b[1;32m    253\u001b[0m   \u001b[0;31m# Sort actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m   \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromptGPT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_sort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m   \u001b[0mresp_sorted_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ranking'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m   \u001b[0;31m# Rate distance between sorted actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Unterminated string starting at: line 1 column 17562 (char 17561)"
          ]
        }
      ],
      "source": [
        "# # Create list of lists with all items\n",
        "niter = 100\n",
        "dimension = 'hedonic'\n",
        "n_items = len(action_list_all)\n",
        "result_lists = [[action, []] for action in action_list_all]\n",
        "\n",
        "for i in range(niter):\n",
        "  result_df = []\n",
        "  print(i)\n",
        "\n",
        "  count=0\n",
        "  check=False\n",
        "  while(check==False):\n",
        "    print(check)\n",
        "    result_df = sort_rate(action_list_all, dimension)\n",
        "    count = count+1\n",
        "    a = set(list(result_df['item']))\n",
        "    b = set(action_list_all)\n",
        "    if(len(b.difference(a))==0):\n",
        "      check=True\n",
        "    else:\n",
        "      check=False\n",
        "    print(count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "Xfi2wQuzxfIH",
        "outputId": "79a78fb7-4b2e-43b0-f997-d393207a4fd3"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-8e52ba315d4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maction_list_all\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mact_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rescored_'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0mcur_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_lists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ],
      "source": [
        " for action in action_list_all:\n",
        "    act_val = result_df[result_df['item'] == action]['rescored_'+ dimension].values[0]\n",
        "    cur_vals = []\n",
        "    for row in result_lists:\n",
        "      if row[0] == action:\n",
        "        row[1].append(act_val)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atNKoTpoyjHs"
      },
      "outputs": [],
      "source": [
        "#summary statistics for each item\n",
        "stats_results_lists = []\n",
        "\n",
        "for row in result_lists:\n",
        "  if len(row[1]) > 0:\n",
        "    CI = st.t.interval(0.95, len(row[1])-1, loc=np.mean(row[1]), scale=st.sem(row[1]))\n",
        "    new_row = [\n",
        "        row[0],\n",
        "        max(0, CI[0]), # 0 or low CI\n",
        "        np.mean(row[1]),\n",
        "        min(100, CI[1]), # 100 or high CI\n",
        "        ]\n",
        "    stats_results_lists.append(new_row)\n",
        "  else:\n",
        "    print('Need more data for ' + row[0])\n",
        "\n",
        "stat_results_df = pd.DataFrame(stats_results_lists, columns=['item', '95CI_Low', 'mean', '95CI_High'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wpaL7UiypSG"
      },
      "source": [
        "## read in prompting data from past results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "7-KI4BViymAI",
        "outputId": "666b793d-a75a-4f07-ecb6-1b6c06fbaf00"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'data_path' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-1b54ab9ae7cd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmoral_df_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'moral_100_iter_results.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mhedonic_df_stats\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'hedonic_100_iter_results.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmovement_df_stats\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'movement_100_iter_results.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmoral_df_stats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'null'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'low_moral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mean_moral'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'high_moral'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_path' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "moral_df_stats = pd.read_csv(data_path + 'moral_100_iter_results.csv')\n",
        "hedonic_df_stats =  pd.read_csv(data_path + 'hedonic_100_iter_results.csv')\n",
        "movement_df_stats =  pd.read_csv(data_path + 'movement_100_iter_results.csv')\n",
        "\n",
        "moral_df_stats.columns = ['null', 'item', 'low_moral', 'mean_moral', 'high_moral']\n",
        "hedonic_df_stats.columns = ['null', 'item', 'low_hedonic', 'mean_hedonic', 'high_hedonic']\n",
        "movement_df_stats.columns = ['null', 'item', 'low_movement', 'mean_movement', 'high_movement']\n",
        "\n",
        "moral_df_stats = moral_df_stats.drop(columns=['null'])\n",
        "hedonic_df_stats = hedonic_df_stats.drop(columns=['null'])\n",
        "movement_df_stats = movement_df_stats.drop(columns=['null'])\n",
        "\n",
        "\n",
        "full_df = pd.merge(moral_df_stats, hedonic_df_stats, on='item', how='inner')\n",
        "full_df = pd.merge(full_df, movement_df_stats, on='item', how='inner')\n",
        "\n",
        "#now subset to the items of concern here\n",
        "full_df_prompting = full_df[full_df[\"item\"].isin(action_list_all)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuhpdDUfzVA1"
      },
      "source": [
        "## analyze prompting data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJumQVu9ys7x"
      },
      "outputs": [],
      "source": [
        "print('Prompting Correlations:')\n",
        "print('r(moral, hedonic) = '+ str(round(np.corrcoef(full_df_prompting[\"mean_moral\"], full_df_prompting[\"mean_hedonic\"])[0, 1], 3)))\n",
        "print('r(moral, movement) = '+ str(round(np.corrcoef(full_df_prompting[\"mean_moral\"], full_df_prompting[\"mean_movement\"])[0, 1], 3)))\n",
        "print('r(hedonic, movement) = '+ str(round(np.corrcoef(full_df_prompting[\"mean_hedonic\"], full_df_prompting[\"mean_movement\"])[0, 1], 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmXLoGt0zOhn"
      },
      "outputs": [],
      "source": [
        "#@title generate plots\n",
        "full_df_prompting_sorted = full_df_prompting.sort_values('mean_moral', ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "plot_i = 1\n",
        "for i in range(len(full_df_prompting_sorted)):\n",
        "\n",
        "    # X axis, means with 95% CI low and high values\n",
        "    x = full_df_prompting_sorted['mean_hedonic'].iloc[i]\n",
        "    x_l = full_df_prompting_sorted['mean_hedonic'].iloc[i] - full_df_prompting_sorted['low_hedonic'].iloc[i]\n",
        "    x_h = full_df_prompting_sorted['high_hedonic'].iloc[i] - full_df_prompting_sorted['mean_hedonic'].iloc[i]\n",
        "\n",
        "    # Y axis, means with 95% CI low and high values\n",
        "    y = full_df_prompting_sorted['mean_moral'].iloc[i]\n",
        "    y_l = full_df_prompting_sorted['mean_moral'].iloc[i] - full_df_prompting_sorted['low_moral'].iloc[i]\n",
        "    y_h = full_df_prompting_sorted['high_moral'].iloc[i] - full_df_prompting_sorted['mean_moral'].iloc[i]\n",
        "\n",
        "    l = str(plot_i) + '. ' + full_df_prompting_sorted['item'].iloc[i].lstrip('[').rstrip(']')\n",
        "    ax.errorbar(x, y, xerr=[[x_l], [x_h]], yerr=[[y_l], [y_h]], label=l, fmt='o', ecolor='lightgray', elinewidth=2)\n",
        "    ax.annotate(plot_i, (x+.3, y+.5))\n",
        "\n",
        "    plot_i += 1\n",
        "\n",
        "ax.set_xlabel('Hedonic')\n",
        "ax.set_ylabel('Moral')\n",
        "ax.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p8__cHVzhYO"
      },
      "outputs": [],
      "source": [
        "data_table.DataTable(full_df_prompting_sorted, include_index = False, num_rows_per_page = 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gyqdBcKzuID"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpwsuGUeznuk"
      },
      "source": [
        "# Direct Judgment/ Likert Rating Prompt (GPT-3.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpiL_CIZzun1"
      },
      "outputs": [],
      "source": [
        "# @title define functions\n",
        "\n",
        "\n",
        "def prompt_rate(action_list, dimension, context=\"\"):\n",
        "  \"\"\"\n",
        "  Given a list of actions to sort, and a dimension to sort them on, craft a prompt\n",
        "  for GPT, with example calls and responses.\n",
        "\n",
        "  Optionally, include context regarding the hypothetical rater (Ziv),\n",
        "  which may influence the ranking results.\n",
        "\n",
        "  Returns a list of dicts to pass on to GPT as prompt messages\n",
        "  \"\"\"\n",
        "\n",
        "  #randomize the order of the list\n",
        "  random.shuffle(action_list)\n",
        "\n",
        "  #turn list into formatted string\n",
        "  action_string = (', ').join(action_list)\n",
        "\n",
        "  # Switch by sorting dimension\n",
        "  if dimension == 'moral':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the moral virtues of different actions.\n",
        "Moral actions are those which humans consider virtuous, that consider others' wellbeing and happiness, and that are guided by principles of ethics.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "You must rate them in terms of their moral virtue according to Ziv.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: a sorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. rating - Array: A list of lists, each list contains 2 elements:\n",
        "1. action from the original list\n",
        "2. rating from 0 to 10 for this action, where 0 is the most morally wrong, and 10 is the most morally virtuous\"\"\"\n",
        "\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"walk a dog\", \"rob a bank\", \"rescue a cat\", \"steal food\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"rating\": [[\"walk a dog\", 5], [\"rob a bank\", 1],[ \"rescue a cat\",8],[\"steal food\", 2]]})\n",
        "    }\n",
        "\n",
        "  elif dimension == 'hedonic':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the hedonic reward of different actions.\n",
        "Hedonically rewarding actions are those which humans consider pleasant, make them feel happy, and benefit their own wellbeing.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "You must rate them in terms of their hedonic reward according to Ziv.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: a sorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. rating - Array: A list of lists, each list contains 2 elements:\n",
        "1. action from the original list\n",
        "2. rating from 0 to 10 for this action, where 0 is the least hedonically rewarding, and 10 is the most hedonically rewarding\"\"\"\n",
        "\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"miss the bus\", \"win a sweepstakes\", \"eat a meal\", \"lose my wallet\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"rating\": [[\"miss the bus\", 3], [\"win a sweepstakes\", 9], [\"eat a meal\", 7], [\"lose my wallet\", 2]]})\n",
        "    }\n",
        "\n",
        "  elif dimension == 'movement':\n",
        "\n",
        "    system_prompt_content = \"\"\"You are an expert judge of the physical body movement of different actions.\n",
        "Physically active actions are those which humans consider to involve substantial movement of the body.\n",
        "You will be given a list of actions that an average human person, Ziv, is considering.\n",
        "You must rate them in terms of how much physical movement they involve.\n",
        "You will recieve a JSON object of items to rate. This object will have the following key / value pairs:\n",
        "1. actions - Array<string>: a sorted list of actions that the user has been faced with\n",
        "2. context - string: information that you can use to help you make your choice\n",
        "You must respond with a JSON packet with a single key / value pair:\n",
        "1. rating - Array: A list of lists, each list contains 2 elements:\n",
        "1. action from the original list\n",
        "2. rating from 0 to 10 for this action, where 0 indicates the least physical body movement, and 10 indicates the most physical body movement\"\"\"\n",
        "\n",
        "    system_prompt= {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": system_prompt_content\n",
        "    }\n",
        "    user_example = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": json.dumps({\"actions\": [\"play soccer\", \"listen to music\", \"pack boxes\", \"order coffee at a cafe\"],\n",
        "                               \"context\": \"\"}) # Initial example has no context\n",
        "    }\n",
        "    assistant_example = {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": json.dumps({\"rating\": [[\"play soccer\", 9], [\"listen to music\", 0], [\"pack boxes\", 6], [\"order coffee at a cafe\", 2]]})\n",
        "    }\n",
        "\n",
        "  else:\n",
        "    raise Exception(\"Specify a rating dimension: moral, hedonic, movement\")\n",
        "\n",
        "\n",
        "  # List actions to sort\n",
        "  user_prompt = {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": json.dumps({\"actions\": action_list,\n",
        "                             \"context\": context}) # Optional context, defaults to empty string\n",
        "  }\n",
        "\n",
        "  return [system_prompt, user_example, assistant_example, user_prompt]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GyRrMmkz8fX"
      },
      "source": [
        "## run iterative prompting and compute stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_CPOMuQz717"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create list of lists with all items\n",
        "resample_size = 20\n",
        "dimension = 'movement'\n",
        "\n",
        "result_lists = [[action, []] for action in action_list_all]\n",
        "\n",
        "for i in range(resample_size):\n",
        "\n",
        "  # Rate actions\n",
        "  resp = promptGPT(prompt_rate(action_list_all, dimension))\n",
        "  resp_list = json.loads(resp)['rating']\n",
        "  resp_df = pd.DataFrame(resp_list, columns=['item', 'rating'])\n",
        "  resp_df['rating'] = pd.to_numeric(resp_df['rating'])\n",
        "\n",
        "  for action in action_list_all:\n",
        "    act_val = resp_df[resp_df['item'] == action]['rating'].values[0]\n",
        "    cur_vals = []\n",
        "    for row in result_lists:\n",
        "      if row[0] == action:\n",
        "        row[1].append(act_val)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpU1t04W0AwX"
      },
      "outputs": [],
      "source": [
        "# summary statistics for each item\n",
        "new_result_lists = []\n",
        "\n",
        "for row in result_lists:\n",
        "  if len(row[1]) > 0:\n",
        "    CI = st.t.interval(0.95, len(row[1])-1, loc=np.mean(row[1]), scale=st.sem(row[1]))\n",
        "    new_row = [\n",
        "        row[0],\n",
        "        max(0, CI[0]), # 0 or low CI\n",
        "        np.mean(row[1]),\n",
        "        min(10, CI[1]), # 10 or high CI\n",
        "        ]\n",
        "    new_result_lists.append(new_row)\n",
        "  else:\n",
        "    print('Need more data for ' + row[0])\n",
        "\n",
        "new_results_df = pd.DataFrame(new_result_lists, columns=['item', '95CI_Low', 'mean', '95CI_High'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}